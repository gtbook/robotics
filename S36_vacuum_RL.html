
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3.6. Learning to Act Optimally &#8212; Introduction to Robotics and Perception</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=51e3b7cf" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=c73c0f3e"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'S36_vacuum_RL';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3.7. Chapter Summary" href="S37_vacuum_summary.html" />
    <link rel="prev" title="3.5. Markov Decision Processes" href="S35_vacuum_decision.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Introduction to Robotics and Perception - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Introduction to Robotics and Perception - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="S10_introduction.html">1. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S11_models.html">1.1. Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S12_reasoning.html">1.2. Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S13_math.html">1.3. The Mathematics of Robotics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S20_sorter_intro.html">2. A Trash Sorting Robot</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S21_sorter_state.html">2.1. Modeling the World State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S22_sorter_actions.html">2.2. Actions for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S23_sorter_sensing.html">2.3. Sensors for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S24_sorter_perception.html">2.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S25_sorter_decision_theory.html">2.5. Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="S26_sorter_learning.html">2.6. Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S27_sorter_summary.html">2.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="S30_vacuum_intro.html">3. A Robot Vacuum Cleaner</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="S31_vacuum_state.html">3.1. Modeling the State of the Vacuum Cleaning Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S32_vacuum_actions.html">3.2. Actions over time</a></li>
<li class="toctree-l2"><a class="reference internal" href="S33_vacuum_sensing.html">3.3. Dynamic Bayesian Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="S34_vacuum_perception.html">3.4. Perception with Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S35_vacuum_decision.html">3.5. Markov Decision Processes</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">3.6. Learning to Act Optimally</a></li>
<li class="toctree-l2"><a class="reference internal" href="S37_vacuum_summary.html">3.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S40_logistics_intro.html">4. Warehouse Robots in 2D</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S41_logistics_state.html">4.1. Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S42_logistics_actions.html">4.2. Moving in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="S43_logistics_sensing.html">4.3. Sensor Models with Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S44_logistics_perception.html">4.4. Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S45_logistics_planning.html">4.5. Planning for Logistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="S46_logistics_learning.html">4.6. Some System Identification</a></li>
<li class="toctree-l2"><a class="reference internal" href="S47_logistics_summary.html">4.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S50_diffdrive_intro.html">5. A Mobile Robot With Simple Kinematics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S51_diffdrive_state.html">5.1. State Space for a Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S52_diffdrive_actions.html">5.2. Motion Model for the Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S53_diffdrive_sensing.html">5.3. Robot Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="S54_diffdrive_perception.html">5.4. Computer Vision 101</a></li>
<li class="toctree-l2"><a class="reference internal" href="S55_diffdrive_planning.html">5.5. Path Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S56_diffdrive_learning.html">5.6. Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S57_diffdrive_summary.html">5.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S60_driving_intro.html">6. Autonomous Vehicles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S61_driving_state.html">6.1. Planar Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="S62_driving_actions.html">6.2. Kinematics for Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S63_driving_sensing.html">6.3. Sensing for Autonomous Vehicles</a></li>
<li class="toctree-l2"><a class="reference internal" href="S64_driving_perception.html">6.4. SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S65_driving_planning.html">6.5. Planning for Autonomous Driving.</a></li>
<li class="toctree-l2"><a class="reference internal" href="S66_driving_DRL.html">6.6. Deep Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S67_driving_summary.html">6.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S70_drone_intro.html">7. Autonomous Drones in 3D</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S71_drone_state.html">7.1. Moving in Three Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S72_drone_actions.html">7.2. Multi-rotor Aircraft</a></li>
<li class="toctree-l2"><a class="reference internal" href="S73_drone_sensing.html">7.3. Sensing for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S74_drone_perception.html">7.4. Visual SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S75_drone_planning.html">7.5. Trajectory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S76_drone_learning.html">7.6. Neural Radiance Fields for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S77_drone_summary.html">7.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">8. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="genindex.html">Index</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/S36_vacuum_RL.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Learning to Act Optimally</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-optimal-value-function">3.6.1. The Optimal Value Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#action-values-and-the-optimal-policy">3.6.2. Action Values and the Optimal Policy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">3.6.2.1. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">3.6.3. Policy Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">3.6.4. Value Iteration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.6.4.1. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-reinforcement-learning">3.6.5. Model-based Reinforcement Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-reinforcement-learning">3.6.6. Model-free Reinforcement Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation">3.6.6.1. Exploration vs Exploitation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">3.6.6.2. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.6.7. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S36_vacuum_RL.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="learning-to-act-optimally">
<span id="index-0"></span><h1><span class="section-number">3.6. </span>Learning to Act Optimally<a class="headerlink" href="#learning-to-act-optimally" title="Link to this heading">#</a></h1>
<blockquote>
<div><p>Learning to act optimally in a stochastic world.</p>
</div></blockquote>
<a class="reference internal image-reference" href="_images/S36-iRobot_vacuuming_robot-04.jpg"><img alt="Splash image with intelligent looking robot" class="align-center" src="_images/S36-iRobot_vacuuming_robot-04.jpg" style="width: 40%;" /></a>
<p>When a Markov Decision Process is fully specified we can <em>compute</em> an optimal policy.
Below we first define optimal value functions and examine their properties, most notably the Bellman equation.
We then discuss value iteration and policy iteration, two algorithms to calculate the optimal value function and its associated optimal policy. However, both these algorithms need a fully-defined MDP.</p>
<p>When the MDP is not known in advance, however, we have to <em>learn</em> an optimal policy over time. There are two main approaches: model-based and model-free.</p>
<section id="the-optimal-value-function">
<span id="index-1"></span><h2><span class="section-number">3.6.1. </span>The Optimal Value Function<a class="headerlink" href="#the-optimal-value-function" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>The optimal policy maximizes the value function.</p>
</div></blockquote>
<p>We now turn our attention to defining the <em>optimal</em> value function,
which can be used to construct the <strong>optimal policy</strong> <span class="math notranslate nohighlight">\(\pi^*\)</span>.
From Section 3.5 we know how to compute the value function for an arbitrary policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3052907c-be5d-467d-94ac-7c6cb324849a">
<span class="eqno">(3.57)<a class="headerlink" href="#equation-3052907c-be5d-467d-94ac-7c6cb324849a" title="Permalink to this equation">#</a></span>\[\begin{equation}
V^\pi(x) = \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^\pi(x').
\end{equation}\]</div>
<p>To begin, we recall the famous <strong>principle of optimality</strong>
as stated by Bellman in a
<a class="reference external" href="https://www.rand.org/content/dam/rand/pubs/papers/2008/P1416.pdf">1960 article in the IEEE Transactions on Automatic Control</a> <span id="id1">[<a class="reference internal" href="bibliography.html#id6" title="Richard Bellman and Robert Kalaba. Dynamic programming and adaptive processes I: mathematical foundation. IRE Transactions on Automatic Control, AC-5(1):5-10, 1960. doi:10.1109/TAC.1960.6429288.">Bellman and Kalaba, 1960</a>]</span>:</p>
<blockquote>
<div><p><em>An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.</em></p>
</div></blockquote>
<p id="index-2">This principle enables a key step in deriving a recursive formulation for the optimal policy. Indeed, the <strong>optimal value function</strong> <span class="math notranslate nohighlight">\(V^*: {\cal X} \rightarrow {\cal A}\)</span>
is merely the value function for the optimal policy.
This can be written mathematically as</p>
<div class="amsmath math notranslate nohighlight" id="equation-af8c8e3e-ba6e-4c1b-bec1-d16e25674007">
<span class="eqno">(3.58)<a class="headerlink" href="#equation-af8c8e3e-ba6e-4c1b-bec1-d16e25674007" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{aligned}
V^*(x) &amp;= \max_\pi V^{\pi}(x) \\
&amp;=
\max_\pi \left\{ \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^\pi(x')   \right\}\\
&amp;=
\max_\pi \left\{ \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^*(x')   \right\}\\
&amp;=
\max_a  \left\{ \bar{R}(x,a) + \gamma \sum_{x'} P(x'|x, a) V^*(x')   \right\} \\
\end{aligned}
\end{equation}\]</div>
<p>In the above, the second line follows immediately by using the definition of <span class="math notranslate nohighlight">\(V^\pi\)</span> above. The third line is more interesting.
By applying the principle of optimality, we replace <span class="math notranslate nohighlight">\(V^\pi(x')\)</span> with <span class="math notranslate nohighlight">\(V^*(x')\)</span>.
Simply put, if remaining decisions from state <span class="math notranslate nohighlight">\(x'\)</span> must constitute an optimal policy,
the corresponding value function at <span class="math notranslate nohighlight">\(x'\)</span> will be the optimal value function for <span class="math notranslate nohighlight">\(x'\)</span>.
For the fourth line,
because the value function has been written in recursive form,
<span class="math notranslate nohighlight">\(\pi\)</span> is applied only to the current state (i.e., when <span class="math notranslate nohighlight">\(\pi\)</span> is evaluated in the optimization,
it always appears as <span class="math notranslate nohighlight">\(\pi(x)\)</span>).
Therefore, we can write the optimization
as a maximization with respect to the <em>action</em> applied in the <em>current state</em>, rather than as a
maximization with respect to the entire policy <span class="math notranslate nohighlight">\(\pi\)</span>!</p>
<p>This equation is known as the <strong>Bellman equation</strong>.
It is named after Richard Bellman, the mathematician
who discovered it, and it is one of the most important equations in all of computer science.
The Bellman equation has a very nice interpretation:
the optimal value function of a state is the maximum expected reward
<em>plus</em> the discounted expected value function when acting optimally in the future.</p>
</section>
<section id="action-values-and-the-optimal-policy">
<span id="index-3"></span><h2><span class="section-number">3.6.2. </span>Action Values and the Optimal Policy<a class="headerlink" href="#action-values-and-the-optimal-policy" title="Link to this heading">#</a></h2>
<p>Using Bellman’s equation, it is straightforward to compute the optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span> from a given state <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-60aab2a5-df5d-468d-9764-b525eab568fb">
<span class="eqno">(3.59)<a class="headerlink" href="#equation-60aab2a5-df5d-468d-9764-b525eab568fb" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pi^*(x) = \arg
\max_a  \left\{ \bar{R}(x,a) + \gamma \sum_{x'} P(x'|x, a) V^*(x')   \right\}.
\end{equation}\]</div>
<p>This computation is performed so often that it is convenient to introduce the so-called <strong><span class="math notranslate nohighlight">\(Q\)</span>-function</strong>, which is the value of being in state <span class="math notranslate nohighlight">\(x\)</span> and taking action <span class="math notranslate nohighlight">\(a\)</span>, for a given value function <span class="math notranslate nohighlight">\(V\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2af7f6c3-f907-48a0-b2ed-88b36dfcfe01">
<span class="eqno">(3.60)<a class="headerlink" href="#equation-2af7f6c3-f907-48a0-b2ed-88b36dfcfe01" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{aligned}
Q(x,a;  V) \doteq \bar{R}(x,a) + \gamma \sum_{x'} P(x'|x, a) V(x') 
\end{aligned}
\end{equation}\]</div>
<p>Another name for Q-values is <strong>action values</strong>, to be contrasted with the state values, i.e., the value function <span class="math notranslate nohighlight">\(V(x)\)</span>. They allow us to write the optimal policy <span class="math notranslate nohighlight">\(\pi^*(x)\)</span> simply as picking, for any given state <span class="math notranslate nohighlight">\(x\)</span>, the action <span class="math notranslate nohighlight">\(a\)</span> with the highest action value <span class="math notranslate nohighlight">\(Q(x,a; V^*)\)</span> computed from the optimal value function <span class="math notranslate nohighlight">\(V^*\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f3e97ea9-1ae2-4f9b-bec4-db3fad89b2dc">
<span class="eqno">(3.61)<a class="headerlink" href="#equation-f3e97ea9-1ae2-4f9b-bec4-db3fad89b2dc" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pi^*(x) = \arg \max_a  Q(x,a; V^*)
\end{equation}\]</div>
<p>We will use <span class="math notranslate nohighlight">\(Q\)</span>-values in many of the algorithms in this section, and an efficient way to compute a Q-value from a value function is given below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Q_value</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate Q(x,a) from given value function&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">@</span> <span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A very efficient way to compute all Q-values for all state-action pairs at once, using <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, is</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>which we will also use below. It yields a matrix of size <span class="math notranslate nohighlight">\(|X| \times |A|\)</span>.</p>
<section id="exercise">
<h3><span class="section-number">3.6.2.1. </span>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Try to understand the function <code class="docutils literal notranslate"><span class="pre">Q_value</span></code> above for calculating the Q-values. Use the notebook to investigate the calculation for specific values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>Similarly, try to understand the “vectorized” form above that yields the entire table of Q-values at once.</p></li>
</ol>
</section>
</section>
<section id="policy-iteration">
<h2><span class="section-number">3.6.3. </span>Policy Iteration<a class="headerlink" href="#policy-iteration" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>By iteratively improving an estimate of the optimal policy, we eventually find <span class="math notranslate nohighlight">\(\pi^*\)</span>.</p>
</div></blockquote>
<p id="index-4">We will describe two methods for determining the optimal policy.
The method we describe below, policy iteration, iteratively improves candidate policies, ultimately converging to the optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span>.
The second method, value iteration, iteratively improves an estimate of <span class="math notranslate nohighlight">\(V^*\)</span>, ultimately converging to the optimal value function.
Both, however, need access to the MDP’s transition probabilities and the reward function.</p>
<p><strong>Policy Iteration</strong> starts with an initial guess at the optimal policy, and then iteratively improves our guess until no further improvements are possible.
In particular, policy iteration generates a sequence of policies
<span class="math notranslate nohighlight">\(\pi^0, \pi^1, \dots \pi^n\)</span>, such that <span class="math notranslate nohighlight">\(\pi^{i+1}\)</span> is better than policy <span class="math notranslate nohighlight">\(\pi^i\)</span>.
This process ends when no further improvement is possible, which
occurs when <span class="math notranslate nohighlight">\(\pi^{i+1} = \pi^i.\)</span></p>
<p>To improve the policy <span class="math notranslate nohighlight">\(\pi^i\)</span>, we update the action chosen <em>for each state</em> by applying
Bellman’s equation using <span class="math notranslate nohighlight">\(\pi^i\)</span> in place of <span class="math notranslate nohighlight">\(\pi^*\)</span>.
This can be achieved with the following algorithm:</p>
<p>Start with a random policy <span class="math notranslate nohighlight">\(\pi^0\)</span> and <span class="math notranslate nohighlight">\(i=0\)</span>, and repeat until convergence:</p>
<ol class="arabic simple">
<li><p>Compute the value function <span class="math notranslate nohighlight">\(V^{\pi^i}\)</span></p></li>
<li><p>Improve the policy for each state <span class="math notranslate nohighlight">\(x \in {\cal X}\)</span> using the update rule:
\begin{equation}
\pi^{i+1}(x) \leftarrow\arg \max_a Q(x,a; V^{\pi^i})
\end{equation}</p></li>
<li><p>Increment <span class="math notranslate nohighlight">\(i\)</span></p></li>
</ol>
<p>Notice that this algorithm has the side benefit of computing
successively better approximations to the value function at each iteration.
Because there are a finite number of actions that can be applied in each state, there are only finitely many ways to update
a policy. Therefore, we expect this policy iteration algorithm to converge in finite time.</p>
<p>We already know how to do step (1) above, using the<code class="docutils literal notranslate"><span class="pre">calculate_value_function</span></code>.
The second step of the algorithm is easily implemented with the following code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_policy</span><span class="p">(</span><span class="n">value_function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update policy given a value function&quot;&quot;&quot;</span>
    <span class="n">new_policy</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">room</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
        <span class="n">Q_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q_value</span><span class="p">(</span><span class="n">value_function</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
        <span class="n">new_policy</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_values</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_policy</span>
</pre></div>
</div>
</div>
</div>
<p>The whole policy iteration algorithm then simply iterates these until the policy no longer changes. If no initial policy is given, we can
start with a zero value function
<span class="math notranslate nohighlight">\(V^{\pi^0}(x) = 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">pi</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Do policy iteration, starting from policy `pi`.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        <span class="n">value_for_pi</span> <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">calculate_value_function</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span> <span class="k">if</span> <span class="n">pi</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,))</span>
        <span class="n">new_policy</span> <span class="o">=</span> <span class="n">update_policy</span><span class="p">(</span><span class="n">value_for_pi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_policy</span> <span class="o">==</span> <span class="n">pi</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pi</span><span class="p">,</span> <span class="n">value_for_pi</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="n">new_policy</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No stable policy found after </span><span class="si">{max_iterations}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>On the other hand, if we have a guess for the initial policy, we can initialize
<span class="math notranslate nohighlight">\(\pi^0\)</span> accordingly.
For example, we can start with a not-so-smart <code class="docutils literal notranslate"><span class="pre">always_right</span></code> policy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">RIGHT</span> <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">)</span>

<span class="n">always_right</span> <span class="o">=</span> <span class="p">[</span><span class="n">RIGHT</span><span class="p">,</span> <span class="n">RIGHT</span><span class="p">,</span> <span class="n">RIGHT</span><span class="p">,</span> <span class="n">RIGHT</span><span class="p">,</span> <span class="n">RIGHT</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimal_policy</span><span class="p">,</span> <span class="n">optimal_value_function</span> <span class="o">=</span> <span class="n">policy_iteration</span><span class="p">(</span><span class="n">always_right</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">optimal_policy</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;L&#39;, &#39;L&#39;, &#39;R&#39;, &#39;U&#39;, &#39;U&#39;]
</pre></div>
</div>
</div>
</div>
<p>Starting with the <code class="docutils literal notranslate"><span class="pre">always_right</span></code> policy, our policy iteration algorithm converges to an
intuitively pleasing policy.
In the dining room and kitchen we go <code class="docutils literal notranslate"><span class="pre">left</span></code>, in the office we go <code class="docutils literal notranslate"><span class="pre">right</span></code>, and in the hallway and dining room we go <code class="docutils literal notranslate"><span class="pre">up</span></code>.
This is significantly different from the <code class="docutils literal notranslate"><span class="pre">always_right</span></code> policy (which might be better named <code class="docutils literal notranslate"><span class="pre">almost_always_wrong</span></code>).
In fact, it is exactly the <code class="docutils literal notranslate"><span class="pre">reasonable_policy</span></code> that we created in Section 3.5.
We already knew that it should be pretty good at getting to the living room as fast as possible. In fact, it is optimal!</p>
<p>We also print out the optimal value function below, which shows that if we are close to the living room the value function is very high, but it is a bit lower in the office in the dining room:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">room</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">room</span><span class="si">:</span><span class="s2">12</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">optimal_value_function</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  Living Room : 100.00
  Kitchen     : 97.56
  Office      : 85.66
  Hallway     : 97.56
  Dining Room : 85.66
</pre></div>
</div>
</div>
</div>
<p>The optimal policy is also obtained when we start without a policy, starting with a zero value function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimal_policy</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">policy_iteration</span><span class="p">()</span>
<span class="nb">print</span><span class="p">([</span><span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">optimal_policy</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;L&#39;, &#39;L&#39;, &#39;R&#39;, &#39;U&#39;, &#39;U&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="value-iteration">
<span id="index-5"></span><h2><span class="section-number">3.6.4. </span>Value Iteration<a class="headerlink" href="#value-iteration" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Dynamic programming can be used to obtain the optimal value function.</p>
</div></blockquote>
<p>Let us restate Bellman’s equation, which must hold for each state <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dcfdf1ee-61d2-4248-b8ea-2369261cbd53">
<span class="eqno">(3.62)<a class="headerlink" href="#equation-dcfdf1ee-61d2-4248-b8ea-2369261cbd53" title="Permalink to this equation">#</a></span>\[\begin{equation}
V^*(x) = \max_a  \left\{ \bar{R}(x,a) + \gamma \sum_{x'} P(x'|x, a) V^*(x')   \right\}.
\end{equation}\]</div>
<p>If we have <span class="math notranslate nohighlight">\(n\)</span> states, and since we would then have <span class="math notranslate nohighlight">\(n\)</span> equations, it seems like we should be able to solve for the <span class="math notranslate nohighlight">\(n\)</span> unknown values <span class="math notranslate nohighlight">\(V^*(x)\)</span>.
Sadly, they are not <em>linear</em> equations, as the maximization operation is not linear. Hence, unlike the case when the policy is fixed, we cannot just solve a system of linear equations to recover <span class="math notranslate nohighlight">\(V^*\)</span>.</p>
<p><strong>Value iteration</strong> approximates <span class="math notranslate nohighlight">\(V^*\)</span> by constructing a sequence of estimates,
<span class="math notranslate nohighlight">\(V^0, V^1, \dots , V^n\)</span> that converges to <span class="math notranslate nohighlight">\(V^*\)</span>.
Starting with an initial guess, <span class="math notranslate nohighlight">\(V^0\)</span>, at each iteration we update
our approximation of the value function for each state by the update rule:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0e313526-d04a-4d25-98c9-6279c35eeaaa">
<span class="eqno">(3.63)<a class="headerlink" href="#equation-0e313526-d04a-4d25-98c9-6279c35eeaaa" title="Permalink to this equation">#</a></span>\[\begin{equation}
V^{i+1}(x) \leftarrow \max_a \left\{ \bar{R}(x,a) + \gamma \sum_{x'} P(x'|x, a) V^i(x')   \right\} 
\end{equation}\]</div>
<p>Notice that the right hand side includes two terms:
the expected reward (which we can compute exactly), and a term in <span class="math notranslate nohighlight">\(V^i\)</span> (our current best guess at the value function).
Value iteration operates by iteratively using our <em>current best guess</em> <span class="math notranslate nohighlight">\(V^i\)</span> along with the <em>known</em> expected reward to update the approximation.
Unlike policy iteration, we do not expect value iteration to converge to the exact result in finite time.
Therefore, we cannot use <span class="math notranslate nohighlight">\(V^{i+1} = V^i\)</span> as our termination condition.
Instead, we often use a condition such as <span class="math notranslate nohighlight">\(|V^{i+1} - V^i| &lt; \epsilon\)</span>, for some small value of <span class="math notranslate nohighlight">\(\epsilon\)</span>
as the termination condition.</p>
<p>Finally, note that we can once again use the Q-values to obtain a concise description for the value update:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e7fe47ec-9661-43d0-8d52-f92e1d58a605">
<span class="eqno">(3.64)<a class="headerlink" href="#equation-e7fe47ec-9661-43d0-8d52-f92e1d58a605" title="Permalink to this equation">#</a></span>\[\begin{equation}
V^{i+1}(x) \leftarrow \max_a Q(x, a; V^i).
\end{equation}\]</div>
<p>In code, this is actually easier than policy iteration, using the concise vectorized Q-table update we discussed above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">5</span><span class="p">,),</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">Q_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">V_k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 5 x 4</span>
    <span class="n">V_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># max over actions</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">V_k</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100.  98.  90.  98.  90.]
[100.    97.64  86.76  97.64  86.76]
[100.    97.58  85.92  97.58  85.92]
[100.    97.56  85.72  97.56  85.72]
[100.    97.56  85.68  97.56  85.68]
[100.    97.56  85.67  97.56  85.67]
[100.    97.56  85.66  97.56  85.66]
[100.    97.56  85.66  97.56  85.66]
[100.    97.56  85.66  97.56  85.66]
[100.    97.56  85.66  97.56  85.66]
</pre></div>
</div>
</div>
</div>
<p>Compare with optimal value function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">optimal_value_function</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100.    97.56  85.66  97.56  85.66]
</pre></div>
</div>
</div>
</div>
<p>And we can easily <em>extract</em> the optimal policy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Q_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">V_k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pi_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;policy = </span><span class="si">{</span><span class="n">pi_k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">pi_k</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>policy = [0 0 1 2 0]
[&#39;L&#39;, &#39;L&#39;, &#39;R&#39;, &#39;U&#39;, &#39;L&#39;]
</pre></div>
</div>
</div>
</div>
<section id="id2">
<h3><span class="section-number">3.6.4.1. </span>Exercise<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Above we initialized the value function at 100 everywhere. Examine the effect on convergence of initializing it differently.</p></li>
<li><p>Implement a convergence criterion that stops the iterations after convergence.</p></li>
</ol>
</section>
</section>
<section id="model-based-reinforcement-learning">
<h2><span class="section-number">3.6.5. </span>Model-based Reinforcement Learning<a class="headerlink" href="#model-based-reinforcement-learning" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Just explore, then solve the MDP.</p>
</div></blockquote>
<p>We can attempt to <em>learn</em> the MDP and then solve it. Both policy and value iteration require access to the transition probabilities <span class="math notranslate nohighlight">\(T\)</span> and the reward function <span class="math notranslate nohighlight">\(R\)</span>. However, when faced with a new environment, we might not know how our robot will behave. And likewise, we might not have access to the reward function: how can we know in advance where we will find pots of gold?</p>
<p>One way to learn the MDP is to randomly explore. Let’s adapt the <code class="docutils literal notranslate"><span class="pre">policy_rollout</span></code> code from the previous section to generate a whole lot of <em>experiences</em> of the form <span class="math notranslate nohighlight">\((x,a,x',r)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">explore_randomly</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="n">N</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Roll out states given a random policy, for given horizon.&quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x1</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">horizon</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">next_state_distribution</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">DiscreteDistribution</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">])</span>
        <span class="n">x_prime</span> <span class="o">=</span> <span class="n">next_state_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">,</span> <span class="n">R</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">]))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_prime</span>
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<p>Let us use it to create 499 experiences and show the first 5:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">explore_randomly</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;Living Room&quot;</span><span class="p">),</span> <span class="n">horizon</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0, 2, 0, 10.0), (0, 0, 0, 10.0), (0, 2, 0, 10.0), (0, 0, 0, 10.0), (0, 0, 0, 10.0)]
</pre></div>
</div>
</div>
</div>
<p>We can <em>estimate</em> the transition probabilities <span class="math notranslate nohighlight">\(T\)</span> and reward table <span class="math notranslate nohighlight">\(R\)</span> from the data, and then we can use the algorithms from before to calculate the value function and/or optimal policy.</p>
<p>The math is just a variant of what we saw in the learning section of the last chapter. The rewards are the easiest to estimate:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f0a30fc5-25b0-41f9-9d63-94012fa749a3">
<span class="eqno">(3.65)<a class="headerlink" href="#equation-f0a30fc5-25b0-41f9-9d63-94012fa749a3" title="Permalink to this equation">#</a></span>\[\begin{equation}
R(x,a,x') \approx \frac{1}{N(x,a,x')} \sum_{x,a,x'} r
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(N(x,a,x')\)</span> counts how many times an experience <span class="math notranslate nohighlight">\((x,a,x')\)</span> was recorded. The transition probabilities are a bit trickier:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f93705a9-02b0-497b-8ac0-069cd4a98d97">
<span class="eqno">(3.66)<a class="headerlink" href="#equation-f93705a9-02b0-497b-8ac0-069cd4a98d97" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(x'|x,a) \approx \frac{N(x,a,x)}{N(x,a)}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(N(x,a)=\sum_{x'} N(x,a,x')\)</span> is the number of times we took action <span class="math notranslate nohighlight">\(a\)</span> in a state <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>The code associated with that is fairly simple, modulo some numpy trickery to deal with division by zero and <em>broadcasting</em> the division:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">R_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="nb">float</span><span class="p">)</span>
<span class="n">T_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="nb">float</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">R_sum</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">]</span> <span class="o">+=</span> <span class="n">r</span>
    <span class="n">T_count</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">R_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">R_sum</span><span class="p">,</span> <span class="n">T_count</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">T_count</span><span class="o">!=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">xa_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T_count</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">T_estimate</span> <span class="o">=</span> <span class="n">T_count</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">xa_count</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Above <code class="docutils literal notranslate"><span class="pre">T_count</span></code> corresponds to <span class="math notranslate nohighlight">\(N(x,a,x')\)</span>, and the variable <code class="docutils literal notranslate"><span class="pre">xa_count</span></code> is <span class="math notranslate nohighlight">\(N(x,a)\)</span>. It is good to check the latter to see whether our experiences were more or less representative, i.e., visited all state-action pairs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xa_count</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[23., 20., 29., 27.],
       [22., 28., 29., 29.],
       [16., 22., 22., 19.],
       [18., 26., 26., 29.],
       [27., 28., 33., 26.]])
</pre></div>
</div>
</div>
</div>
<p>This seems pretty good. If not, we can always gather more data, which we encourage you to experiment with.</p>
<p>We can compare the ground truth transition probabilities <span class="math notranslate nohighlight">\(T\)</span> with the estimated transition probabilities <span class="math notranslate nohighlight">\(\hat{T}\)</span>, e.g., for the living room:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ground truth:</span><span class="se">\n</span><span class="si">{</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;estimate:</span><span class="se">\n</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">T_estimate</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ground truth:
[[1.  0.  0.  0.  0. ]
 [0.2 0.8 0.  0.  0. ]
 [1.  0.  0.  0.  0. ]
 [0.2 0.  0.  0.8 0. ]]
estimate:
[[1.   0.   0.   0.   0.  ]
 [0.15 0.85 0.   0.   0.  ]
 [1.   0.   0.   0.   0.  ]
 [0.3  0.   0.   0.7  0.  ]]
</pre></div>
</div>
</div>
</div>
<p>Not bad. And for the rewards:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ground truth:</span><span class="se">\n</span><span class="si">{</span><span class="n">R</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;estimate:</span><span class="se">\n</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">R_estimate</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ground truth:
[[10.  0.  0.  0.  0.]
 [10.  0.  0.  0.  0.]
 [10.  0.  0.  0.  0.]
 [10.  0.  0.  0.  0.]]
estimate:
[[10.   0.  77.1 87.8 77.1]
 [10.   0.  77.1 87.8 77.1]
 [10.  87.8 77.1 87.8 77.1]
 [10.  87.8 77.1  0.  77.1]]
</pre></div>
</div>
</div>
</div>
<p>In summary, learning in this context can simply be done by gathering lots of experiences, and estimating models for how the world behaves. After that, you can use either policy or value iteration to recover the optimal policy.</p>
</section>
<section id="model-free-reinforcement-learning">
<h2><span class="section-number">3.6.6. </span>Model-free Reinforcement Learning<a class="headerlink" href="#model-free-reinforcement-learning" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>All you need is Q.</p>
</div></blockquote>
<p>A different, model-free approach is <strong>Q-learning</strong>. In the above we tried to <em>model</em> the world by trying estimate the (large) transition and reward tables. However, remember from the previous section that there is a much smaller table of Q-values <span class="math notranslate nohighlight">\(Q(x,a)\)</span> that also allow us to act optimally. This is because we can calculate the optimal policy <span class="math notranslate nohighlight">\(\pi^*(x)\)</span> from the optimal Q-values <span class="math notranslate nohighlight">\(Q^*(x,a) \doteq Q(x, a; V^*)\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-60af77c8-15ce-4965-94fe-d3503281f97f">
<span class="eqno">(3.67)<a class="headerlink" href="#equation-60af77c8-15ce-4965-94fe-d3503281f97f" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pi^*(x) = \arg \max_a Q^*(x,a).
\end{equation}\]</div>
<p>This begs the question whether we can simply learn the Q-values instead, which might be more <em>sample-efficient</em>. In other words, we would get more accurate values with less training data, as we have less quantities to estimate.</p>
<p>To do this, recall that the Bellman equation can be written as</p>
<div class="amsmath math notranslate nohighlight" id="equation-8ed6c370-d813-40f0-83b7-300c18f0bfa2">
<span class="eqno">(3.68)<a class="headerlink" href="#equation-8ed6c370-d813-40f0-83b7-300c18f0bfa2" title="Permalink to this equation">#</a></span>\[\begin{equation}
V^*(x) = \max_a Q^*(x,a)
\end{equation}\]</div>
<p>allowing us to rewrite the Q-values from above as</p>
<div class="amsmath math notranslate nohighlight" id="equation-a343406c-c2bd-470f-9f0e-6220b45dd640">
<span class="eqno">(3.69)<a class="headerlink" href="#equation-a343406c-c2bd-470f-9f0e-6220b45dd640" title="Permalink to this equation">#</a></span>\[\begin{equation}
Q^*(x,a) = \sum_{x'} P(x'|x, a) \{ R(x,a,x') + \gamma \max_{a'} Q^*(x',a') \}
\end{equation}\]</div>
<p>This gives us a way to estimate the Q-values, as we can approximate the above using a Monte Carlo estimate, summing over our experiences:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a8fa4c4f-4d6a-4c27-8ba4-f01ef14c46ee">
<span class="eqno">(3.70)<a class="headerlink" href="#equation-a8fa4c4f-4d6a-4c27-8ba4-f01ef14c46ee" title="Permalink to this equation">#</a></span>\[\begin{equation}
Q^*(x,a) \approx \frac{1}{N(x,a)} \sum_{x,a,x'} R(x,a,x') + \gamma \max_{a'} Q^*(x',a')
\end{equation}\]</div>
<p>Unfortunately the estimate above <em>depends</em> on the optimal Q-values. Hence, the final Q-learning algorithm applies this estimate gradually, by “alpha-blending” between old and new estimates, which also averages over the reward:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b997e758-93a4-470b-8413-bf2d341d37dd">
<span class="eqno">(3.71)<a class="headerlink" href="#equation-b997e758-93a4-470b-8413-bf2d341d37dd" title="Permalink to this equation">#</a></span>\[\begin{equation}
\hat{Q}(x,a) \leftarrow (1-\alpha) \hat{Q}(x,a) + \alpha \{R(x,a,x') +  \gamma \max_{a'} \hat{Q}(x',a') \}
\end{equation}\]</div>
<p>In code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># learning rate</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="c1"># discount factor</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="nb">float</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">old_Q_estimate</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">]</span>
    <span class="n">new_Q_estimate</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">x_prime</span><span class="p">])</span>
    <span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">old_Q_estimate</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">new_Q_estimate</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[85.02142229 67.60715011 82.85280985 76.55935426]
 [77.16903571 66.59521035 64.44902059 61.12897595]
 [46.27550533 66.69025684 49.90723269 40.9381659 ]
 [57.91923184 57.9713762  82.0101778  69.94736352]
 [67.55202159 53.14335446 68.65284257 55.62331342]]
</pre></div>
</div>
</div>
</div>
<p>These values are not yet quite accurate, as you can ascertain yourself by changing the number of experiences above, but note that an optimal policy can be achieved before we even converge.</p>
<section id="exploration-vs-exploitation">
<span id="index-6"></span><h3><span class="section-number">3.6.6.1. </span>Exploration vs Exploitation<a class="headerlink" href="#exploration-vs-exploitation" title="Link to this heading">#</a></h3>
<p>The above assumed that we gather data by acting <em>randomly</em>, but that might be very inefficient. Indeed, we might be spending a lot of time - literally - bumping our heads into the walls. A better idea might be to act randomly at first (exploration), but as time progresses, spend more and more time refining the optimal policy by trying to act optimally (exploitation).</p>
<p>Greedy action selection can lead to bad learning outcomes. We will use Q-learning as an example, but similar problems exist for other reinforcement learning methods. During Q-learning, upon reaching a state <span class="math notranslate nohighlight">\(x\)</span>, the <strong>greedy action selection</strong> method is to simply pick the action <span class="math notranslate nohighlight">\(a^*\)</span> according to the <em>current</em> estimate of the Q-values:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b15802a9-543c-4a9e-a095-cd84927f32a1">
<span class="eqno">(3.72)<a class="headerlink" href="#equation-b15802a9-543c-4a9e-a095-cd84927f32a1" title="Permalink to this equation">#</a></span>\[\begin{equation}
a^* = \arg \max_a \hat{Q}(x,a).
\end{equation}\]</div>
<p>Unfortunately, this tends to often lead to Q-learning getting stuck in local minima of the policy search space: state-action pairs that might be more promising are never visited as their correct (higher) Q-values have not been estimated correctly, so they always get passed over.</p>
<p>Epsilon-greedy or <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy methods balance exploration with exploitation while learning. Instead of always choosing the best possible action according to the current estimate, we could simply choose an action at random a fraction of the time, say with probability <span class="math notranslate nohighlight">\(\epsilon\)</span>. This is the <strong>epsilon-greedy</strong> method. Typical values for <span class="math notranslate nohighlight">\(\epsilon\)</span> are 0.01 or even 0.1, i.e., 10% of the time we choose to act randomly. Schemes also exist to decrease <span class="math notranslate nohighlight">\(\epsilon\)</span> over time.</p>
</section>
<section id="id3">
<h3><span class="section-number">3.6.6.2. </span>Exercise<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Think about how to apply <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy methods in the model-based reinforcement learning method we discussed above.</p>
</section>
</section>
<section id="summary">
<h2><span class="section-number">3.6.7. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>We discussed</p>
<ul class="simple">
<li><p>The optimal policy and value function, governed by the Bellman equation.</p></li>
<li><p>Two algorithms to compute those: policy iteration and value iteration.</p></li>
<li><p>A model-based method to learn from experience.</p></li>
<li><p>A model-free method, Q-learning, that updates the action values.</p></li>
<li><p>Balancing exploitation and exploration.</p></li>
</ul>
<p>The field of reinforcement learning is much richer, and we will return to  it several times throughout this book.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="S35_vacuum_decision.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3.5. </span>Markov Decision Processes</p>
      </div>
    </a>
    <a class="right-next"
       href="S37_vacuum_summary.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3.7. </span>Chapter Summary</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-optimal-value-function">3.6.1. The Optimal Value Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#action-values-and-the-optimal-policy">3.6.2. Action Values and the Optimal Policy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">3.6.2.1. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">3.6.3. Policy Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">3.6.4. Value Iteration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.6.4.1. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-reinforcement-learning">3.6.5. Model-based Reinforcement Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-reinforcement-learning">3.6.6. Model-free Reinforcement Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation">3.6.6.1. Exploration vs Exploitation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">3.6.6.2. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.6.7. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank Dellaert and Seth Hutchinson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>