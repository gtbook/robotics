
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6.4. SLAM &#8212; Introduction to Robotics and Perception</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=51e3b7cf" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-312077-7"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'UA-312077-7');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'UA-312077-7');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'S64_driving_perception';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6.5. Planning for Autonomous Driving." href="S65_driving_planning.html" />
    <link rel="prev" title="6.3. Sensing for Autonomous Vehicles" href="S63_driving_sensing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Introduction to Robotics and Perception - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Introduction to Robotics and Perception - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction to Robotics and Perception
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="S10_introduction.html">1. Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S11_models.html">1.1. Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S12_reasoning.html">1.2. Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S13_math.html">1.3. The Mathematics of Robotics</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S20_sorter_intro.html">2. A Trash Sorting Robot</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S21_sorter_state.html">2.1. Modeling the World State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S22_sorter_actions.html">2.2. Actions for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S23_sorter_sensing.html">2.3. Sensors for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S24_sorter_perception.html">2.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S25_sorter_decision_theory.html">2.5. Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="S26_sorter_learning.html">2.6. Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S30_vacuum_intro.html">3. A Robot Vacuum Cleaner</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S31_vacuum_state.html">3.1. Modeling the State of the Vacuum Cleaning Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S32_vacuum_actions.html">3.2. Actions over time</a></li>
<li class="toctree-l2"><a class="reference internal" href="S33_vacuum_sensing.html">3.3. Dynamic Bayes Nets</a></li>
<li class="toctree-l2"><a class="reference internal" href="S34_vacuum_perception.html">3.4. Perception with Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S35_vacuum_decision.html">3.5. Markov Decision Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="S36_vacuum_RL.html">3.6. Reinforcement Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S40_logistics_intro.html">4. Warehouse Robots in 2D</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S41_logistics_state.html">4.1. Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S42_logistics_actions.html">4.2. Moving in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="S43_logistics_sensing.html">4.3. Sensor Models with Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S44_logistics_perception.html">4.4. Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S45_logistics_planning.html">4.5. Planning for Logistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="S46_logistics_learning.html">4.6. Some System Identification</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S50_diffdrive_intro.html">5. A Mobile Robot With Simple Kinematics</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S51_diffdrive_state.html">5.1. State Space for a Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S52_diffdrive_actions.html">5.2. Motion Model for the Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S53_diffdrive_sensing.html">5.3. Robot Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="S54_diffdrive_perception.html">5.4. Computer Vision 101</a></li>
<li class="toctree-l2"><a class="reference internal" href="S55_diffdrive_planning.html">5.5. Path Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S56_diffdrive_learning.html">5.6. Deep Learning</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="S60_driving_intro.html">6. Autonomous Vehicles</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="S61_driving_state.html">6.1. Planar Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="S62_driving_actions.html">6.2. Kinematics for Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S63_driving_sensing.html">6.3. Sensing for Autonomous Vehicles</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">6.4. SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S65_driving_planning.html">6.5. Planning for Autonomous Driving.</a></li>
<li class="toctree-l2"><a class="reference internal" href="S66_driving_DRL.html">6.6. Deep Reinforcement Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S70_drone_intro.html">7. Autonomous Drones in 3D</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S71_drone_state.html">7.1. Moving in Three Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S72_drone_actions.html">7.2. Multi-rotor Aircraft</a></li>
<li class="toctree-l2"><a class="reference internal" href="S73_drone_sensing.html">7.3. Sensing for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S74_drone_perception.html">7.4. Visual SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S75_drone_planning.html">7.5. Trajectory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S76_drone_learning.html">7.6. Neural Radiance Fields for Drones</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gtbook/robotics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS64_driving_perception.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/S64_driving_perception.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>SLAM</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poses-in-2d-and-3d">6.4.1. Poses in 2D and 3D</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-iterative-closest-points-algorithm">6.4.2. The Iterative Closest Points Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-closest-points">6.4.2.1. Finding Closest Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-pairwise-transform">6.4.2.2. Estimating the Pairwise Transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poseslam">6.4.3. PoseSLAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-poseslam-factor-graph">6.4.4. The PoseSLAM Factor Graph</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#map-inference-via-least-squares">6.4.5. MAP Inference via Least-Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poseslam-is-nonlinear">6.4.6. PoseSLAM is Nonlinear !</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-optimization-for-poseslam">6.4.7. Nonlinear Optimization for PoseSLAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-gtsam">6.4.8. Optimization with GTSAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slam-with-landmarks">6.4.9. SLAM with Landmarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-larger-slam-example">6.4.10. A Larger SLAM Example</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/master/S64_driving_perception.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="slam">
<h1><span class="section-number">6.4. </span>SLAM<a class="headerlink" href="#slam" title="Link to this heading">#</a></h1>
<blockquote>
<div><p>SLAM is <em>Simultaneous Localization and Mapping</em>, a key capability for mobile robots.</p>
</div></blockquote>
<a class="reference internal image-reference" href="_images/S64-Autonomous_Vehicle_with_LIDAR_and_cameras-03.jpg"><img alt="Splash image with steampunk autonomous car" class="align-center" src="_images/S64-Autonomous_Vehicle_with_LIDAR_and_cameras-03.jpg" style="width: 40%;" /></a>
<p>When an autonomous vehicle explores a new environment, it should be able to simultaneously build a map of this environment, and localize itself relative to that map. That capability is known as <em>Simultaneous Localization and Mapping</em> or <strong>SLAM</strong>. SLAM is a bit like a chicken and egg problem: if we knew the map, we could use the localization techniques we studied in Section 4.4, e.g., Monte Carlo Localization. Conversely, if we were already localized, it would not be too hard to create a map: if we have a LIDAR sensor, we could simply transform the points from the LIDAR frame into the world frame. But now we have to do <em>both</em> at the same time! Is that even possible?</p>
<p>In this section we show that this <em>is</em> possible, and we expand upon two different SLAM techniques: <em>PoseSLAM</em> and <em>SLAM with landmarks</em>. In the former we concentrate on estimating the robot’s motion from a rich sensor such as LIDAR, and building the map is generated as a side effect. In the latter, we truly optimize for both the location of the vehicle and the location of a observed landmarks, which leads to a sparse map of the environment. Before investigating these two SLAM algorithms, we review the math of vehicle poses in 2D, and discuss ICP, a seminal algorithm for aligning two point clouds, and which we use as a building block in PoseSLAM.</p>
<section id="poses-in-2d-and-3d">
<h2><span class="section-number">6.4.1. </span>Poses in 2D and 3D<a class="headerlink" href="#poses-in-2d-and-3d" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>SE(2) generalizes to SE(3).</p>
</div></blockquote>
<p>In both SLAM variants we need to optimize over the pose of a vehicle.
To keep things simple, we will concentrate on poses in the plane for now.
Recall from Section 6.1 that a 2D pose
<span class="math notranslate nohighlight">\(T\doteq(x,y,\theta)\)</span> is an element of the Special Euclidean group <span class="math notranslate nohighlight">\(SE(2)\)</span>,
and
can be represented by a <span class="math notranslate nohighlight">\(3\times3\)</span> matrix of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
T=\left[\begin{array}{cc|c}
\cos\theta &amp; -\sin\theta &amp; x\\
\sin\theta &amp; \cos\theta &amp; y\\
\hline 0 &amp; 0 &amp; 1
\end{array}\right]=\left[\begin{array}{cc}
R &amp; t\\
0 &amp; 1
\end{array}\right]
\end{split}\]</div>
<p>with the <span class="math notranslate nohighlight">\(2\times1\)</span> vector <span class="math notranslate nohighlight">\(t\)</span>
representing the position of the vehicle, and <span class="math notranslate nohighlight">\(R\)</span> the <span class="math notranslate nohighlight">\(2\times2\)</span>
rotation matrix representing the vehicle’s orientation in the plane.</p>
<p>Below it is important to understand what coordinate frames are involved, and hence we insist on always annotating the transformation matrices with indices indicating the reference frame as superscript and the frame under consideration using subscript. This convention also
provides a reminder to how points in one frame are transformed into the other.
As an example, the following illustrates how points in the frame <span class="math notranslate nohighlight">\(b\)</span> are transformed into the reference frame <span class="math notranslate nohighlight">\(a\)</span> by referring to the pose <span class="math notranslate nohighlight">\(T^a_b\)</span> of frame <span class="math notranslate nohighlight">\(b\)</span> in <span class="math notranslate nohighlight">\(a\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P^a = \left[\begin{array}{cc}
R^a_b &amp; t^a_b\\
0 &amp; 1
\end{array}\right] P^b = T^a_b P^b 
\end{split}\]</div>
<p>A good rule to keep in mind, illustrated above, is that the superscript <span class="math notranslate nohighlight">\(b\)</span> of the point <span class="math notranslate nohighlight">\(P^b\)</span> on the right-hand side is “canceled” by the subscript <span class="math notranslate nohighlight">\(b\)</span> in the pose <span class="math notranslate nohighlight">\(T^a_b\)</span>, and so the left-hand side <span class="math notranslate nohighlight">\(P^a\)</span> only retains the superscript <span class="math notranslate nohighlight">\(a\)</span>, indicating it now lives in the reference frame <span class="math notranslate nohighlight">\(a\)</span>. This rule is quite useful when implementing these types of transformations in code, as well. Also, keep in mind that we are talking about the <em>same</em> point <span class="math notranslate nohighlight">\(P\)</span> in both cases: it is simply the <em>coordinates</em> that change by expressing them in a different frame.</p>
<p>Note that this representation generalizes easily to three dimensions,
but of course <span class="math notranslate nohighlight">\(t^a_b\)</span> will be a three-vector, and <span class="math notranslate nohighlight">\(R^a_b\)</span> will be a <span class="math notranslate nohighlight">\(3\times3\)</span>
rotation matrix representing the 3DOF orientation of the vehicle. The
latter can be decomposed into roll, pitch, and yaw, if so desired,
but we will not need that until the next chapter.</p>
</section>
<section id="the-iterative-closest-points-algorithm">
<h2><span class="section-number">6.4.2. </span>The Iterative Closest Points Algorithm<a class="headerlink" href="#the-iterative-closest-points-algorithm" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>ICP is a seminal method to align two point clouds.</p>
</div></blockquote>
<p><strong>Iterative closest points</strong> or <strong>ICP</strong> is a method to align two point clouds, e.g., two successive LIDAR scans, and it is a crucial component in the PoseSLAM algorithm we will discuss next. Leaning into the notation re-introduced above, we use superscripts <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> to distinguish the two point clouds, and the points therein. Under the assumption that we have a good initial estimate <span class="math notranslate nohighlight">\(\hat{T^a_b}\)</span> for the relative pose <span class="math notranslate nohighlight">\(T^a_b\)</span> between the two point clouds, ICP iterates between two steps:</p>
<ol class="arabic simple">
<li><p>Find closest point correspondences between the two clouds.</p></li>
<li><p>Use these point correspondences to re-estimate the relative pose <span class="math notranslate nohighlight">\(\hat{T^a_b}\)</span> between the two point clouds.</p></li>
</ol>
<p>These two steps are iterated until convergence, hence the name <em>iterated</em> closest points. Below we  explain both steps in more detail.</p>
<section id="finding-closest-points">
<h3><span class="section-number">6.4.2.1. </span>Finding Closest Points<a class="headerlink" href="#finding-closest-points" title="Link to this heading">#</a></h3>
<p>The first step is the easiest: for each point <span class="math notranslate nohighlight">\(P^a_j\)</span> in the first point cloud, we define <span class="math notranslate nohighlight">\(P^b_j\)</span> to be its closest point in the second point cloud. Stated formally, for each point <span class="math notranslate nohighlight">\(P^a_j\)</span>, we solve the following minimization problem:</p>
<div class="math notranslate nohighlight">
\[
P^b_j = \arg \min_{P^b_k} \| P^b_k - P^a_j\|^2
\]</div>
<p>We prefer using indices <span class="math notranslate nohighlight">\(j\)</span> for points as we use <span class="math notranslate nohighlight">\(i\)</span> for poses below, making it easier to associate notation with geometric concepts. Two things about the above minimization are worth noting:
(a) it can be solved using a linear search over all points <span class="math notranslate nohighlight">\(P^b\)</span>,
and (b) rather than computing the distance we can save the computation of a square root by minimizing the square, which is just as good as they are monotonically related.</p>
<p>The linear search problem above is known as the <strong>nearest neighbor</strong> problem, and solving this problem for all points is the <strong>all nearest neighbors</strong> problem.
Iterating over all points in the second cloud can be quite slow, and indeed finding all nearest neighbors that way has quadratic complexity. However, very fast <em>approximate</em> nearest neighbor algorithms are available. Many of these use specialized data structures, such as “KD-trees” or “Oct-trees” (in 3D). While the details are beyond the scope of this book,
intuitively these data structures  recursively divide up the point clouds into sub-clouds, such that sub-clouds unlikely to contain the nearest neighbor can be quickly excluded. We build this data structure once for the second cloud, and then use it for all nearest neighbor searches, leading to complexity which is approximately <span class="math notranslate nohighlight">\(O(N \log N\)</span>),
for point clouds of size <span class="math notranslate nohighlight">\(N\)</span>.</p>
</section>
<section id="estimating-the-pairwise-transform">
<h3><span class="section-number">6.4.2.2. </span>Estimating the Pairwise Transform<a class="headerlink" href="#estimating-the-pairwise-transform" title="Link to this heading">#</a></h3>
<p>The second step is the more interesting one: given a set of closest point pairs <span class="math notranslate nohighlight">\(\{(P^a_j, P^b_j)\}\)</span>, how can we estimate the relative pose <span class="math notranslate nohighlight">\(\widehat{T^a_b}\)</span> between the two point clouds? This is known as the <strong>pose alignment</strong> problem.</p>
<p>Let us first assume that the two point clouds only differ by a rotation <span class="math notranslate nohighlight">\(R^a_b\)</span>. When this is the case, and assuming we have corresponding points <span class="math notranslate nohighlight">\(P^a\)</span> and <span class="math notranslate nohighlight">\(P^b\)</span>, then each point <span class="math notranslate nohighlight">\(P^a\)</span> in the first cloud can be expressed as a function of a point <span class="math notranslate nohighlight">\(P^b\)</span> in the second cloud:</p>
<div class="math notranslate nohighlight">
\[
P^a = R^a_b P^b
\]</div>
<p>One might be tempted to think that therefore</p>
<div class="math notranslate nohighlight">
\[
R^a_b = P^a (P^b)^{-1}
\]</div>
<p>but, of course, there since <span class="math notranslate nohighlight">\(P^b\)</span> is a vector, it does not have an inverse. So this would not work.
Interestingly, though, if we form the matrix</p>
<div class="math notranslate nohighlight">
\[
H = \sum_j P^a_j (P^b_j)^T
\]</div>
<p>by summing over at least 3 point pairs <span class="math notranslate nohighlight">\((P^a_j, P^b_j)\)</span>, it turns out that the rotation matrix <span class="math notranslate nohighlight">\(\widehat{R^a_b}\)</span> closest to <span class="math notranslate nohighlight">\(H\)</span> in the least squares sense <em>is</em> the best possible estimate for the unknown rotation <span class="math notranslate nohighlight">\(R^a_b\)</span>. In addition, using the <em>singular value decomposition</em> from linear algebra, it is <em>very</em> easy to compute.
The singular value decomposition of the matrix <span class="math notranslate nohighlight">\(H\)</span> is defined by <span class="math notranslate nohighlight">\(H=U\Lambda V^T\)</span>, in which <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal
matrix of singular values, and both <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are orthogonal matrices.
The optimal estimate for the rotation matrix is given by</p>
<div class="math notranslate nohighlight">
\[
\widehat{R^a_b} = U V^T
\]</div>
<p>Interesting aside: this problem is known as the <em>orthogonal Procrustes problem</em> and its solution via SVD has been known since 1966, from <a class="reference external" href="https://doi.org/10.1007%2FBF02289451">a paper by Peter Schönemann</a>. Even more interesting is that Procrustes was a mythological greek villain who either stretched or cut his victims to size to fit their bed, analogous to how we make the rotation matrix above fit the data.</p>
<p>The above solves the problem when there is only rotation, but we must also consider relative translation between the two point clouds.
Fortunately, it turns out that the best possible translation estimate will always align the <em>centroids</em> of the two point clouds. Hence, when there is translation present, we simply compute the matrix <span class="math notranslate nohighlight">\(H\)</span> from the <em>centered</em> points,</p>
<div class="math notranslate nohighlight">
\[
H = \sum_j (P^a_j-C^a) (P^b_j-C^b)^T
\]</div>
<p>where the point cloud centroids <span class="math notranslate nohighlight">\(C^a\)</span> and <span class="math notranslate nohighlight">\(C^b\)</span> are computed as</p>
<div class="math notranslate nohighlight">
\[
C^a = \frac{1}{N} \sum_j P^a_j\text{    and    }C^b = \frac{1}{N} \sum_j P^b_j.
\]</div>
<p>Given the estimated rotation <span class="math notranslate nohighlight">\(\widehat{R^a_b}\)</span>, the translation estimate <span class="math notranslate nohighlight">\(\widehat{t^a_b}\)</span> can then be estimated from</p>
<div class="math notranslate nohighlight">
\[
C^a = \widehat{R^a_b} C^b + \widehat{t^a_b},
\]</div>
<p>and the final relative pose estimate is given by <span class="math notranslate nohighlight">\(\widehat{T^a_b} =(\widehat{R^a_b}, \widehat{t^a_b})\)</span>. By the way, all of the above math is identical for both the 2D and 3D case.</p>
</section>
</section>
<section id="poseslam">
<h2><span class="section-number">6.4.3. </span>PoseSLAM<a class="headerlink" href="#poseslam" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>PoseSLAM is SLAM with pose priors and relative pose constraints only. We can derive those from Iterative Closest Points (ICP).</p>
</div></blockquote>
<p>As we discussed at the beginning of this section, in SLAM the goal is to localize a robot using the information coming
from the robot’s sensors, while simultaneously building the map.
We have already covered the localization problem in chapter 4,
using Markov localization, Monte Carlo localization, and the Kalman filter.
The additional wrinkle in SLAM is that we do
<em>not</em> know the map a priori, and hence we have to infer the unknown map
while simultaneously estimating the robot’s location with respect to the evolving map.</p>
<p><strong>PoseSLAM</strong> is a variant of SLAM that uses pose constraints (using an algorithm such as ICP) as a
basic building block when optimizing over the unknown vehicle
poses.
In PoseSLAM we do <em>not</em> explicitly optimize over a map.
Instead the map is reconstructed after the fact.
This can only work if (a) the sensor is sufficiently rich to make the pose constraints very accurate,
and (b) we can build a sufficiently good map using this sensor data. LIDAR satisfies both of these assumptions,
making LIDAR-based PoseSLAM a very popular technique.</p>
<p>In a nutshell, the PoseSLAM problem can be stated as:</p>
<blockquote>
<div><p>Given a set of noisy relative measurements or <strong>pose constraints</strong>
<span class="math notranslate nohighlight">\(\tilde{T}_{ij}\)</span>, recover the optimal set of poses <span class="math notranslate nohighlight">\(T_{i}^{*}\)</span> that
maximizes the posteriori probability, i.e., recover the MAP solution.</p>
</div></blockquote>
<p>In the case of mapping for autonomous driving, these relative
measurements can be derived by performing ICP between overlapping
LIDAR scans. If available, we can additionally use GPS and/or IMU measurements
to decide which scans overlap, so that we do not have to do this <span class="math notranslate nohighlight">\(O(n^{2})\)</span> times.
Depending on the situation, we can optimize for 3D or 2D poses, in the way we will
discus below. Afterwards, we can reconstruct a detailed map by
transforming the local LIDAR scans into the world frame, using the
optimized poses <span class="math notranslate nohighlight">\(T_{i}^{*}\)</span>.</p>
</section>
<section id="the-poseslam-factor-graph">
<h2><span class="section-number">6.4.4. </span>The PoseSLAM Factor Graph<a class="headerlink" href="#the-poseslam-factor-graph" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Factor graphs expose the sparse set of constraints tying absolute poses together.</p>
</div></blockquote>
<p>In our factor-graph-based view of the world, a pose constraint is
represented as a factor. As before, the factor graph represent the
posterior distribution over the unknown pose variables
<span class="math notranslate nohighlight">\(\mathcal{T}=\{X_{1}\dots X_{5}\}\)</span> given the known measurements:</p>
<div class="math notranslate nohighlight">
\[
\phi(\mathcal{T})=\prod_{i}\phi_{i}(\mathcal{T}_{i}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{T}_{i}\)</span> is the set of poses involved with factor <span class="math notranslate nohighlight">\(\phi_i\)</span>.
In this way, the factor graph encodes which factors are connected to which variables,
exposing the sparsity pattern of the corresponding estimation problem.</p>
<figure id="fig:PoseSLAMFG">
<img src="https://github.com/gtbook/robotics/blob/main/Figures6/PoseSLAM-FG.png?raw=1" style="width:60.0%" /><figcaption>PoseSLAM factor graph example.</figcaption>
</figure>
<p>An example is shown in Figure
<a href="#fig:PoseSLAMFG" data-reference-type="ref" data-reference="fig:PoseSLAMFG">1</a>.
The example represents a vehicle driving around, and taking LIDAR scans
at 5 different world poses, represented by <span class="math notranslate nohighlight">\(T_{1}\)</span> to <span class="math notranslate nohighlight">\(T_{5}\)</span>.
The factors <span class="math notranslate nohighlight">\(f_{1}\)</span> to <span class="math notranslate nohighlight">\(f_{4}\)</span> are binary factors representing the pose
constraints obtained by matching successive LIDAR scans using ICP.</p>
<p>The factor
<span class="math notranslate nohighlight">\(f_{5}(T_{5},T_{2})\)</span> is a so-called <strong>loop closure</strong> constraint:
rather than derived from two successive scans, this one is derived from
matching the scan taken at <span class="math notranslate nohighlight">\(T_{5}\)</span> with the one at <span class="math notranslate nohighlight">\(T_{2}\)</span>.
Detecting such loops can be done through a variety of means. The final,
unary factor <span class="math notranslate nohighlight">\(f_{0}(T_{1})\)</span> is there to “anchor” the solution to the
origin: if it is not there, the solution will be undetermined. Another
way to anchor the solution is to add unary factors at every time-step,
derived from GPS.</p>
</section>
<section id="map-inference-via-least-squares">
<h2><span class="section-number">6.4.5. </span>MAP Inference via Least-Squares<a class="headerlink" href="#map-inference-via-least-squares" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Linear problems with zero-mean Gaussian noise lead to least-squares problems.</p>
</div></blockquote>
<p>When measurements are linear functions of continuous variables,
finding the maximum a posteriori (MAP) solution can be done via
least-squares optimization.
Earlier in this book we have discussed MAP inference for discrete
variables, and we have discussed probability distributions for
continuous variables, but we have never put the two together.</p>
<p>In the case of linear measurements corrupted by zero-mean Gaussian noise, we can
recover the MAP solution by minimizing a sum of squared differences.
Recall that a univariate Gaussian density <strong>with mean</strong> <span class="math notranslate nohighlight">\(\mu\)</span> and <strong>variance</strong> <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is
given by</p>
<div class="math notranslate nohighlight">
\[
\mathcal{N} (x; \mu, \sigma^2) =\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right\} .
\]</div>
<p>An example can help explain this more clearly.
In particular, if we focus our attention in PoseSLAM on <em>just the x coordinates</em>, then we
predict relative measurements <span class="math notranslate nohighlight">\(\tilde{x}_{ij}\)</span> by</p>
<div class="math notranslate nohighlight">
\[
\tilde{x}_{ij}\approx h(x_{i,}x_{j})=x_{j}-x_{i}
\]</div>
<p>and, taking <span class="math notranslate nohighlight">\(\sigma=1\)</span> for now, each factor in
Figure
<a href="#fig:PoseSLAMFG" data-reference-type="ref" data-reference="fig:PoseSLAMFG">1</a>
could be written as</p>
<div class="math notranslate nohighlight">
\[
\phi(x_{i},x_{j})=\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{1}{2}\left(x_{j}-x_{i}-\tilde{x}_{ij}\right)^{2}\right\} ,
\]</div>
<p>By taking the negative log,
maximizing the posterior corresponds to minimizing the following sum of
squares, where them sum ranges over all <span class="math notranslate nohighlight">\((i,j)\)</span> pairs for which we have a
pairwise measurement:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{X}^{*}=\arg\min_{\mathcal{X}}\sum_{k}\frac{1}{2}\left(h(x_{i},x_{j})-\tilde{x}_{ij}\right)^{2}=\arg\min_{\mathcal{X}}\sum_{k}\frac{1}{2}\left(x_{j}-x_{i}-\tilde{x}_{ij}\right)^{2}.
\]</div>
<p>Linear least squares problems like these are easily solved by numerical
computing packages like MATLAB or numpy.</p>
</section>
<section id="poseslam-is-nonlinear">
<h2><span class="section-number">6.4.6. </span>PoseSLAM is Nonlinear !<a class="headerlink" href="#poseslam-is-nonlinear" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Nonlinear problems need nonlinear solutions.</p>
</div></blockquote>
<p>Unfortunately, in the PoseSLAM case we cannot use linear least squares,
because poses are not simply vectors, and the measurements are not
simply linear functions of the poses. Indeed, in PoseSLAM both the
prediction <span class="math notranslate nohighlight">\(h(T_{i},T_{j})\)</span> and the measurement <span class="math notranslate nohighlight">\(\tilde{T}_{ij}\)</span>
are relative poses. The measurement prediction function <span class="math notranslate nohighlight">\(h(\cdot)\)</span> is given
by</p>
<div class="math notranslate nohighlight">
\[
h(T_{i},T_{j})=T_{i}^{-1}T_{j}
\]</div>
<p>and the
measurement error to be minimized is</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2}\left\Vert \log\left(\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j}\right)\right\Vert ^{2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\log:SE(2)\rightarrow\mathbb{R}^3\)</span> is a variation of the logarithm function
that maps a pose in <span class="math notranslate nohighlight">\(SE(2)\)</span> to a
three-dimensional local coordinate vector <span class="math notranslate nohighlight">\(\xi\)</span>;
we will describe this in more detail below.</p>
<p>There are two ways to deal with this nonlinear problem. The first is to
realize that the only nonlinearities stem from the <span class="math notranslate nohighlight">\(\sin\)</span> and <span class="math notranslate nohighlight">\(\cos\)</span>
that are associated with the unknown orientations
<span class="math notranslate nohighlight">\(\theta_{i}\)</span>.
Recognizing this, we could try to solve for the
orientations first, and then solve for the translations using linear
least squares, exactly as above. This approach is known as <strong>rotation
averaging</strong> followed by linear translation recovery. Unfortunately this approach is
suboptimal, as it does not consider the orientation and translation
simultaneously. However, it can serve to provide a (very) good initial
estimate for nonlinear optimization, discussed below.</p>
<p>Indeed, we will prefer to take a second route, which is to use
<strong>nonlinear optimization</strong>, which we discuss below.</p>
</section>
<section id="nonlinear-optimization-for-poseslam">
<h2><span class="section-number">6.4.7. </span>Nonlinear Optimization for PoseSLAM<a class="headerlink" href="#nonlinear-optimization-for-poseslam" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Linearize, solve, repeat…</p>
</div></blockquote>
<p>At first glance, minimizing the measurement error might seem to be a straightforward
nonlinear optimization problem: simply use your favorite nonlinear optimizer, e.g., with
an initial estimate provided using the method in the previous section.
This approach might well succeed in finding matrices <span class="math notranslate nohighlight">\(T_i\)</span> that minimize the error,
but, unfortunately, there is no guarantee that <span class="math notranslate nohighlight">\(T_i \in SE(2)\)</span>, since
this approach does not ensure that the upper right submatrix is a valid rotation matrix.</p>
<p>Instead, we will locally linearize the problem and solve
the corresponding linear problem using least-squares, and iterate this
until convergence. We do this by, at each iteration, parameterizing a
pose <span class="math notranslate nohighlight">\(T\)</span> by</p>
<div class="math notranslate nohighlight">
\[
T\approx\bar{T}\Delta(\xi)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi\)</span> are local coordinates
<span class="math notranslate nohighlight">\(\xi\doteq(\delta x,\delta y,\delta\theta)\)</span> and the incremental pose
<span class="math notranslate nohighlight">\(\Delta(\xi)\in SE(2)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Delta(\xi)=\left[\begin{array}{cc|c}
1 &amp; -\delta\theta &amp; \delta x\\
\delta\theta &amp; 1 &amp; \delta y\\
\hline 0 &amp; 0 &amp; 1
\end{array}\right]
\end{split}\]</div>
<p>which you can recognize as a small angle
approximation of the <span class="math notranslate nohighlight">\(SE(2)\)</span> matrix.
In 3D the local coordinates <span class="math notranslate nohighlight">\(\xi\)</span> are 6-dimensional, and the small angle
approximation is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Delta(\xi)=\left[\begin{array}{ccc|c}
1 &amp; -\delta\theta_{z} &amp; \delta\theta_{y} &amp; \delta x\\
\delta\theta_{z} &amp; 1 &amp; -\delta\theta_{x} &amp; \delta y\\
-\delta\theta_{y} &amp; \delta\theta_{x} &amp; 1 &amp; \delta z\\
\hline 0 &amp; 0 &amp; 0 &amp; 1
\end{array}\right]
\end{split}\]</div>
<p>With this new notation, we can approximate the
nonlinear error by a linear approximation:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2}\left\Vert \log\left(\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j}\right)\right\Vert ^{2}\approx\frac{1}{2}\left\Vert A_{i}\xi_{i}+A_{j}\xi_{j}-b\right\Vert ^{2}.
\]</div>
<p>For <span class="math notranslate nohighlight">\(SE(2)\)</span> the matrices <span class="math notranslate nohighlight">\(A_{i}\)</span> and <span class="math notranslate nohighlight">\(A_{j}\)</span> are the <span class="math notranslate nohighlight">\(3\times3\)</span> <strong>or
Jacobian matrices</strong> and <span class="math notranslate nohighlight">\(b\)</span> is a <span class="math notranslate nohighlight">\(3\times1\)</span> bias term. The above
provides a linear approximation of the term within the norm as a
function of the incremental local coordinates <span class="math notranslate nohighlight">\(\xi_{i}\)</span> and <span class="math notranslate nohighlight">\(\xi_{j}\)</span>.
Deriving the detailed expressions for these Jacobians is beyond the
scope of this document, but suffice to say that they exist and not too
expensive to compute. In three dimensions, the Jacobian matrices are
<span class="math notranslate nohighlight">\(6\times6\)</span> and <span class="math notranslate nohighlight">\(16\times6\)</span>, respectively.</p>
<p>The final optimization will—in each iteration—minimize over the local
coordinates of all poses by summing over all pose constraints. If we
index those constraints by <span class="math notranslate nohighlight">\(k\)</span>, we have the following least squares
problem:</p>
<div class="math notranslate nohighlight">
\[
\Xi^{*}=\arg\min_{\Xi}\sum_{k}\frac{1}{2}\Vert A_{ki}\xi_{i}+A_{kj}\xi_{j}-b_{k}\Vert ^{2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Xi\doteq \{  \xi_{i}\}\)</span>, the set
of all incremental pose coordinates.</p>
<p>After solving for the incremental updates <span class="math notranslate nohighlight">\(\Xi\)</span>, we update all poses
using the update equation above and check for convergence.
If the error does not decrease significantly
we terminate, otherwise we linearize and solve again, until the error
converges. While this is not guaranteed to converge to a global minimum,
in practice it does so if there are enough relative measurements and a
good initial estimate is available. For example, GPS can provide us with
a good initial estimate. However, especially in urban environments GPS
can be quite noisy, and it could happen that the map quality suffers by
converging to a bad local minimum. Hence, a good quality control process
is absolutely necessary in production environments.</p>
<p>In summary, the algorithm for nonlinear optimization is</p>
<ul class="simple">
<li><p>Start with an initial estimate <span class="math notranslate nohighlight">\(\mathcal{T}^{0}\)</span></p></li>
<li><p>Iterate:</p>
<ol class="arabic simple">
<li><p>Linearize the factors
<span class="math notranslate nohighlight">\(\frac{1}{2}\Vert \log(\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j})\Vert ^{2}\approx\frac{1}{2}\Vert A_{i}\xi_{i}+A_{j}\xi_{j}-b\Vert ^{2}\)</span></p></li>
<li><p>Solve the least squares problem
<span class="math notranslate nohighlight">\(\Xi^{*}=\arg\min_{\Xi}\sum_{k}\frac{1}{2}\Vert A_{ki}\xi_{i}+A_{kj}\xi_{j}-b_{k}\Vert ^{2}\)</span></p></li>
<li><p>Update <span class="math notranslate nohighlight">\(X_{i}^{t+1}\leftarrow X_{j}^{t}\Delta(\xi_{i})\)</span></p></li>
</ol>
</li>
<li><p>Until the nonlinear error
<span class="math notranslate nohighlight">\(J(\mathcal{T})\doteq\sum_{k}\frac{1}{2}\Vert \log(\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j})\Vert ^{2}\)</span>
converges.</p></li>
</ul>
</section>
<section id="optimization-with-gtsam">
<h2><span class="section-number">6.4.8. </span>Optimization with GTSAM<a class="headerlink" href="#optimization-with-gtsam" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>GTSAM rocks PoseSLAM.</p>
</div></blockquote>
<p>For SLAM we typically use specialized packages such as GTSAM that
exploit the sparsity of the factor graphs to dramatically
speed up computation.
Typically
measurements only provide information on the relationship between a
handful of variables, and hence the resulting factor graph will be
sparsely connected. This is exploited by the algorithms implemented in
GTSAM to reduce computational complexity. Even when graphs are too dense
to be handled efficiently by direct methods, GTSAM provides iterative
methods that are quite efficient.</p>
<p>The following code, included in GTSAM as an example, creates the
factor graph from Figure
<a href="#fig:PoseSLAMFG" data-reference-type="ref" data-reference="fig:PoseSLAMFG">1</a>
in code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">NonlinearFactorGraph</span><span class="p">()</span>
<span class="n">prior_model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">((</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">PriorFactorPose2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">prior_model</span><span class="p">))</span>

<span class="c1"># Create odometry (Between) factors between consecutive poses</span>
<span class="n">odometry_model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">((</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="n">Between</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">BetweenFactorPose2</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>

<span class="c1"># Add the loop closure constraint</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The first block of code creates a nonlinear factor graph and adds the unary factor
<span class="math notranslate nohighlight">\(f_{0}(T_{1})\)</span>.
As the vehicle travels through the world, it adds
binary factors <span class="math notranslate nohighlight">\(f_{t}(T_{t},T_{t+1})\)</span> corresponding to odometry measurements.
Note that the final line of code models a different event: a <strong>loop closure</strong>.
For example,
the vehicle might recognize the same location using vision or a laser
range finder, and calculate the geometric pose constraint to when it
first visited this location. This is illustrated for poses <span class="math notranslate nohighlight">\(T_{5}\)</span>
and <span class="math notranslate nohighlight">\(T_{2}\)</span>, and generates the (red) loop closing factor
<span class="math notranslate nohighlight">\(f_{5}(T_{5},T_{2})\)</span>.</p>
<p>Before we can optimize, we need to create an initial estimate. In GTSAM, this is done via the <code class="docutils literal notranslate"><span class="pre">gtsam.Values</span></code> type:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the initial estimate</span>
<span class="n">initial_estimate</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Values</span><span class="p">()</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">4.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">initial_estimate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Values with 5 values:
Value 1: (gtsam::Pose2)
(0.5, 0, 0.2)

Value 2: (gtsam::Pose2)
(2.3, 0.1, -0.2)

Value 3: (gtsam::Pose2)
(4.1, 0.1, 1.5708)

Value 4: (gtsam::Pose2)
(4, 2, 3.14159)

Value 5: (gtsam::Pose2)
(2.1, 2.1, -1.5708)
</pre></div>
</div>
</div>
</div>
<p>We can use this initial estimate to show the factor graph below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Factor graph with odometry and loop closure constraints.</span>
<span class="c1">#| label: fig:factor_graph_with_loop_closure</span>
<span class="n">show</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">initial_estimate</span><span class="p">,</span> <span class="n">binary_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/97958c94dec8c060a2587aa720d8adf705f13eec7d0985775ed25704fea4ff7c.svg" src="_images/97958c94dec8c060a2587aa720d8adf705f13eec7d0985775ed25704fea4ff7c.svg" /></div>
</div>
<p>Optimization is done using non-linear minimization, as explained above. In GTSAM, this is done via a <code class="docutils literal notranslate"><span class="pre">NonlinearOptimizer</span></code> class. The specific optimizer we use below is <code class="docutils literal notranslate"><span class="pre">GaussNewtonOptimizer</span></code>, which exactly implements the pseudo-code given above, but exploiting sparsity in the factor graph to do this very efficiently. The optimizer only needs a graph and an initial estimate, both of which we already created, and hence the code below is quite simple:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optimize the initial values using a Gauss-Newton nonlinear optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">GaussNewtonOptimizer</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">initial_estimate</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Result:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final Result:
Values with 5 values:
Value 1: (gtsam::Pose2)
(-2.76843e-20, -8.152e-20, -3.57721e-20)

Value 2: (gtsam::Pose2)
(2, -1.89295e-19, -5.34287e-20)

Value 3: (gtsam::Pose2)
(4, -3.42174e-11, 1.5708)

Value 4: (gtsam::Pose2)
(4, 2, 3.14159)

Value 5: (gtsam::Pose2)
(2, 2, -1.5708)
</pre></div>
</div>
</div>
</div>
<p>We can also inspect the result graphically. Looking at the result as printed above only gets us so far, and more importantly, it only shows us the maximum a posteriori (MAP) solution, but not the uncertainty around it. Luckily, GTSAM can also compute the <strong>posterior marginals</strong>, which show the uncertainty on each recovered pose as a Gaussian density <span class="math notranslate nohighlight">\(P(T_i|Z)\)</span>, taking into account all the measurements <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>In code, we do this via the <code class="docutils literal notranslate"><span class="pre">gtsam.Marginals</span></code> object, and we can plot marginals with a special function <code class="docutils literal notranslate"><span class="pre">plot_pose2</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: The optimized trajectory with the estimated covariances.</span>
<span class="c1">#| label: fig:optimized_trajectory_with_covariances</span>
<span class="n">marginals</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Marginals</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">):</span>
    <span class="n">gtsam_plot</span><span class="o">.</span><span class="n">plot_pose2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">atPose2</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">,</span>
                          <span class="n">marginals</span><span class="o">.</span><span class="n">marginalCovariance</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/927fd6e77cae6f06b71e6b167e26e45ebccee79aa500280673c588b466e27df8.png" src="_images/927fd6e77cae6f06b71e6b167e26e45ebccee79aa500280673c588b466e27df8.png" />
</div>
</div>
<p>The result is shown graphically in Figure
<a href="#fig:example" data-reference-type="ref" data-reference="fig:example">2</a>,
along with covariance ellipses. These covariance ellipses
in 2D indicate the marginal over position, <em>over all possible
orientations</em>, and show the area that contains 99% of the probability
mass (in 1D this would correspond to three standard deviations). The graph
shows in a clear manner that the uncertainty on pose <span class="math notranslate nohighlight">\(T_{5}\)</span> is now
much less than if there would be only odometry measurements. The pose
with the highest uncertainty, <span class="math notranslate nohighlight">\(T_{4}\)</span>, is the one furthest away from
the unary constraint <span class="math notranslate nohighlight">\(f_{0}(T_{1})\)</span>, which is the only factor tying
the graph to a global coordinate frame.</p>
</section>
<section id="slam-with-landmarks">
<h2><span class="section-number">6.4.9. </span>SLAM with Landmarks<a class="headerlink" href="#slam-with-landmarks" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Take PoseSLAM, add landmarks.</p>
</div></blockquote>
<p>So far we optimized over one type of variable, but often we build a landmark map <em>simultaneously</em> with the trajectory, i.e., this is <em>true</em> SLAM. In the next chapter, we will more thoroughly examine the full 3D case, whereas here we will model landmarks with 2D points in the plane. That does not mean that they cannot represent real 3-D entities in the environment: they can be the location of trees, poles, building corners, the sides of windows, the location of a stop sign in traffic, even moving pedestrians in more advanced applications.</p>
<p>How do we measure such landmarks? The most typical <em>type</em> of measurements are either <strong>range</strong> measurements, <strong>bearing</strong> measurements, or <strong>bearing-range</strong> measurements. The details on how to obtain them are typically application-dependent, and below we will abstract away the sensor preprocessing details. For example, in the case of a LIDAR sensor,
bearing-range measurements can be obtained by preprocessing every LIDAR scan, detecting prominent vertical structures for example. A real-life example that we will discuss below involves detecting and measuring the bearing-range to trees.
Radar is another often-used sensor for autonomous driving, and it can often be modeled or idealized to give
bearing-range measurements as well.</p>
<p>To illustrate SLAM with landmarks, we build a small toy example with 3 bearing-range measurements to two different landmarks:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">slam_graph</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">NonlinearFactorGraph</span><span class="p">()</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">PriorFactorPose2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">prior_model</span><span class="p">))</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add Range-Bearing measurements to two different landmarks L1 and L2</span>
<span class="n">measurement_model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]))</span>
<span class="n">BR</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">BearingRangeFactor2D</span>
<span class="n">l</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">gtsam</span><span class="o">.</span><span class="n">symbol</span><span class="p">(</span><span class="s1">&#39;l&#39;</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]}</span> <span class="c1"># name landmark variables</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BR</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="o">.</span><span class="n">fromDegrees</span><span class="p">(</span><span class="mi">45</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">4.0</span> <span class="o">+</span> <span class="mf">4.0</span><span class="p">),</span> <span class="n">measurement_model</span><span class="p">))</span> <span class="c1"># pose 1 -*- landmark 1</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BR</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="o">.</span><span class="n">fromDegrees</span><span class="p">(</span><span class="mi">90</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">,</span><span class="n">measurement_model</span><span class="p">))</span> <span class="c1"># pose 2 -*- landmark 1</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BR</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="o">.</span><span class="n">fromDegrees</span><span class="p">(</span><span class="mi">90</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">,</span><span class="n">measurement_model</span><span class="p">))</span> <span class="c1"># pose 3 -*- landmark 2</span>
</pre></div>
</div>
</div>
</div>
<p>When we have an initial estimate, we can look at the structure of this factor graph:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">slam_initial</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Values</span><span class="p">()</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.30</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.20</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">4.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mf">1.80</span><span class="p">,</span> <span class="mf">2.10</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mf">4.10</span><span class="p">,</span> <span class="mf">1.80</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Factor graph with odometry and range-bearing constraints.</span>
<span class="c1">#| label: fig:factor_graph_with_range_bearing</span>
<span class="n">show</span><span class="p">(</span><span class="n">slam_graph</span><span class="p">,</span> <span class="n">slam_initial</span><span class="p">,</span> <span class="n">binary_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/35852e950b8dcf156386c848756dd8b44906f077508eb7cdceeb36ed4fd7f7fa.svg" src="_images/35852e950b8dcf156386c848756dd8b44906f077508eb7cdceeb36ed4fd7f7fa.svg" /></div>
</div>
<p>We optimize again using Levenberg Marquardt, and show the marginals on both robot position and landmarks, as before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">LevenbergMarquardtOptimizer</span><span class="p">(</span><span class="n">slam_graph</span><span class="p">,</span> <span class="n">slam_initial</span><span class="p">)</span>
<span class="n">slam_result</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: The optimized trajectory with the estimated covariances, with bearing-range measurements to landmarks.</span>
<span class="c1">#| label: fig:optimized_trajectory_with_covariances_and_landmarks_br</span>
<span class="n">marginals</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Marginals</span><span class="p">(</span><span class="n">slam_graph</span><span class="p">,</span> <span class="n">slam_result</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]:</span>
    <span class="n">gtsam_plot</span><span class="o">.</span><span class="n">plot_pose2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">slam_result</span><span class="o">.</span><span class="n">atPose2</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">marginals</span><span class="o">.</span><span class="n">marginalCovariance</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]:</span>
    <span class="n">gtsam_plot</span><span class="o">.</span><span class="n">plot_point2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">slam_result</span><span class="o">.</span><span class="n">atPoint2</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="n">j</span><span class="p">]),</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">P</span><span class="o">=</span><span class="n">marginals</span><span class="o">.</span><span class="n">marginalCovariance</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/29d5500ea3bfdea9f599028384210d93cdc87d884842fcca6084c841d8ac8350.png" src="_images/29d5500ea3bfdea9f599028384210d93cdc87d884842fcca6084c841d8ac8350.png" />
</div>
</div>
</section>
<section id="a-larger-slam-example">
<h2><span class="section-number">6.4.10. </span>A Larger SLAM Example<a class="headerlink" href="#a-larger-slam-example" title="Link to this heading">#</a></h2>
<p>Below we optimize a piece of the (old) <a class="reference external" href="http://www-personal.acfr.usyd.edu.au/nebot/victoria_park.htm">Victoria park dataset</a>, which involves a truck driving through a park in Sydney, extracting the position of trees in the park from LIDAR scans, just as we discussed above. The factor graph for this example is created from file and shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">datafile</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">findExampleDataFile</span><span class="p">(</span><span class="s1">&#39;example.graph&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span><span class="p">])</span>
<span class="p">[</span><span class="n">graph</span><span class="p">,</span><span class="n">initial</span><span class="p">]</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">load2D</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Factor graph for a more realistic example, derived from a real-world dataset.</span>
<span class="c1">#| label: fig:factor_graph_real_world</span>
<span class="n">show</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span><span class="n">initial</span><span class="p">,</span> <span class="n">binary_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/51c1f8118133e940dd9a9c401485dfd48695d3e9345b9390df72578ba71723a3.svg" src="_images/51c1f8118133e940dd9a9c401485dfd48695d3e9345b9390df72578ba71723a3.svg" /></div>
</div>
<p>This is a much larger factor graph than any we have encountered before, and we can distinguish several features:</p>
<ul class="simple">
<li><p>There is a prominent backbone of truck poses, connected via odometry measurements.</p></li>
<li><p>There are about 20 landmarks, some of which are seen briefly, while others are seen for longer periods of time.</p></li>
<li><p>The graph is very sparsely connected, and hence optimization will still be quite fast.</p></li>
</ul>
<p>Optimizing with <code class="docutils literal notranslate"><span class="pre">gtsam.LevenbergMarquardtOptimizer</span></code>, again…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_poses</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">extractPose2</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">initial_poses</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">initial</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">initial</span><span class="o">.</span><span class="n">atPose2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">retract</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">LevenbergMarquardtOptimizer</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Below we plot both the initial estimate, which was created by adding random noise on top of the ground truth, and the optimized trajectory:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_poses</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">extractPose2</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">initial_poses</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">initial_poses</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;initial&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">))</span>
<span class="n">final_poses</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">extractPose2</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">final_poses</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">final_poses</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;optimized&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">scaleanchor</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span><span class="n">scaleratio</span> <span class="o">=</span> <span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Initial and optimized trajectories for a more realistic example.</span>
<span class="c1">#| label: fig:initial_and_optimized_trajectories</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2967d948e5a91a95a9168623a8fa7d0d2fd33de902b47276c0c4d655d09af7b0.png" src="_images/2967d948e5a91a95a9168623a8fa7d0d2fd33de902b47276c0c4d655d09af7b0.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="S63_driving_sensing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">6.3. </span>Sensing for Autonomous Vehicles</p>
      </div>
    </a>
    <a class="right-next"
       href="S65_driving_planning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.5. </span>Planning for Autonomous Driving.</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poses-in-2d-and-3d">6.4.1. Poses in 2D and 3D</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-iterative-closest-points-algorithm">6.4.2. The Iterative Closest Points Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-closest-points">6.4.2.1. Finding Closest Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-pairwise-transform">6.4.2.2. Estimating the Pairwise Transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poseslam">6.4.3. PoseSLAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-poseslam-factor-graph">6.4.4. The PoseSLAM Factor Graph</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#map-inference-via-least-squares">6.4.5. MAP Inference via Least-Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poseslam-is-nonlinear">6.4.6. PoseSLAM is Nonlinear !</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-optimization-for-poseslam">6.4.7. Nonlinear Optimization for PoseSLAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-gtsam">6.4.8. Optimization with GTSAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slam-with-landmarks">6.4.9. SLAM with Landmarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-larger-slam-example">6.4.10. A Larger SLAM Example</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank Dellaert and Seth Hutchinson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>