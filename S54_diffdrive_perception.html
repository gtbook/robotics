
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5.4. Computer Vision 101 &#8212; Introduction to Robotics and Perception</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=51e3b7cf" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=c73c0f3e"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'S54_diffdrive_perception';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5.5. Path Planning" href="S55_diffdrive_planning.html" />
    <link rel="prev" title="5.3. Cameras for Robot Vision" href="S53_diffdrive_sensing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Introduction to Robotics and Perception - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Introduction to Robotics and Perception - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="S10_introduction.html">1. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S11_models.html">1.1. Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S12_reasoning.html">1.2. Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S13_math.html">1.3. The Mathematics of Robotics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S20_sorter_intro.html">2. A Trash Sorting Robot</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S21_sorter_state.html">2.1. Modeling the World State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S22_sorter_actions.html">2.2. Actions for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S23_sorter_sensing.html">2.3. Sensors for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S24_sorter_perception.html">2.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S25_sorter_decision_theory.html">2.5. Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="S26_sorter_learning.html">2.6. Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S27_sorter_summary.html">2.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S30_vacuum_intro.html">3. A Robot Vacuum Cleaner</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S31_vacuum_state.html">3.1. Modeling the State of the Vacuum Cleaning Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S32_vacuum_actions.html">3.2. Actions over time</a></li>
<li class="toctree-l2"><a class="reference internal" href="S33_vacuum_sensing.html">3.3. Dynamic Bayesian Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="S34_vacuum_perception.html">3.4. Perception with Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S35_vacuum_decision.html">3.5. Markov Decision Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="S36_vacuum_RL.html">3.6. Learning to Act Optimally</a></li>
<li class="toctree-l2"><a class="reference internal" href="S37_vacuum_summary.html">3.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S40_logistics_intro.html">4. Warehouse Robots in 2D</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S41_logistics_state.html">4.1. Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S42_logistics_actions.html">4.2. Moving in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="S43_logistics_sensing.html">4.3. Sensor Models with Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S44_logistics_perception.html">4.4. Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S45_logistics_planning.html">4.5. Planning for Logistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="S46_logistics_learning.html">4.6. Some System Identification</a></li>
<li class="toctree-l2"><a class="reference internal" href="S47_logistics_summary.html">4.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="S50_diffdrive_intro.html">5. A Mobile Robot With Simple Kinematics</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="S51_diffdrive_state.html">5.1. State Space for a differential-drive robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S52_diffdrive_actions.html">5.2. Motion Model for the Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S53_diffdrive_sensing.html">5.3. Cameras for Robot Vision</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">5.4. Computer Vision 101</a></li>
<li class="toctree-l2"><a class="reference internal" href="S55_diffdrive_planning.html">5.5. Path Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S56_diffdrive_learning.html">5.6. Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S57_diffdrive_summary.html">5.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S60_driving_intro.html">6. Autonomous Vehicles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S61_driving_state.html">6.1. Planar Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="S62_driving_actions.html">6.2. Kinematics for Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S63_driving_sensing.html">6.3. Sensing for Autonomous Vehicles</a></li>
<li class="toctree-l2"><a class="reference internal" href="S64_driving_perception.html">6.4. SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S65_driving_planning.html">6.5. Planning for Autonomous Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S66_driving_DRL.html">6.6. Deep Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S67_driving_summary.html">6.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S70_drone_intro.html">7. Autonomous Drones in 3D</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S71_drone_state.html">7.1. Moving in Three Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S72_drone_actions.html">7.2. Multi-rotor Aircraft</a></li>
<li class="toctree-l2"><a class="reference internal" href="S73_drone_sensing.html">7.3. Sensing for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S74_drone_perception.html">7.4. Visual SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S75_drone_planning.html">7.5. Trajectory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S76_drone_learning.html">7.6. Neural Radiance Fields for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S77_drone_summary.html">7.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">8. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="genindex.html">Index</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/S54_diffdrive_perception.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Computer Vision 101</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-filtering">5.4.1. Linear Filtering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convolution-example">5.4.2. 1D Convolution Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#arbitrary-2d-convolutions">5.4.3. Arbitrary 2D convolutions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-vs-edges">5.4.4. Gradients vs. Edges</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-connected-neural-networks">5.4.5. Fully Connected Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks">5.4.6. Convolutional Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-wide-in-cnns">5.4.6.1. Going Wide in CNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-deep-in-cnns">5.4.6.2. Going Deep in CNNS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-layers">5.4.6.3. Pooling Layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-cnn-example-lenet-5">5.4.7. A CNN Example: LeNet-5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-segmentation">5.4.8. Semantic Segmentation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-image-depth">5.4.9. Single Image Depth</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="computer-vision-101">
<h1><span class="section-number">5.4. </span>Computer Vision 101<a class="headerlink" href="#computer-vision-101" title="Link to this heading">#</a></h1>
<p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S54_diffdrive_perception.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<blockquote id="index-0">
<div><p>Convolution is the workhorse for much of computer vision.</p>
</div></blockquote>
<a class="reference internal image-reference" href="_images/S54-Two-wheeled_Toy_Robot-03.jpg"><img alt="Splash image with steampunk differential-drive robot thinking" class="align-center" src="_images/S54-Two-wheeled_Toy_Robot-03.jpg" style="width: 40%;" /></a>
<p>No book about robotics is complete without mentioning computer vision and introducing some of its main ideas, which we do in this section. However, computer vision is a large subject and it is not our intention to summarize the entire field and its many recent developments in this section. Rather, we give a broad overview of the ideas, and our treatment is necessarily somewhat superficial.</p>
<p>A very good resource for a deeper dive into the concepts introduced here and in Section 5.6 is the book <a class="reference external" href="https://d2l.ai/">Dive into Deep Learning</a> <span id="id1">[<a class="reference internal" href="bibliography.html#id62" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://d2l.ai.">Zhang <em>et al.</em>, 2023</a>]</span>, which -like this book- was authored as a set of executable jupyter notebooks. We encourage you to check it out.</p>
<section id="linear-filtering">
<h2><span class="section-number">5.4.1. </span>Linear Filtering<a class="headerlink" href="#linear-filtering" title="Link to this heading">#</a></h2>
<blockquote id="index-1">
<div><p>Filtering can be applied to spatial, as well as temporal, data.</p>
</div></blockquote>
<p>In the previous chapter, we developed the Bayes filter, and applied it to temporal data streams.
Here, we extend the idea of filtering to spatial data, such as images, instead of time sequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: The color image from the previous section.</span>
<span class="c1">#| label: fig:color_image_by_robot</span>
<span class="n">image_name</span> <span class="o">=</span> <span class="s2">&quot;LL_color_1201754063.387872.jpeg&quot;</span>
<span class="n">lagr_image</span> <span class="o">=</span> <span class="n">diffdrive</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span> <span class="c1"># locally: PIL.Image.open(image_name)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">lagr_image</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6f65948a3d30d2b3706c89034bef2c63b1e2a3945f9a0e5b4a2865276bbd69a5.png" src="_images/6f65948a3d30d2b3706c89034bef2c63b1e2a3945f9a0e5b4a2865276bbd69a5.png" />
</div>
</div>
<p>Recall the image from the previous section, shown again in Figure <a class="reference internal" href="#fig:color_image_by_robot"><span class="xref myst">1</span></a>.
First, to explain linear filtering operations, we will convert the image to grayscale.
The PIL code to do so and the result is shown in Figure <a class="reference internal" href="#fig:gray_image_by_robot"><span class="xref myst">2</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Gray scale version of the same image.</span>
<span class="c1">#| label: fig:gray_image_by_robot</span>
<span class="n">grayscale_image</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">ImageOps</span><span class="o">.</span><span class="n">grayscale</span><span class="p">(</span><span class="n">lagr_image</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grayscale_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ad9369322527c92b317f3b7ed329bb78009e3bb8684d65ef7422781d54f72ffe.png" src="_images/ad9369322527c92b317f3b7ed329bb78009e3bb8684d65ef7422781d54f72ffe.png" />
</div>
</div>
<p id="index-2">To further analyze this image, we use the <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> library below, which operates on <strong>tensors</strong>. These are basically equivalent to multidimensional numpy arrays. It is easy to convert from numpy to pytorch tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grayscale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">grayscale_image</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;type=</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">grayscale</span><span class="p">)</span><span class="si">}</span><span class="s2">, dtype=</span><span class="si">{</span><span class="n">grayscale</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, shape=</span><span class="si">{</span><span class="n">grayscale</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>type=&lt;class &#39;torch.Tensor&#39;&gt;, dtype=torch.float64, shape=torch.Size([384, 512])
</pre></div>
</div>
</div>
</div>
<p id="index-3">Below we first motivate filtering using an edge detection example, explain it in 1D, and then generalize to arbitrary filters.</p>
<p>A frequent operation in computer vision is <strong>edge detection</strong>, which is to find transitions between dark and light areas in an image, or vice versa. A simple edge detector can be implemented using a <em>linear filtering</em> operation.  We first show the code below and then explain it below that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sobel_u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">I_u</span> <span class="o">=</span> <span class="n">diffdrive</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">grayscale</span><span class="p">,</span> <span class="n">sobel_u</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Above the first line creates a “filter” of size <span class="math notranslate nohighlight">\(1 \times 3\)</span>, with values <span class="math notranslate nohighlight">\(\begin{bmatrix}-1 &amp; 0 &amp; 1\end{bmatrix}\)</span>, and then the second line calls a function <code class="docutils literal notranslate"><span class="pre">conv2</span></code> which implements the filtering. The results are shown in Figure <a class="reference internal" href="#fig:gray_image_and_edges_by_robot"><span class="xref myst">3</span></a>.
We show the input image and the computed “edge image” side by side. The edge image is color-coded: red is negative, green is positive, and yellow is zero. By comparing with the input, you can see that the edge image highlights strong <em>vertical edges</em> in the input image, where green corresponds to dark-light transitions, and red corresponds to light-dark transitions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Gray scale image and its edges.</span>
<span class="c1">#| label: fig:gray_image_and_edges_by_robot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot_mosaic</span><span class="p">([[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;edges&#39;</span><span class="p">]],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grayscale</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;edges&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">I_u</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;RdYlGn&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4b4d43b7d814db1097ba1a941a14675ea57299fdf75307037cc8dfefd9d68303.png" src="_images/4b4d43b7d814db1097ba1a941a14675ea57299fdf75307037cc8dfefd9d68303.png" />
</div>
</div>
</section>
<section id="d-convolution-example">
<h2><span class="section-number">5.4.2. </span>1D Convolution Example<a class="headerlink" href="#d-convolution-example" title="Link to this heading">#</a></h2>
<p>It is easier to appreciate how this image came to be by showing what is happening on a simpler image.
Consider a simple <span class="math notranslate nohighlight">\(1 \times 10\)</span> image consisting of ten pixels.
What happens if we apply the Sobel edge detector to this image?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simple</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">simple</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
<span class="n">diffdrive</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">simple</span><span class="p">,</span> <span class="n">sobel_u</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 3.  3.  3.  5.  5.  5.  5.  2.  2.  2.]
 [ 3.  0.  2.  2.  0.  0. -3. -3.  0. -2.]]
</pre></div>
</div>
</div>
</div>
<p>
The first line above shows the pixel values for the original <span class="math notranslate nohighlight">\(1 \times 10\)</span> image,
and the second line shows the “edge” image.
Every value in the edge image is computed from three values in the original image.
For example, the first pixel in the edge image has the value 2, which is calculated from the values
<span class="math notranslate nohighlight">\(\begin{bmatrix}3 &amp; 3 &amp; 5\end{bmatrix}\)</span>, as highlighted below:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7c826de6-5afb-4a00-aa4f-2a1e78720c96">
<span class="eqno">(5.31)<a class="headerlink" href="#equation-7c826de6-5afb-4a00-aa4f-2a1e78720c96" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{bmatrix}
3 &amp; \textbf{3} &amp; \textbf{3} &amp; \textbf{5} &amp; 5 &amp; 5 &amp; 5 &amp; 2 &amp; 2 &amp; 2 \\
3 &amp; 0 &amp; \textbf{2} &amp; 2 &amp; 0 &amp; 0 &amp; -3 &amp; -3 &amp; 0 &amp; -2
\end{bmatrix}
\end{equation}\]</div>
<p>The “recipe” to calculate the edge value is just taking a weighted sum,
where the weights are defined by our filter:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2f8b8c3f-9e84-498e-998b-21215fc2dd00">
<span class="eqno">(5.32)<a class="headerlink" href="#equation-2f8b8c3f-9e84-498e-998b-21215fc2dd00" title="Permalink to this equation">#</a></span>\[\begin{equation}
3 \times -1 + 3 \times 0 + 5 \times 1 = 2 = \begin{bmatrix}3 &amp; 3 &amp; 5\end{bmatrix} \begin{bmatrix}-1 &amp; 0 &amp; 1\end{bmatrix}^T
\end{equation}\]</div>
<p>The value <span class="math notranslate nohighlight">\(2\)</span> indicates a <em>positive</em> edge where the input values go from <span class="math notranslate nohighlight">\(3\)</span> to <span class="math notranslate nohighlight">\(5\)</span>. This operation is then repeated for every output pixel. In other words, every output pixel is computed as the dot product of the filter <span class="math notranslate nohighlight">\(\begin{bmatrix}-1 &amp; 0 &amp; 1\end{bmatrix}\)</span> with the <em>window</em> of pixels in the input image, centered around the location of the output pixel.</p>
<p id="index-4">For the simple 1D example above, we could write this with the simple formula</p>
<div class="amsmath math notranslate nohighlight" id="equation-fcf0754a-05a2-4aeb-ad73-ba76943ad04d">
<span class="eqno">(5.33)<a class="headerlink" href="#equation-fcf0754a-05a2-4aeb-ad73-ba76943ad04d" title="Permalink to this equation">#</a></span>\[\begin{equation}
h[i] = \sum_{k=-1}^1 g[k] f[i+k]
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the 1D input image, <span class="math notranslate nohighlight">\(g\)</span> is the 1D filter or <strong>kernel</strong>, and <span class="math notranslate nohighlight">\(h\)</span> is the output edge image. Note that we index into the kernel <span class="math notranslate nohighlight">\(g\)</span> with coordinates <span class="math notranslate nohighlight">\(k\in[-1,0,1]\)</span>. By adding <span class="math notranslate nohighlight">\(k\)</span> to the output coordinate <span class="math notranslate nohighlight">\(i\)</span>, we automatically take the weighted sum of pixels in the input image <span class="math notranslate nohighlight">\(f\)</span> centered around <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p id="index-5">Let us examine the input and output again:</p>
<div class="amsmath math notranslate nohighlight" id="equation-07236a3d-93d9-46e7-b555-430a1367f826">
<span class="eqno">(5.34)<a class="headerlink" href="#equation-07236a3d-93d9-46e7-b555-430a1367f826" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{bmatrix}
3 &amp; 3 &amp; 3 &amp; 5 &amp; 5 &amp; 5 &amp; 5 &amp; 2 &amp; 2 &amp; 2 \\
3 &amp; 0 &amp; 2 &amp; 2 &amp; 0 &amp; 0 &amp; -3 &amp; -3 &amp; 0 &amp; -2
\end{bmatrix}
\end{equation}\]</div>
<p>We already understand the first <span class="math notranslate nohighlight">\(2\)</span>. The output pixel next to it <em>also</em> has the value <span class="math notranslate nohighlight">\(2\)</span>, as you can verify using the formula. You might object to the fact that the edge seems to be “doubly wide”, and that we could do better with the simpler filter <span class="math notranslate nohighlight">\(\begin{bmatrix}-1 &amp; 1\end{bmatrix}\)</span>, which people also use. However, making a <span class="math notranslate nohighlight">\(1\times 3\)</span> filter with a zero in the middle ensures that the edges do not “shift”. The resulting simple filter is widely used and known a <strong>Sobel filter</strong>.</p>
<p>We can now look at the remaining values. It is easy to verify that the <span class="math notranslate nohighlight">\(-3\)</span> values result from the negative edge transition from <span class="math notranslate nohighlight">\(5\)</span> to <span class="math notranslate nohighlight">\(2\)</span> in the image. Also, where the input image is <em>constant</em>, the edge output image has a zero, which is great!</p>
<p>However, we need to make a decision about the output array size and padding. Indeed, what is less obvious are the <span class="math notranslate nohighlight">\(3\)</span> and <span class="math notranslate nohighlight">\(-2\)</span> values at the beginning and end of the output array. The answer is that here we used <em>zero-padding</em>: note that the calculation of the output <span class="math notranslate nohighlight">\(h[0]\)</span> requires access to input values <span class="math notranslate nohighlight">\(f[-1]\)</span>, <span class="math notranslate nohighlight">\(f[0]\)</span>, and <span class="math notranslate nohighlight">\(f[1]\)</span>. This is problematic, because <span class="math notranslate nohighlight">\(f[-1]\)</span> is <em>out of bounds</em>. Zero-padding is the convention to use a zero for every value that is out of bounds for the input. Another way to deal with this issue is to calculate a smaller output image that will only require access to valid input values. Many other strategies can be employed but “zero padding” and “valid” output image size are the two most common ones.</p>
<p id="index-6"><strong>Correlation vs. Convolution</strong>: we use the term convolution above, but the formula above is really <em>correlation</em>. The correct formula for <em>convolution</em>, a term from the signal processing literature, is</p>
<div class="amsmath math notranslate nohighlight" id="equation-d8fa4994-da2b-4db9-9df3-639d8287651d">
<span class="eqno">(5.35)<a class="headerlink" href="#equation-d8fa4994-da2b-4db9-9df3-639d8287651d" title="Permalink to this equation">#</a></span>\[\begin{equation}
h[i] = \sum_{k=-1}^1 g[k] f[i-k]
\end{equation}\]</div>
<p>with the only difference being the minus sign. With the advent of convolutional neural networks (CNNs),
which we will see below, everyone is now using the term <em>convolution</em> even when strictly speaking we mean <em>correlation</em>. The justification is that when the kernel is <em>learned</em>, the distinction stops mattering, which is actually true: you can just flip the kernel and get the same output as convolution would give you. However, hardcore signal and image processing experts still bristle at this.</p>
</section>
<section id="arbitrary-2d-convolutions">
<span id="index-7"></span><h2><span class="section-number">5.4.3. </span>Arbitrary 2D convolutions<a class="headerlink" href="#arbitrary-2d-convolutions" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Extending the concept of convolution to 2D.</p>
</div></blockquote>
<p>The 1D definition of convolution above can be easily extended to 2D images and 2D filters:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bcac6814-57a2-451b-8389-af69bc11e711">
<span class="eqno">(5.36)<a class="headerlink" href="#equation-bcac6814-57a2-451b-8389-af69bc11e711" title="Permalink to this equation">#</a></span>\[\begin{equation}
h[i,j] = \sum_{k, l} g[k,l] f[i+k, j+l]
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(l\)</span> range over the two kernel dimensions.
For example, if we applied our 1D filter above to a 2D image,
we would have <span class="math notranslate nohighlight">\(k \in \{ 0 \}\)</span> and <span class="math notranslate nohighlight">\(l \in \{ -1, 0 1 \}\)</span>,
which would apply the filter along individual rows of the image.</p>
<p>Armed with this formula, we can now understand the edge detection above. For each output pixel <span class="math notranslate nohighlight">\(h[i,j]\)</span>, we do a pointwise multiplication of the <span class="math notranslate nohighlight">\(1 \times 3\)</span> filter</p>
<div class="amsmath math notranslate nohighlight" id="equation-164902ae-185d-441f-85ba-14be91341b4b">
<span class="eqno">(5.37)<a class="headerlink" href="#equation-164902ae-185d-441f-85ba-14be91341b4b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{pmatrix}g[0,-1] &amp; g[0,0] &amp; g[0,1]\end{pmatrix} = \begin{pmatrix}-1 &amp; 0 &amp; 1\end{pmatrix}
\end{equation}\]</div>
<p>with the <span class="math notranslate nohighlight">\(1 \times 3\)</span> window</p>
<div class="amsmath math notranslate nohighlight" id="equation-dd876dee-3570-4d71-b333-5a87018a08d9">
<span class="eqno">(5.38)<a class="headerlink" href="#equation-dd876dee-3570-4d71-b333-5a87018a08d9" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{pmatrix}f[i,j-1] &amp; f[i,j+0] &amp; f[i,j+1]\end{pmatrix}
\end{equation}\]</div>
<p>in the input image <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>We can use a different filter to detect horizontal edges. Indeed, the <strong>Horizontal Sobel edge detector</strong> is simply the <span class="math notranslate nohighlight">\(3 \times 1\)</span> equivalent, with</p>
<div class="amsmath math notranslate nohighlight" id="equation-9d9613fa-8c3c-47aa-9b46-977955c468e0">
<span class="eqno">(5.39)<a class="headerlink" href="#equation-9d9613fa-8c3c-47aa-9b46-977955c468e0" title="Permalink to this equation">#</a></span>\[\begin{equation}
g = \begin{pmatrix}-1 \\ 0 \\ 1\end{pmatrix}
\end{equation}\]</div>
<p>The code in Figure <a class="reference internal" href="#fig:gray_image_and_horizontal_edges_by_robot"><span class="xref myst">4</span></a> applies the horizontal Sobel edge detector to our original image from above, and shows the result alongside the original.
Note that above we defined the filter such that a positive transition is defined as having dark then light for an increasing value of the <em>row</em> coordinate. This explains why above the strong edge with the sky shows up as <em>negative</em>, perhaps counter to your intuition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Gray scale image and the result of the horizontal Sobel edge detector.</span>
<span class="c1">#| label: fig:gray_image_and_horizontal_edges_by_robot</span>
<span class="n">sobel_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">I_v</span> <span class="o">=</span> <span class="n">diffdrive</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">grayscale</span><span class="p">,</span> <span class="n">sobel_v</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot_mosaic</span><span class="p">([[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;edges&#39;</span><span class="p">]],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grayscale</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;edges&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">I_v</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;RdYlGn&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f62d86ec0e3bcf918920fdf03335e0e6dbe3b045dafef4f355b2d8055a56f110.png" src="_images/f62d86ec0e3bcf918920fdf03335e0e6dbe3b045dafef4f355b2d8055a56f110.png" />
</div>
</div>
</section>
<section id="gradients-vs-edges">
<h2><span class="section-number">5.4.4. </span>Gradients vs. Edges<a class="headerlink" href="#gradients-vs-edges" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>The Sobel operator is a gradient operator, not an edge detector.</p>
</div></blockquote>
<p>Above we told a small white lie: the Sobel filters actually approximate the image <em>gradient</em>, i.e., the spatial derivatives of the image values in the horizontal or vertical directions. We associate high gradient values with edges, but actually the two concepts are not the same: saying that there is an edge in the image can be regarded as a binary classification decision: either there is an edge, or not.
Could we use our Sobel gradient operators to construct an edge detector?</p>
<p>The gradient magnitude (i.e., the Euclidean norm of the gradient) is a positive number that combines both horizontal and vertical gradient values. We can calculate and visualize it in Figure <a class="reference internal" href="#fig:gray_image_and_gradient_magnitude_by_robot"><span class="xref myst">5</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Gray scale and gradient magnitude.</span>
<span class="c1">#| label: fig:gray_image_and_gradient_magnitude_by_robot</span>
<span class="n">I_m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">I_u</span><span class="p">)</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">I_v</span><span class="p">))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot_mosaic</span><span class="p">([[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;edges&#39;</span><span class="p">]],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grayscale</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;edges&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">I_m</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Greys&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/25676f1e20d887c89c96bd69553c9cdaefead22bfde5cede1e50cf0434ffd431.png" src="_images/25676f1e20d887c89c96bd69553c9cdaefead22bfde5cede1e50cf0434ffd431.png" />
</div>
</div>
<p id="index-8">From the figure, you should notice that edges seem to have a high magnitude, and non-edges have a low magnitude. Hence, a simple idea is to threshold the gradient to obtain a <em>binary edge image</em>, as we do in Figure <a class="reference internal" href="#fig:gray_image_and_thresholded_gradient_magnitude_by_robot"><span class="xref myst">6</span></a>. We used a threshold <span class="math notranslate nohighlight">\(\theta=50\)</span>, but feel free to play with this threshold a bit and see what you like best. You might experience that it is not so easy to make this simple, hand-designed edge detector to do what we <em>really</em> want, which is to detect edges as we think of them, and not react to all the noise in the image. Image processing, and computer vision in general, is messy and hard!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Gray scale version and thresholded gradient magnitude.</span>
<span class="c1">#| label: fig:gray_image_and_thresholded_gradient_magnitude_by_robot</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">I_m</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot_mosaic</span><span class="p">([[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;edges&#39;</span><span class="p">]],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grayscale</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="s1">&#39;edges&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Greys&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/60e8f20a02e48b61c73a83d33a321b762693599bad764a8b423998b4284370d6.png" src="_images/60e8f20a02e48b61c73a83d33a321b762693599bad764a8b423998b4284370d6.png" />
</div>
</div>
</section>
<section id="fully-connected-neural-networks">
<span id="index-9"></span><h2><span class="section-number">5.4.5. </span>Fully Connected Neural Networks<a class="headerlink" href="#fully-connected-neural-networks" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Convolution can be used to implement neural networks.</p>
</div></blockquote>
<p>Above we looked at edge detection as a classification problem. Our solution was to calculate the output of two different filters (both Sobel operators), combine these with a non-linear operation (the norm), and then apply a threshold at some hand-tuned level. It is only natural to ask whether this idea of taking a linear combination of pixels and feeding it into some “decision maker” could solve other tasks, including the a main goal of computer vision, detecting and recognizing objects, a capability which seems effortless to people yet which eluded computer vision researchers for a long time.</p>
<p>Inspired by the way neurons in the brain appear to be connected, <strong>neural networks</strong> were first proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> in the 1950s, who with his collaborators proposed the <strong>Perceptron</strong>. The mathematical equation for a perceptron is simple:</p>
<div class="amsmath math notranslate nohighlight" id="equation-904a1871-3306-40dd-89ac-ab6e827c2c0d">
<span class="eqno">(5.40)<a class="headerlink" href="#equation-904a1871-3306-40dd-89ac-ab6e827c2c0d" title="Permalink to this equation">#</a></span>\[\begin{equation}
f(x) = \theta \begin{pmatrix}\sum_k w[k] x[k] + b\end{pmatrix} = \theta(w \cdot x + b)
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(w \in \mathbb{R}^n\)</span> is the 1D vector of weights and <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> is the 1D input signal.
The output <span class="math notranslate nohighlight">\(f(x) \in \mathbb{R} \)</span> of the network is obtained by pointwise multiplying the input signal <span class="math notranslate nohighlight">\(x\)</span> and the weights <span class="math notranslate nohighlight">\(w\)</span>, and thresholding.
Here, <span class="math notranslate nohighlight">\(\theta :\mathbb{R} \rightarrow \{0,1\} \)</span> denotes the thresholding operation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ac367ccf-8f55-40e4-9a69-870f65895f09">
<span class="eqno">(5.41)<a class="headerlink" href="#equation-ac367ccf-8f55-40e4-9a69-870f65895f09" title="Permalink to this equation">#</a></span>\[\begin{equation}
\theta(x) \doteq 1 \text{  if  } (x&gt;0) \text{  else  } 0
\end{equation}\]</div>
<p>The scalar quantity <span class="math notranslate nohighlight">\(b\)</span> is known as the bias, and can be seen as shifting the decision boundary to values different from <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p id="index-10">While a perceptron computes a single output <span class="math notranslate nohighlight">\(f(x)\)</span> from the input signal <span class="math notranslate nohighlight">\(x\)</span>, there are two ways to extend the concept: going <em>wide</em> and going <em>deep</em>. First, we can extend the concept of the perceptron to be multi-output, yielding <span class="math notranslate nohighlight">\(n_o\)</span> <strong>output features</strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8ad7a997-557f-4669-94bc-894189722611">
<span class="eqno">(5.42)<a class="headerlink" href="#equation-8ad7a997-557f-4669-94bc-894189722611" title="Permalink to this equation">#</a></span>\[\begin{equation}
f_o(x) = \theta \begin{pmatrix}\sum_k W[o,k] x[k] + b[o]\end{pmatrix} 
\end{equation}\]</div>
<p>We can stack the <span class="math notranslate nohighlight">\(n_o\)</span> equations to build a single matrix equation for the multi-output case:</p>
<div class="amsmath math notranslate nohighlight" id="equation-56ab9ff1-626a-4162-b48c-da153e63db7a">
<span class="eqno">(5.43)<a class="headerlink" href="#equation-56ab9ff1-626a-4162-b48c-da153e63db7a" title="Permalink to this equation">#</a></span>\[\begin{equation}
f(x) =
\begin{bmatrix}
f_1(x) \\ \vdots \\ f_{n_o}(x)
\end{bmatrix}
=
\begin{bmatrix}
\sum_k W[1,k] x[k] + b[1] \\
\vdots\\
\sum_k W[n_o,k] x[k] + b[n_o]\\
\end{bmatrix}
=
\theta(W x + b)
\end{equation}\]</div>
<p>where now <span class="math notranslate nohighlight">\(W\)</span> is an <span class="math notranslate nohighlight">\(n_o \times n\)</span> matrix, and <span class="math notranslate nohighlight">\(b\)</span> is a vector of dimension <span class="math notranslate nohighlight">\(n_o\)</span>.
The threshold function <span class="math notranslate nohighlight">\(\theta: \mathbb{R}^{n_o} \rightarrow \{0,1\}^{n_o}\)</span>
applies a threshold operation to each entry of a vector (merely a vector version of the threshold
operation above).
This is called going “wide” as we now create multi-dimensional outputs.</p>
<p id="index-12"><span id="index-11"></span>Going <em>deep</em> is taking the output from one (multi-dimensional) perceptron and feeding it into a subsequent perceptron.
We call each stage a <strong>layer</strong> in the neural network.
<strong>Multi-layer perceptrons</strong> or <strong>MLPs</strong> can capture increasingly complex concepts present in the input signal.
The idea is that the output of the first layer learns simpler concepts from the input signal, and the next layer combines these concepts into more complex concepts</p>
<p id="index-13">The notion of “simple concepts” computed at each layer is very useful, and we have already introduced the term <em>feature</em> above to denote these concepts. A feature can be hand-designed, much like the output of the Sobel operator we introduced above, or <em>learned</em>. While we will postpone the discussion of <em>how</em> to learn these features from data until section 5.6, it is important to know that almost all successful vision pipelines these days learn the feature representations from data, and which features are learned very heavily depends on the task.</p>
<p>The MLP architecture is very powerful, but it is also expensive. For every MLP layer with <span class="math notranslate nohighlight">\(n\)</span> input features and <span class="math notranslate nohighlight">\(n_o\)</span>, we need a <em>weight matrix</em> <span class="math notranslate nohighlight">\(W\)</span> of size <span class="math notranslate nohighlight">\(n_o \times n\)</span>. That seems doable in the 1D case, but when thinking about images this becomes rather expensive. Even for relatively low-resolution images, say <span class="math notranslate nohighlight">\(256\times 256\)</span>, the number of input features <span class="math notranslate nohighlight">\(n =256^2=65,536\)</span>. Even if we wanted to only compute a relatively modest number of features, say <span class="math notranslate nohighlight">\(32\)</span>, that still requires over <span class="math notranslate nohighlight">\(2\)</span> million weights to be specified. However, even if we had infinite compute and storage, there is another issue with having that many weights when they are to be learned: there might simply not be enough <em>data</em> to nail down the weights in a principled manner.</p>
<p id="index-14">For computer vision applications, a different class of neural networks called
<strong>convolutional neural networks</strong> alleviates both concerns
by combining notions of multi-layer networks with the earlier introduced concept of convolutions.</p>
</section>
<section id="convolutional-neural-networks">
<h2><span class="section-number">5.4.6. </span>Convolutional Neural Networks<a class="headerlink" href="#convolutional-neural-networks" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Convolve, “threshold”, repeat…</p>
</div></blockquote>
<p id="index-15">Three separate ideas gave rise to <strong>convolutional neural networks</strong> or <strong>CNN</strong>s, which replace fully connected or <em>dense</em> layers with <em>convolutional</em> layers:</p>
<ul class="simple">
<li><p>Linear filtering: convolution and correlation, and linear filtering in general, are primordial concepts from signal and image processing where they have proved immensely useful.</p></li>
<li><p>Shared weights: The idea to replace a very large weight matrix <span class="math notranslate nohighlight">\(W\)</span> with a much smaller <span class="math notranslate nohighlight">\(kernel\)</span>, which we will denote by <span class="math notranslate nohighlight">\(g\)</span>, is attractive from a computational resources point of view.</p></li>
<li><p>Translation invariance: Intuitively, a useful feature at one location in the image (or 1D signal) should also be useful at other locations.</p></li>
</ul>
<p>The concept of translation invariance is important,
and warrants some more explanation.
Suppose we apply kernel <span class="math notranslate nohighlight">\(g\)</span> to input <span class="math notranslate nohighlight">\(x\)</span> to obtain the output <span class="math notranslate nohighlight">\(f\)</span>.
Translation invariance implies that if we shift the input by <span class="math notranslate nohighlight">\(t\)</span>, the resulting output
will merely be the original <span class="math notranslate nohighlight">\(f\)</span> also shifted by <span class="math notranslate nohighlight">\(t\)</span>.
Formally, in 1D we write this condition as</p>
<div class="amsmath math notranslate nohighlight" id="equation-b21b52d3-5a01-4a57-9f07-8037aa49cffd">
<span class="eqno">(5.44)<a class="headerlink" href="#equation-b21b52d3-5a01-4a57-9f07-8037aa49cffd" title="Permalink to this equation">#</a></span>\[\begin{equation}
f[i+t] = \sum_{k=-1}^1 g[k] x[i+t+k],
\end{equation}\]</div>
<p>i.e., if we <em>translate</em> the input signal <span class="math notranslate nohighlight">\(x\)</span> by <span class="math notranslate nohighlight">\(t\)</span>, the output signal <span class="math notranslate nohighlight">\(f\)</span> is identical but translated by the same amount. A similar definition holds for 2D translation in the case of 2D convolution.</p>
<section id="going-wide-in-cnns">
<h3><span class="section-number">5.4.6.1. </span>Going Wide in CNNs<a class="headerlink" href="#going-wide-in-cnns" title="Link to this heading">#</a></h3>
<p id="index-16">As with fully connected neural networks, we can go wide with multi-layer CNNs.
For <strong>convolutional layer</strong> <span class="math notranslate nohighlight">\(l\)</span>, denote the number of input channels by <span class="math notranslate nohighlight">\(n_{l,i}\)</span> and the number
of output channels by <span class="math notranslate nohighlight">\(n_{l,o}\)</span>.
The <span class="math notranslate nohighlight">\(o^{th}\)</span> output of layer <span class="math notranslate nohighlight">\(l\)</span> is obtained by convolution of its input with the
kernel for output <span class="math notranslate nohighlight">\(o\)</span> of layer <span class="math notranslate nohighlight">\(l\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e12bec83-7085-4172-95d3-34b8de67a138">
<span class="eqno">(5.45)<a class="headerlink" href="#equation-e12bec83-7085-4172-95d3-34b8de67a138" title="Permalink to this equation">#</a></span>\[\begin{equation}
f_{l,o}(x) = \sum_{ k} g[l, o, k] x[l, k] + b[l, o]
\end{equation}\]</div>
<p>In CNNs, each layer also has one bias value <span class="math notranslate nohighlight">\(b[o]\)</span> per output channel, as indicated above.</p>
</section>
<section id="going-deep-in-cnns">
<h3><span class="section-number">5.4.6.2. </span>Going Deep in CNNS<a class="headerlink" href="#going-deep-in-cnns" title="Link to this heading">#</a></h3>
<p id="index-18"><span id="index-17"></span>To go <em>deep</em>, we specify a nonlinear operation or <strong>activation function</strong> that is applied after the linear convolution step.
This activation function plays the role of <span class="math notranslate nohighlight">\(\theta\)</span> in our fully connected networks above.
Indeed, <em>without</em> an activation function, it makes little sense to have two successive linear layers with weights <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>: one could just as easily replace them with a <em>single</em> linear layer with weights <span class="math notranslate nohighlight">\(W = A B\)</span>. In addition, from the multi-layer perceptron work we know that the <em>threshold</em> operation is a crucial step in <em>activating</em> a feature, i.e., deciding when a feature is really present or whether the generated signal is just due to noise. Think back to our primitive edge detector above as well: both the thresholding and the threshold value (which in a perceptron is encoded in a bias <span class="math notranslate nohighlight">\(b\)</span>) are important for the final result.</p>
<p>The most important activation functions are the sigmoid and “ReLU”. We have already encountered the threshold function <span class="math notranslate nohighlight">\(\theta\)</span>, but historically a “soft threshold” function called the <strong>sigmoid function</strong> was, and still is, very popular:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c19bc756-3b38-436f-973b-816035847e57">
<span class="eqno">(5.46)<a class="headerlink" href="#equation-c19bc756-3b38-436f-973b-816035847e57" title="Permalink to this equation">#</a></span>\[\begin{equation}
\sigma(x) \doteq \frac{1}{1+\exp(-x)}.
\end{equation}\]</div>
<p>Many other activation functions have been proposed since, but it is now widely accepted that the exact shape of the function matters less than the simple fact of having <em>some</em> non-linearity. One of the simplest and most popular activation functions just zeroes out any negative values, but otherwise leaves the input signal untouched. This is called the <strong>rectified linear unit</strong>, abbreviated ReLU, and is given by this simple formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-17f1daa0-5244-4cf9-acc5-aa25d5cc9f30">
<span class="eqno">(5.47)<a class="headerlink" href="#equation-17f1daa0-5244-4cf9-acc5-aa25d5cc9f30" title="Permalink to this equation">#</a></span>\[\begin{equation}
ReLU(x) \doteq \max(0,x).
\end{equation}\]</div>
</section>
<section id="pooling-layers">
<h3><span class="section-number">5.4.6.3. </span>Pooling Layers<a class="headerlink" href="#pooling-layers" title="Link to this heading">#</a></h3>
<p id="index-19">Finally, CNNs also frequently have <strong>pooling layers</strong>. A downside of a convolutional layer is that each output layer is as large as the previous one: the convolution, even when in <code class="docutils literal notranslate"><span class="pre">valid</span></code> mode, only slightly reduces the image size, and not at all when using <code class="docutils literal notranslate"><span class="pre">same</span></code> mode with zero-padding. So called <em>pooling</em> layers were again inspired by the human visual system, where an experimentalists observed that while early processing layers where “retinotopic”, i.e., had a one-to-one mapping to locations on the imaging surface, successive layers gradually became coarser <em>and</em> activated by wider “receptive fields”, defined as the area on the retina that were able to influence the activation of a neuron at a given processing stage. There are also computational reasons to wanting to “downscale” the resolution in deeper layers, as many neural net architectures increase the number of features with depth, and hence reducing the resolution correspondingly yielded in approximately the same amount of computation per layer.</p>
<p>Formally, a pooling layer is most often an averaging or maximization operation over an input window of a given size. For example, a “max-pooling” layer in 2D implements the following equation,</p>
<div class="amsmath math notranslate nohighlight" id="equation-bc31405d-99f7-42b3-9c56-517e998f8863">
<span class="eqno">(5.48)<a class="headerlink" href="#equation-bc31405d-99f7-42b3-9c56-517e998f8863" title="Permalink to this equation">#</a></span>\[\begin{equation}
h[c, i, j] = \max_{k, l} f[c, i+k, j+l],
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(c\in[n_c]\)</span> ranges over the <span class="math notranslate nohighlight">\(n_c\)</span> channels, and once again <span class="math notranslate nohighlight">\(k \in [H_k]\)</span> and <span class="math notranslate nohighlight">\(l \in [W_k]\)</span>. The max operation actually <em>does</em> not reduce the resolution by itself: that is done by only computing the pooling output at a certain <em>stride</em> <span class="math notranslate nohighlight">\(s\)</span>, where often the stride is equal to the size of the window. Formally, we have</p>
<div class="amsmath math notranslate nohighlight" id="equation-a8fc57d8-f433-4a7e-9dc6-72784d522b07">
<span class="eqno">(5.49)<a class="headerlink" href="#equation-a8fc57d8-f433-4a7e-9dc6-72784d522b07" title="Permalink to this equation">#</a></span>\[\begin{equation}
h[c, i, j] = \max_{k, l} f[c, i s_i + k, j s_j + l],
\end{equation}\]</div>
<p>where the strides <span class="math notranslate nohighlight">\(s_i\)</span> and <span class="math notranslate nohighlight">\(s_j\)</span> for the row and column indexing can be different. More often than not, however, practitioners choose <span class="math notranslate nohighlight">\(s_i=s_j=W_k=H_k\)</span>, i.e., square pooling windows with the same stride, e.g., max pooling over <span class="math notranslate nohighlight">\(2\times 2\)</span> windows with a stride of <span class="math notranslate nohighlight">\(2\)</span>. Average and sum-pooling are similarly defined, and are equivalent up to a constant scaling factor.</p>
</section>
</section>
<section id="a-cnn-example-lenet-5">
<h2><span class="section-number">5.4.7. </span>A CNN Example: LeNet-5<a class="headerlink" href="#a-cnn-example-lenet-5" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>A historically important example.</p>
</div></blockquote>
<p>Convolutional neural networks were pioneered by <a class="reference external" href="https://en.wikipedia.org/wiki/Kunihiko_Fukushima">Kunihiko Fukushima</a> in the 70s, and and <a class="reference external" href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> in the 1980s. The latter created several CNN-style neural networks for the task of handwritten digit recognition, motivated by an application for the US Postal Service. This work took part from the late 80s to well into the 90s, and is described in a <a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/726791">highly cited 1998 overview paper</a> <span id="id2">[<a class="reference internal" href="bibliography.html#id36" title="Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. URL: https://ieeexplore.ieee.org/abstract/document/726791.">LeCun <em>et al.</em>, 1998</a>]</span>. Below we show the architecture of LeNet-5 described in that paper.</p>
<p>LeNet-5 takes a single-channel, <span class="math notranslate nohighlight">\(32\times 32\)</span> grayscale image as input, and has the following layers:</p>
<ul class="simple">
<li><p>6-channel, <span class="math notranslate nohighlight">\(28\times 28\)</span> convolutional layer with <span class="math notranslate nohighlight">\(5\times 5\)</span> kernel;</p></li>
<li><p>6-channel <span class="math notranslate nohighlight">\(14\times 14\)</span> pooling layer with stride 2, i.e., sub-sampling to half-resolution;</p></li>
<li><p>16-channel, <span class="math notranslate nohighlight">\(10\times 10\)</span> convolutional layer with <span class="math notranslate nohighlight">\(5\times 5\)</span> kernel;</p></li>
<li><p>16-channel <span class="math notranslate nohighlight">\(5\times 5\)</span> pooling layer with stride 2, i.e., sub-sampling once again by half;</p></li>
<li><p>120-channel fully connected layer;</p></li>
<li><p>84-unit fully connected layer;</p></li>
<li><p>10-unit fully connected output layer, i.e., one output unit for every possible digit from 0..9.</p></li>
</ul>
<p>Note that above we have not given all the implementation details specific to LeNet-5, only the broad architecture, because there are quite a few quirks in actual implementation that are no longer very relevant today, and hence we omit those details here.</p>
<p>An instance of this network was able to achieve 0.8% error rate on a standard OCR dataset called <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>.
You can see an animation of it <a class="reference external" href="http://yann.lecun.com/exdb/lenet/">here</a>.
The MNIST task is now considered solved, and LeNet-5 is not even the best performer anymore, but the network is a great example of the seminal work on CNNs that ultimately led to the latest deep learning revolution.</p>
<p>LeNet-5 it is an example of a very typical architecture that is still used by many modern methods: a sequence of convolutional and pooling layers, progressively increasing the feature-channel count as we go deeper while at the same time decreasing the feature map resolution, until the point where a few fully connected layers are used to perform a final classification decision.</p>
</section>
<section id="semantic-segmentation">
<h2><span class="section-number">5.4.8. </span>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Link to this heading">#</a></h2>
<blockquote id="index-20">
<div><p>What can neural networks do for robots?</p>
</div></blockquote>
<p>In section 5.6 we will see some other applications, but a very relevant task for robotics is <strong>semantic segmentation</strong>. In this task, every pixel in the image is classified into a finite set of <em>classes</em>, such as road, vegetation, building, sky, etc. Think back to the trash sorting robot’s need to classify pieces of trash into different categories. Semantic segmentation is similar, but we now do this for <em>every pixel</em> in the image. This is a very useful capability for a <em>mobile</em> robot, e.g., it can help plan a path over drivable surfaces.</p>
<p>Below we give a pytorch example running a pre-trained semantic segmentation neural network. We can load it from the <a class="reference external" href="https://pytorch.org/hub/">Pytorch Hub</a>, a repository of state of the art neural network models:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;pytorch/vision:v0.10.0&#39;</span><span class="p">,</span> <span class="s1">&#39;deeplabv3_resnet50&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span> <span class="c1"># DEVICE will be equal to &#39;cuda&#39; if GPU is available</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<p>In Figure <a class="reference internal" href="#fig:highway_image_by_robot"><span class="xref myst">7</span></a> we load a highway driving example for the purposes of testing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Color image taken from a front-facing camera on a highway.</span>
<span class="c1">#| label: fig:highway_image_by_robot</span>
<span class="n">image_name</span> <span class="o">=</span> <span class="s2">&quot;highway280.jpg&quot;</span>
<span class="n">highway_image</span> <span class="o">=</span> <span class="n">diffdrive</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;resolution = </span><span class="si">{</span><span class="n">highway_image</span><span class="o">.</span><span class="n">width</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">highway_image</span><span class="o">.</span><span class="n">height</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">highway_image</span> <span class="o">=</span> <span class="n">highway_image</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">highway_image</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>resolution = 1282x962
</pre></div>
</div>
<img alt="_images/27ae3acd0b739718f000fa0063f426d27c62739011cba5f2c41e822647a7c610.png" src="_images/27ae3acd0b739718f000fa0063f426d27c62739011cba5f2c41e822647a7c610.png" />
</div>
</div>
<p>We then run the model, after first transforming the image to match the way it was presented to the network during training (which we glean from the PyTorch Hub examples for this model):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample execution (requires torchvision)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
<span class="p">])</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">highway_image</span><span class="p">)</span>
<span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># create mini-batch as expected by the model</span>
<span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)[</span><span class="s1">&#39;out&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">output_predictions</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, we use example code from PyTorch Hub to display the result as a color-coded per-pixel segmentation image. Note the method calls <code class="docutils literal notranslate"><span class="pre">.cpu()</span></code> and <code class="docutils literal notranslate"><span class="pre">.numpy()</span></code> which respectively transfer a PyTorch tensor to the CPU (if it’s not already there) and then convert the tensor to a <code class="docutils literal notranslate"><span class="pre">numpy</span></code> array, to play nicely with <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>. The result is shown in Figure <a class="reference internal" href="#fig:semantic_segmentation_by_robot"><span class="xref myst">8</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Semantic segmentation of the highway image.</span>
<span class="c1">#| label: fig:semantic_segmentation_by_robot</span>
<span class="c1"># create a color pallette, selecting a color for each class</span>
<span class="n">palette</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">25</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">15</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">21</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">21</span><span class="p">)])[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">palette</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">(</span><span class="n">colors</span> <span class="o">%</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>

<span class="c1"># plot the semantic segmentation predictions of 21 classes in each color</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">output_predictions</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">highway_image</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="n">r</span><span class="o">.</span><span class="n">putpalette</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">r</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f0a1999583e7ff9c94306c440422eb1b9280e93a38f0f1909f33dc9d3ad4c1e0.png" src="_images/f0a1999583e7ff9c94306c440422eb1b9280e93a38f0f1909f33dc9d3ad4c1e0.png" />
</div>
</div>
</section>
<section id="single-image-depth">
<h2><span class="section-number">5.4.9. </span>Single Image Depth<a class="headerlink" href="#single-image-depth" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Deep neural networks can infer depth even from a single image.</p>
</div></blockquote>
<p>Below we give a pytorch example running a pre-trained single image depth neural network. Again we read it from the PyTorch Hub, using the MiDaS model trained by Intel researchers:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;MiDaS_small&quot;</span>  <span class="c1"># MiDaS v2.1 - Small</span>
<span class="n">midas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;intel-isl/MiDaS&quot;</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
<span class="n">midas</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">);</span>
<span class="n">midas</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<p>This time we manually transform the image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">resized</span> <span class="o">=</span> <span class="n">highway_image</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
<span class="n">normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">resized</span><span class="p">)</span><span class="o">-</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span><span class="o">/</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]</span>
<span class="n">transposed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">normalized</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">image32</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">transposed</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image32</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>After evaluating the network, we show the result using matplotlib, in Figure <a class="reference internal" href="#fig:depth_estimation_by_robot"><span class="xref myst">9</span></a>. The output of this network, which was selected for its high inference speed, is not particularly accurate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Depth estimation of the highway image.</span>
<span class="c1">#| label: fig:depth_estimation_by_robot</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">midas</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/aa723fdac8e5b5c6ac4bbaee6a43456931a2d557674976684a365ef4ce941495.png" src="_images/aa723fdac8e5b5c6ac4bbaee6a43456931a2d557674976684a365ef4ce941495.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="S53_diffdrive_sensing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5.3. </span>Cameras for Robot Vision</p>
      </div>
    </a>
    <a class="right-next"
       href="S55_diffdrive_planning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5.5. </span>Path Planning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-filtering">5.4.1. Linear Filtering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convolution-example">5.4.2. 1D Convolution Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#arbitrary-2d-convolutions">5.4.3. Arbitrary 2D convolutions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-vs-edges">5.4.4. Gradients vs. Edges</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-connected-neural-networks">5.4.5. Fully Connected Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks">5.4.6. Convolutional Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-wide-in-cnns">5.4.6.1. Going Wide in CNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-deep-in-cnns">5.4.6.2. Going Deep in CNNS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-layers">5.4.6.3. Pooling Layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-cnn-example-lenet-5">5.4.7. A CNN Example: LeNet-5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-segmentation">5.4.8. Semantic Segmentation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-image-depth">5.4.9. Single Image Depth</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank Dellaert and Seth Hutchinson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>