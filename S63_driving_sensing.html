
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6.3. Sensing for Autonomous Vehicles &#8212; Introduction to Robotics and Perception</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6.4. SLAM" href="S64_driving_perception.html" />
    <link rel="prev" title="6.2. Kinematics for Driving" href="S62_driving_actions.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-312077-7', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Robotics and Perception</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction to Robotics and Perception
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S10_introduction.html">
   1. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S11_intro_state.html">
     1.1. Representing State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S12_intro_actions.html">
     1.2. Robot Actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S13_intro_sensing.html">
     1.3. Sensing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S14_intro_perception.html">
     1.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S15_intro_decision.html">
     1.5. Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S16_intro_learning.html">
     1.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S20_sorter_intro.html">
   2. A Trash Sorting Robot
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S21_sorter_state.html">
     2.1. Modeling the World State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S22_sorter_actions.html">
     2.2. Actions for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S23_sorter_sensing.html">
     2.3. Sensors for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S24_sorter_perception.html">
     2.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S25_sorter_decision_theory.html">
     2.5. Decision Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S26_sorter_learning.html">
     2.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S30_vacuum_intro.html">
   3. A Robot Vacuum Cleaner
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S31_vacuum_state.html">
     3.1. Modeling the State of the Vacuum Cleaning Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S32_vacuum_actions.html">
     3.2. Actions over time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S33_vacuum_sensing.html">
     3.3. Dynamic Bayes Nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S34_vacuum_perception.html">
     3.4. Perception with Graphical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S35_vacuum_decision.html">
     3.5. Markov Decision Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S36_vacuum_RL.html">
     3.6. Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S40_logistics_intro.html">
   4. Warehouse Robots in 2D
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S41_logistics_state.html">
     4.1. Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S42_logistics_actions.html">
     4.2. Moving in 2D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S43_logistics_sensing.html">
     4.3. Sensor Models with Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S44_logistics_perception.html">
     4.4. Localization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S45_logistics_planning.html">
     4.5. Planning for Logistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S46_logistics_learning.html">
     4.6. Some System Identification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S50_diffdrive_intro.html">
   5. A Mobile Robot With Simple Kinematics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S51_diffdrive_state.html">
     5.1. State Space for a Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S52_diffdrive_actions.html">
     5.2. Motion Model for the Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S53_diffdrive_sensing.html">
     5.3. Robot Vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S54_diffdrive_perception.html">
     5.4. Computer Vision 101
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S55_diffdrive_planning.html">
     5.5. Path Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S56_diffdrive_learning.html">
     5.6. Deep Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="S60_driving_intro.html">
   6. Autonomous Vehicles
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="S61_driving_state.html">
     6.1. Planar Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S62_driving_actions.html">
     6.2. Kinematics for Driving
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.3. Sensing for Autonomous Vehicles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S64_driving_perception.html">
     6.4. SLAM
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S70_drone_intro.html">
   7. Autonomous Drones in 3D
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S71_drone_state.html">
     7.1. Moving in Three Dimensions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S72_drone_actions.html">
     7.2. Multi-rotor Aircraft
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S73_drone_sensing.html">
     7.3. Sensing for Drones
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/S63_driving_sensing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gtbook/robotics"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS63_driving_sensing.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/gtbook/robotics/main?urlpath=tree/S63_driving_sensing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lidar">
   6.3.1. LIDAR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ray-intersection">
   6.3.2. Ray Intersection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulating-a-2d-lidar">
   6.3.3. Simulating a 2D LIDAR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#geometry-of-a-moving-lidar">
   6.3.4. Geometry of a Moving LIDAR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-lidar-scans">
   6.3.5. Real LIDAR Scans
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-3d-maps">
   6.3.6. Creating 3D Maps
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Sensing for Autonomous Vehicles</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lidar">
   6.3.1. LIDAR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ray-intersection">
   6.3.2. Ray Intersection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulating-a-2d-lidar">
   6.3.3. Simulating a 2D LIDAR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#geometry-of-a-moving-lidar">
   6.3.4. Geometry of a Moving LIDAR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-lidar-scans">
   6.3.5. Real LIDAR Scans
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-3d-maps">
   6.3.6. Creating 3D Maps
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S63_driving_sensing.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="tex2jax_ignore mathjax_ignore section" id="sensing-for-autonomous-vehicles">
<h1><span class="section-number">6.3. </span>Sensing for Autonomous Vehicles<a class="headerlink" href="#sensing-for-autonomous-vehicles" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><p>LIDAR once, LIDAR twice…</p>
</div></blockquote>
<p><strong>This Section is still in draft mode and was released for adventurous spirits (and TAs) only.</strong></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<div align='center'>
<img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S63-Autonomous%20Vehicle%20with%20LIDAR%20and%20cameras-00.jpg?raw=1' style='height:256 width:100%'/>
</div>
</div></div>
</div>
<div class="section" id="lidar">
<h2><span class="section-number">6.3.1. </span>LIDAR<a class="headerlink" href="#lidar" title="Permalink to this headline">¶</a></h2>
<p>LIDAR (LIght raDAR) is a technology that measures distance to an object by using laser light and the “Time of Flight” or <strong>ToF</strong> principle. There are several variants in use, and the simplest to explain is the <strong>direct ToF</strong> sensor, which sends out a short pulse and measures the elapsed time <span class="math notranslate nohighlight">\(\Delta t\)</span> for the light to bounce off an abject and return to a detector collocated with the laser pulse emitter. If the object is situated at a distance <span class="math notranslate nohighlight">\(d\)</span>, we then have</p>
<div class="math notranslate nohighlight">
\[
\Delta t = \frac{2 d}{c} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(c\approx300,000km/s\)</span> is the speed of light. For example, for an object at 15m, we have</p>
<div class="math notranslate nohighlight">
\[
\Delta t \approx \frac{2 \times 0.015}{300,000} = 0.1\mu s
\]</div>
<p>Assuming we can measure <span class="math notranslate nohighlight">\(\Delta t\)</span> accurately, we can then easily compute the distance:</p>
<div class="math notranslate nohighlight">
\[
d= c\frac{\Delta t}{2}
\]</div>
<p>In a <em>scanning LIDAR</em>, there is typically one detector, whereas in a <em>flash LIDAR</em> a single pulse is emitted in a wide field of view, and an <em>array</em> of detectors, akin to a CCD sensor, is used to detect the returning light pulses in multiple directions at once.</p>
<p>However, <em>indirect</em> time of flight sensors are more prevalent in robotics and autonomous driving applications. Unfortunately, measuring elapsed times at the nano-second scale is difficult and expensive, and the amount of light energy that needs to be emitted for direct ToF can also be a problem from an eye-safety perspective. Hence, a more widely used principle is <strong>indirect ToF</strong>, where the light is emitted with a waveform, e.g., a sine wave, and the returned light is correlated with the amplitude of the emitted light to calculate a phase-shift. The elapsed time <span class="math notranslate nohighlight">\(\Delta t\)</span> and distance <span class="math notranslate nohighlight">\(d\)</span> can then be calculated from the phase-shift.</p>
<p>Two common scanning LIDAR technologies are in use for robotics: <strong>2D LIDAR</strong>, which consists of a single laser beam that is rotated around a fixed axis, and <strong>3D LIDAR</strong>, which has multiple laser/detector pairs rotated at different inclinations. 2D LIDARs are also often deployed on aircraft to create highly detailed digital elevation maps (DEMs), where the 3D dimension is provided by the aircraft’s forward motion. Finally, LIDAR altimeters are even deployed from satellites in orbit around Earth or <a class="reference external" href="https://pgda.gsfc.nasa.gov/products/62">other planets</a>.</p>
</div>
<div class="section" id="ray-intersection">
<h2><span class="section-number">6.3.2. </span>Ray Intersection<a class="headerlink" href="#ray-intersection" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Intersecting rays is as easy as a dot product</p>
</div></blockquote>
<p>To perform inference about the environment with LIDAR, we have to model how LIDAR beams interact with it, which in the case of polygonal objects comes down to line-line or line-plane intersections.</p>
<p>We examine the 2D line-line intersection case first. Assume we have a line in 2D with equation <span class="math notranslate nohighlight">\(\hat{n} \cdot p = d\)</span>, where <span class="math notranslate nohighlight">\(\hat{n}\)</span> is a normal vector and <span class="math notranslate nohighlight">\(d&gt;0\)</span> is the distance of the line to the origin. The hat notation signifies that the normal vector <span class="math notranslate nohighlight">\(\hat{n}\)</span> is normalized to length 1. Then if we have a ray of points <span class="math notranslate nohighlight">\(p = \hat{r} s\)</span> where <span class="math notranslate nohighlight">\(\hat{r}\)</span> is the <strong>ray direction</strong> and <span class="math notranslate nohighlight">\(s&gt;0\)</span> is a scalar, then we can find the intersection by plugging the ray equation into the line equation</p>
<div class="math notranslate nohighlight">
\[
\hat{n} \cdot (\hat{r} s) = d,
\]</div>
<p>where <span class="math notranslate nohighlight">\(a \cdot b \doteq a^T b = b^T a\)</span> is the standard vector dot product. We find the range <span class="math notranslate nohighlight">\(s\)</span> to the object as:</p>
<div class="math notranslate nohighlight">
\[
s = \frac{d}{\hat{n} \cdot \hat{r}}.
\]</div>
<p>We can ensure that <span class="math notranslate nohighlight">\(s&gt;0\)</span> and avoid a division by zero by checking the dot product before doing the division:</p>
<div class="math notranslate nohighlight">
\[
\hat{n} \cdot \hat{r} &gt; 0
\]</div>
<p>The following code implements this code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">intersect</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Intersect line/plane (n,d) with ray given by direction r.&quot;&quot;&quot;</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">r</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span> <span class="o">/</span> <span class="n">cos</span> <span class="k">if</span> <span class="n">cos</span><span class="o">&gt;</span><span class="mi">0</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">intersect</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">d</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
</div>
<p>The story above generalizes <em>completely</em> to 3D, where with <span class="math notranslate nohighlight">\(\hat{n}\in\mathbb{R}^3\)</span> and <span class="math notranslate nohighlight">\(p\in\mathbb{R}^3\)</span> the equation <span class="math notranslate nohighlight">\(\hat{n} \cdot p = d\)</span> defines a <em>plane</em> in 3D:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">intersect</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">d</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="simulating-a-2d-lidar">
<h2><span class="section-number">6.3.3. </span>Simulating a 2D LIDAR<a class="headerlink" href="#simulating-a-2d-lidar" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Use the intersection, Luke!</p>
</div></blockquote>
<p>We now put the above to work to simulate a 2D LIDAR sensor situated at the origin. A relatively low-cost 2D LIDAR sensor is a SICK Tim1xx, which has a field of view of 200 degrees and a resolution of one beam per degree. Hence, we expect to simulate 200 measurements.</p>
<p>In our simple simulation code below, we create an environment from infinite lines. A more powerful simulator would allow for line <em>segments</em>, but infinite lines are enough to illustrate the principle. We create a corridor-like environment with three sides:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">north</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="mf">2.5</span>
<span class="n">east</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">8</span>
<span class="n">south</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">2.5</span>
</pre></div>
</div>
</div>
</div>
<p>Then we simply create 200 rays, ranging from -100 degrees to 100 degrees, with 0 facing due east:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">rays</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">angle</span><span class="p">))</span><span class="o">.</span><span class="n">matrix</span><span class="p">()[</span><span class="mi">0</span><span class="p">,:]</span> <span class="k">for</span> <span class="n">angle</span> <span class="ow">in</span> <span class="n">angles</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">rays</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="n">rays</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">scaleanchor</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">scaleratio</span> <span class="o">=</span> <span class="mi">1</span><span class="p">);</span> <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S63_driving_sensing_13_0.png" src="_images/S63_driving_sensing_13_0.png" />
</div>
</div>
<p>Finally, we do the intersection:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scan</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rays</span><span class="p">:</span>
    <span class="n">ranges</span> <span class="o">=</span> <span class="p">[</span><span class="n">intersect</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="n">north</span><span class="p">,</span> <span class="n">east</span><span class="p">,</span> <span class="n">south</span><span class="p">]]</span>
    <span class="n">_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmin</span><span class="p">(</span><span class="n">ranges</span><span class="p">)</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="n">_range</span> <span class="o">*</span> <span class="n">r</span>
    <span class="n">scan</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scan_x</span><span class="p">,</span> <span class="n">scan_y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">scan</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">scan_x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">scan_y</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">scaleanchor</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">scaleratio</span> <span class="o">=</span> <span class="mi">1</span><span class="p">);</span> <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S63_driving_sensing_16_0.png" src="_images/S63_driving_sensing_16_0.png" />
</div>
</div>
<p>As you can see, the resulting scan of this corridor-like environment does not yield in a uniform sampling on the walls. Because the rays are distributed uniformly in <em>angle</em> space, they graze the walls at increasingly shallow angles, and the distance between successive intersection points increases. Of course, that depends on the angle that the wall makes with the rays: the end of the corridor is far away and hence is sampled fairly uniformly. This is a typical pattern when working with LiDAR scanners.</p>
</div>
<div class="section" id="geometry-of-a-moving-lidar">
<h2><span class="section-number">6.3.4. </span>Geometry of a Moving LIDAR<a class="headerlink" href="#geometry-of-a-moving-lidar" title="Permalink to this headline">¶</a></h2>
<p>Above we assumed that the ray is situated at the <em>world</em> origin, but we can also generalize to the case where the rays are defined in a <em>body coordinate frame</em> <span class="math notranslate nohighlight">\((R^w_b,t^w_b)\)</span>. In this case, it is convenient to transform the plane to the body frame. We can do this by expressing a point <span class="math notranslate nohighlight">\(p^w\)</span> in world coordinates as a function of a point in body coordinates: <span class="math notranslate nohighlight">\(p^w = R^w_b p^b + t^w_b\)</span> and plugging that into the plane equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{n}^w \cdot p^w &amp;= d^w \\
\hat{n}^w \cdot \{ R^w_b p^b + t^w_b \} &amp;= d^w \\
(R^w_b)^T \hat{n}^w \cdot p^b &amp;= d^w - \hat{n}^w \cdot t^w_b.
\end{align*}
\end{split}\]</div>
<p>Hence, we obtained a new plane equation in the body frame as
$<span class="math notranslate nohighlight">\(
\hat{n}^b \cdot p^b = d^b
\)</span>$</p>
<p>with transformed plane parameters <span class="math notranslate nohighlight">\(\hat{n}^b \doteq (R^w_b)^T \hat{n}^w\)</span> and <span class="math notranslate nohighlight">\(d^b \doteq d^w - \hat{n}^w \cdot t^w_b\)</span>.</p>
<p>We can use a  <code class="docutils literal notranslate"><span class="pre">gtsam.Pose2</span></code> or <code class="docutils literal notranslate"><span class="pre">gtsam.Pose3</span></code> object to specify the body frame, respectively in 2D or 3D, and then use it to transform plane coordinates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transform_to</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">wTb</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform line/plane (n,d) to body coordinate frame&quot;&quot;&quot;</span>
    <span class="n">wRb</span> <span class="o">=</span> <span class="n">wTb</span><span class="o">.</span><span class="n">rotation</span><span class="p">()</span>
    <span class="n">wtb</span> <span class="o">=</span> <span class="n">wTb</span><span class="o">.</span><span class="n">translation</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">wRb</span><span class="o">.</span><span class="n">matrix</span><span class="p">()</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">wtb</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can do this for lines in 2D</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wTb</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">20</span><span class="p">)),</span> <span class="n">t</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">transform_to</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="n">wTb</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([ 0.93969262, -0.34202014]), 3.0)
</pre></div>
</div>
</div>
</div>
<p>and for planes in 3D:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wTb3</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose3</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Rot3</span><span class="o">.</span><span class="n">Yaw</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">45</span><span class="p">)),</span> <span class="n">t</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">transform_to</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="n">wTb3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([ 0.70710678, -0.70710678,  0.        ]), 4.0)
</pre></div>
</div>
</div>
</div>
<p>With this new functionality we can transform the world model and predict what the scan will look like, in body coordinates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scan2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">transformed</span> <span class="o">=</span> <span class="p">[</span><span class="n">transform_to</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">wTb</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="n">north</span><span class="p">,</span> <span class="n">east</span><span class="p">,</span> <span class="n">south</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rays</span><span class="p">:</span>
    <span class="n">ranges</span> <span class="o">=</span> <span class="p">[</span><span class="n">intersect</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">transformed</span><span class="p">]</span>
    <span class="n">_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmin</span><span class="p">(</span><span class="n">ranges</span><span class="p">)</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="n">_range</span> <span class="o">*</span> <span class="n">r</span>
    <span class="n">scan2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scan2_x</span><span class="p">,</span> <span class="n">scan2_y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">scan2</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">scan2_x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">scan2_y</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">scaleanchor</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">scaleratio</span> <span class="o">=</span> <span class="mi">1</span><span class="p">);</span> <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S63_driving_sensing_26_0.png" src="_images/S63_driving_sensing_26_0.png" />
</div>
</div>
<p>As you can see above, when the robot rotates 20 degrees <em>counter-clockwise</em>, the resulting scan in body coordinates seems to be rotated 20 degrees in the <em>opposite</em> direction. This makes sense! The forward direction for the robot is along the horizontal X-axis, and as you can see in the scan, the robot seems to be “looking” at the correct (upper-right) corner of our little hallway example. This perhaps counter-intuitive behavior is something to always keep in mind when looking at animations of LIDAR scans.</p>
</div>
<div class="section" id="real-lidar-scans">
<h2><span class="section-number">6.3.5. </span>Real LIDAR Scans<a class="headerlink" href="#real-lidar-scans" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>From theory to practice.</p>
</div></blockquote>
<p>3D LIDAR sensors are used in many autonomous driving efforts, e.g., by Waymo, Cruise, Argo, etc. Below we explore some real scans from the <a class="reference external" href="https://www.argoverse.org/av2.html">Argoverse 2 Lidar Dataset</a>.</p>
<p>First, let us look at a single scan, which was acquired using a <a class="reference external" href="https://velodynelidar.com/products/ultra-puck/">Velodyne</a> VLP-32C LIDAR sensor. If you follow the link you see some cool animations of sensor in action, in real traffic. The <code class="docutils literal notranslate"><span class="pre">32</span></code> in the model name reflects the fact for this particular sensor used by Argo, there are 32 separate laser beams at different inclinations, that spin around for a full 360 degree field of view. In this case, the inclination angles are uniformly sampled between -25 and +15 degrees with respect to horizontal, but of course this depends on the application/sensor model.</p>
<p>Several things worth noting when looking at the single scan below, taken when the Argo test car is turning at an intersection:</p>
<ul class="simple">
<li><p>the location of the car (at the origin) is marked by concentric circles formed by the lowest beams.</p></li>
<li><p>the range is approximately 200m, so we can see fairly far down the cross streets.</p></li>
<li><p>occlusion is significant: objects close to the car throw “occlusion shadows”.</p></li>
<li><p>everything is in <em>body coordinates</em>, and the fact that the streets appear rotated betrays that the car is actually turning.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">real_scans</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="n">driving</span><span class="o">.</span><span class="n">read_lidar_points</span><span class="p">(</span><span class="s1">&#39;Figures6/lidar/PC_315967795019746000.ply&#39;</span><span class="p">)}</span>
<span class="n">driving</span><span class="o">.</span><span class="n">visualize_cloud</span><span class="p">(</span><span class="n">real_scans</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">show_grid_lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S63_driving_sensing_29_0.png" src="_images/S63_driving_sensing_29_0.png" />
</div>
</div>
<p>We can also learn some things from looking at two successive scans below.
The scans are slightly rotated and translated from each other, and this will be exactly how we can infer the ego-motion of the car in the next section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">real_scans</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">driving</span><span class="o">.</span><span class="n">read_lidar_points</span><span class="p">(</span><span class="s1">&#39;Figures6/lidar/PC_315967795520065000.ply&#39;</span><span class="p">)</span>
<span class="n">driving</span><span class="o">.</span><span class="n">visualize_clouds</span><span class="p">([</span><span class="n">real_scans</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">real_scans</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">show_grid_lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S63_driving_sensing_31_0.png" src="_images/S63_driving_sensing_31_0.png" />
</div>
</div>
<p>Finally, let us look at scans that are taken a bit further apart, in this case there are 8 scans that we skipped:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">real_scans</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="o">=</span> <span class="n">driving</span><span class="o">.</span><span class="n">read_lidar_points</span><span class="p">(</span><span class="s1">&#39;Figures6/lidar/PC_315967795919523000.ply&#39;</span><span class="p">)</span>
<span class="n">driving</span><span class="o">.</span><span class="n">visualize_clouds</span><span class="p">([</span><span class="n">real_scans</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">real_scans</span><span class="p">[</span><span class="mi">9</span><span class="p">]],</span> <span class="n">show_grid_lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S63_driving_sensing_33_0.png" src="_images/S63_driving_sensing_33_0.png" />
</div>
</div>
<p>By inspecting (you can do this interactively in the colab) you can see that the car is <em>still</em> turning, but as the two scans are further apart the “mis-alginment” is more pronounced.</p>
</div>
<div class="section" id="creating-3d-maps">
<h2><span class="section-number">6.3.6. </span>Creating 3D Maps<a class="headerlink" href="#creating-3d-maps" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Blast points in 3D space!</p>
</div></blockquote>
<p>If we know the exact pose at which a LIDAR scan was taken, we can transform the points in the scan into absolute coordinates, and create an extended <strong>point cloud map</strong> of the environment. In math, suppose we <em>know</em> that a scan was taken at <span class="math notranslate nohighlight">\(T^w_b\)</span>, then we can transform all LIDAR points <span class="math notranslate nohighlight">\(P^b\)</span>, which are given in body coordinates, to world coordinates:</p>
<div class="math notranslate nohighlight">
\[
P^w = \phi(T^w_b, P^b)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\)</span> the the action of <span class="math notranslate nohighlight">\(SE(3)\)</span> on points in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. Earlier in this chapter we saw this can be done by a matrix multiplication, by embedding points in <span class="math notranslate nohighlight">\(\mathbb{R}^4\)</span>, but GTSAM actually implements <span class="math notranslate nohighlight">\(\phi\)</span> directly as <code class="docutils literal notranslate"><span class="pre">Pose3::transformFrom</span></code>. This method can be applied to a single <code class="docutils literal notranslate"><span class="pre">Point3</span></code> (just a <span class="math notranslate nohighlight">\(3\times 1\)</span> vector, really) or on many points simultaneously, by passing in a <span class="math notranslate nohighlight">\(3\times N\)</span> matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scan_in_world</span> <span class="o">=</span> <span class="n">wTb3</span><span class="o">.</span><span class="n">transformFrom</span><span class="p">(</span><span class="n">real_scans</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scan_in_world</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3, 86651)
</pre></div>
</div>
</div>
</div>
<p>We will use this capability in the next section to produce large scale sparse point cloud maps of the environment around an autonomous vehicle.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="S62_driving_actions.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6.2. </span>Kinematics for Driving</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="S64_driving_perception.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.4. </span>SLAM</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Frank Dellaert and Seth Hutchinson<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>