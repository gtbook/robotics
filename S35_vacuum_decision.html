
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3.5. Markov Decision Processes &#8212; Introduction to Robotics and Perception</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=51e3b7cf" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-312077-7"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'UA-312077-7');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'UA-312077-7');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'S35_vacuum_decision';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3.6. Learning to Act Optimally" href="S36_vacuum_RL.html" />
    <link rel="prev" title="3.4. Perception with Graphical Models" href="S34_vacuum_perception.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Introduction to Robotics and Perception - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Introduction to Robotics and Perception - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction to Robotics and Perception
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="S10_introduction.html">1. Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S11_models.html">1.1. Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S12_reasoning.html">1.2. Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S13_math.html">1.3. The Mathematics of Robotics</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S20_sorter_intro.html">2. A Trash Sorting Robot</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S21_sorter_state.html">2.1. Modeling the World State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S22_sorter_actions.html">2.2. Actions for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S23_sorter_sensing.html">2.3. Sensors for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S24_sorter_perception.html">2.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S25_sorter_decision_theory.html">2.5. Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="S26_sorter_learning.html">2.6. Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S27_sorter_summary.html">2.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="S30_vacuum_intro.html">3. A Robot Vacuum Cleaner</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="S31_vacuum_state.html">3.1. Modeling the State of the Vacuum Cleaning Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S32_vacuum_actions.html">3.2. Actions over time</a></li>
<li class="toctree-l2"><a class="reference internal" href="S33_vacuum_sensing.html">3.3. Dynamic Bayes Nets</a></li>
<li class="toctree-l2"><a class="reference internal" href="S34_vacuum_perception.html">3.4. Perception with Graphical Models</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">3.5. Markov Decision Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="S36_vacuum_RL.html">3.6. Learning to Act Optimally</a></li>
<li class="toctree-l2"><a class="reference internal" href="S37_vacuum_summary.html">3.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S40_logistics_intro.html">4. Warehouse Robots in 2D</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S41_logistics_state.html">4.1. Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S42_logistics_actions.html">4.2. Moving in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="S43_logistics_sensing.html">4.3. Sensor Models with Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S44_logistics_perception.html">4.4. Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S45_logistics_planning.html">4.5. Planning for Logistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="S46_logistics_learning.html">4.6. Some System Identification</a></li>
<li class="toctree-l2"><a class="reference internal" href="S47_logistics_summary.html">4.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S50_diffdrive_intro.html">5. A Mobile Robot With Simple Kinematics</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S51_diffdrive_state.html">5.1. State Space for a Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S52_diffdrive_actions.html">5.2. Motion Model for the Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S53_diffdrive_sensing.html">5.3. Robot Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="S54_diffdrive_perception.html">5.4. Computer Vision 101</a></li>
<li class="toctree-l2"><a class="reference internal" href="S55_diffdrive_planning.html">5.5. Path Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S56_diffdrive_learning.html">5.6. Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S57_diffdrive_summary.html">5.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S60_driving_intro.html">6. Autonomous Vehicles</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S61_driving_state.html">6.1. Planar Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="S62_driving_actions.html">6.2. Kinematics for Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S63_driving_sensing.html">6.3. Sensing for Autonomous Vehicles</a></li>
<li class="toctree-l2"><a class="reference internal" href="S64_driving_perception.html">6.4. SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S65_driving_planning.html">6.5. Planning for Autonomous Driving.</a></li>
<li class="toctree-l2"><a class="reference internal" href="S66_driving_DRL.html">6.6. Deep Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S67_driving_summary.html">6.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S70_drone_intro.html">7. Autonomous Drones in 3D</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S71_drone_state.html">7.1. Moving in Three Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S72_drone_actions.html">7.2. Multi-rotor Aircraft</a></li>
<li class="toctree-l2"><a class="reference internal" href="S73_drone_sensing.html">7.3. Sensing for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S74_drone_perception.html">7.4. Visual SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S75_drone_planning.html">7.5. Trajectory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S76_drone_learning.html">7.6. Neural Radiance Fields for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S77_drone_summary.html">7.7. Chapter Summary</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gtbook/robotics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS35_vacuum_decision.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/S35_vacuum_decision.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Markov Decision Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-functions">3.5.1. Reward Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-reward">3.5.2. Expected Reward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-for-a-defined-sequence-of-actions">3.5.3. Utility for a Defined Sequence of Actions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-expected-utility-using-control-tape-rollouts">3.5.4. Approximating Expected Utility Using Control Tape Rollouts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">3.5.4.1. Exercises:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policies">3.5.5. Policies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-value-function-for-a-given-policy">3.5.6. The Value Function (for a given policy)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thought-exercise">3.5.6.1. Thought exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-the-value-function">3.5.7. Approximating the Value Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-value-function">3.5.8. Computing The Value Function*</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">3.5.8.1. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.5.9. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S35_vacuum_decision.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="markov-decision-processes">
<h1><span class="section-number">3.5. </span>Markov Decision Processes<a class="headerlink" href="#markov-decision-processes" title="Link to this heading">#</a></h1>
<blockquote>
<div><p>For controlled Markov chains, planning is the process of choosing the control inputs. This leads to
the concept of Markov decision processes (MDPs).</p>
</div></blockquote>
<a class="reference internal image-reference" href="_images/S35-iRobot_vacuuming_robot-06.jpg"><img alt="Splash image with thinking vacuum robot" class="align-center" src="_images/S35-iRobot_vacuuming_robot-06.jpg" style="width: 40%;" /></a>
<p>Previously in this chapter, we described how conditional probability distributions can be used to model uncertainty
in the effects of actions. We defined the belief state <span class="math notranslate nohighlight">\(b_{k+1}\)</span> to be the posterior probability distribution
for the state at time <span class="math notranslate nohighlight">\(k+1\)</span> given the sequence of actions <span class="math notranslate nohighlight">\(a_1 \dots a_k\)</span>.
In every example, the sequence of actions was predetermined, and we merely calculated probabilities
associated with performing these actions from some specified initial state, described
by a probability distribution <span class="math notranslate nohighlight">\(P(X_1)\)</span>.</p>
<p>In this section, we consider the problem of choosing which actions to execute.
Making these decisions requires that we have quantitative criteria for evaluating actions and their effects.
We encode these criteria using <em>reward functions</em>.
Because the effects of actions are uncertain, it is not possible to know the reward
that will be obtained by executing a specific action (or a sequence of actions). Thus, we will again
invoke the concept of expectation to compute expected future benefits of applying actions.</p>
<p>As we work through these concepts, it will rapidly become apparent that executing a predefined sequence of actions
is not the best way to face the future.
Suppose, for example, that our vacuuming robot wants to move from the office to the living room.
Because it is not possible to know how many times we should “move right”
to arrive at the hallway, we might construct a sequence of actions
to move right many times (to increase the probability of arriving at the dining room),
then move up many times (to increase the probability of arriving at the kitchen),
then move left many times (to increase the probability of arriving at the living room).</p>
<p>This kind of plan might make sense if the robot is unable to know its current location.
For example, moving for a long time to the right should eventually bring the robot to the dining room with
high probability,
but if the robot moves to the right for a short time, it could remain in the office,
arrive at the hallway, or arrive at the dining room.</p>
<p>In contrast, if the robot is able to know its location (using perception), it can act
opportunistically when it reaches the hallway, and immediately move up.
The key idea here is that the best action at any moment in time depends
on the state in which the action is executed.
The recipe of which action to execute in each state is called a <em>policy</em>,
and defining policies and their associated <em>value function</em> is the main goal for this section.</p>
<section id="reward-functions">
<h2><span class="section-number">3.5.1. </span>Reward Functions<a class="headerlink" href="#reward-functions" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Reward functions provide a quantitative way to evaluate the effects of actions.</p>
</div></blockquote>
<p>How should our robot evaluate the effects of actions?
One simple approach might be to specify the robot’s goal, and then determine
how effective an action might be with respect to that goal,
e.g., by using the conditional probability tables for the various actions.
If, for example, the goal is to arrive to the living room, we could evaluate
the probability that a particular action would achieve the goal.</p>
<p>This approach has several limitations.  First, if the robot is not currently in a room
that is adjacent to the living room, it is not clear how to measure the possible progress
toward the living room for a specific action.
If, for example, the robot is in the dining room, would it be better to move
to the kitchen or to the hallway as an intermediate step toward the living room?
Second, this approach does not allow the robot to consider
benefits that could arise from visiting other states.
For example, even though the living room is the goal, arriving to the kitchen might not be
such a bad outcome, if for example, the kitchen floor is also in need of cleaning.
Finally, it may be the case that the benefit of executing an action depends
not only on the destination, but also on the state in which the action
is executed.
For example, entering the living room from the hallway might be less desirable
than entering from the kitchen (if, for example, guests are likely to arrive
in the entrance hallway).</p>
<p>Reward functions provide a very general solution that addresses all of these limitation.
Let <span class="math notranslate nohighlight">\(\cal X\)</span> denote the set of states and <span class="math notranslate nohighlight">\(\cal A\)</span> the set of actions.
The reward function, <span class="math notranslate nohighlight">\(R : {\cal X} \times {\cal A} \times {\cal X} \rightarrow \mathbb{R}\)</span>,
assigns a numeric reward to specific state transitions under specific actions.
In particular, <span class="math notranslate nohighlight">\(R(x_k, a_k, x_{k+1})\)</span> is the reward obtained by arriving to state
<span class="math notranslate nohighlight">\(x_{k+1}\)</span> from state <span class="math notranslate nohighlight">\(x_k\)</span> as a result of executing action <span class="math notranslate nohighlight">\(a_k\)</span>.</p>
<p>In what follows we assume that the reward function is time-invariant, i.e., the benefit of moving up from
the hallway to the living room does not change as time passes.
This allows us to use the more compact notation
<span class="math notranslate nohighlight">\(R(x, a, x')\)</span> to represent the reward obtained by arriving to (the next) state <span class="math notranslate nohighlight">\(x'\)</span>
by executing action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(x\)</span>. We will frequently use this notation in the remainder of the section.</p>
<p>This form of the reward function is very general, allowing us to encode context dependent rewards
that depend on the state in which an action is executed.
It is also common to specify rewards merely as a function of state.
In this case, we denote by <span class="math notranslate nohighlight">\(R(x)\)</span> the reward obtained for arriving to state <span class="math notranslate nohighlight">\(x\)</span>, regardless
of the action that was applied, and regardless of the previous state.
An example of such a reward function is given below, implemented as a python function.
It simply returns a reward of 10 upon entering the living room:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reward_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">next_state</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reward that returns 10 upon entering the living room.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">10.0</span> <span class="k">if</span> <span class="n">next_state</span> <span class="o">==</span> <span class="s2">&quot;Living Room&quot;</span> <span class="k">else</span> <span class="mf">0.0</span>

<span class="nb">print</span><span class="p">(</span><span class="n">reward_function</span><span class="p">(</span><span class="s2">&quot;Kitchen&quot;</span><span class="p">,</span> <span class="s2">&quot;L&quot;</span><span class="p">,</span> <span class="s2">&quot;Living Room&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reward_function</span><span class="p">(</span><span class="s2">&quot;Kitchen&quot;</span><span class="p">,</span> <span class="s2">&quot;L&quot;</span><span class="p">,</span> <span class="s2">&quot;Kitchen&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10.0
0.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="expected-reward">
<h2><span class="section-number">3.5.2. </span>Expected Reward<a class="headerlink" href="#expected-reward" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Maximizing immediate expected reward leads to greedy planning.</p>
</div></blockquote>
<p>Because the effects of actions are uncertain,
there is no way for the robot to deterministically predict the value
of the next state, <span class="math notranslate nohighlight">\(x'\)</span>, for a particular action <span class="math notranslate nohighlight">\(a\)</span>;
therefore, it is not possible to evaluate <span class="math notranslate nohighlight">\(R(x,a,x')\)</span>.
In this case, we can use the <strong>expected reward</strong> to evaluate the stochastic
effects of the action.
As we have seen in the previous chapter, this expectation will provide a good
estimate of the average reward that will be received if action <span class="math notranslate nohighlight">\(a\)</span> is executed from state <span class="math notranslate nohighlight">\(x\)</span> may times,
even if it provides no guarantees about the reward that will be obtained by any specific moment in time.</p>
<p>We denote the expected reward for executing action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(x\)</span>
by <span class="math notranslate nohighlight">\(\bar{R}(x,a)\)</span>,
and its value is obtained by evaluating the expectation</p>
<div class="math notranslate nohighlight">
\[
\bar{R}(x,a) \doteq E[R(x,a,X')] = \sum_{x'} P(x'|x,a) R(x, a, x').
\]</div>
<p>Note that we use the upper case <span class="math notranslate nohighlight">\(X'\)</span> to indicate that the expectation is taken with
respect to the random next state.
Accordingly, the sum is over all possible next states, and the reward <span class="math notranslate nohighlight">\(R(x, a, x')\)</span> for each next
state is weighted by the probability <span class="math notranslate nohighlight">\(P(x'|x,a)\)</span> of arriving to that state from state <span class="math notranslate nohighlight">\(x\)</span> by executing action <span class="math notranslate nohighlight">\(a\)</span>.</p>
<!-- In some cases (e.g., when the transition probabilities are *known*),
it can be convenient to work only with the expected reward, instead of dealing wit $R(x,a,x')$
which some texts do. However, below we will continue to work with the most general formulation. --><p>To <em>calculate</em> the expected reward in code, we first create a multidimensional array <span class="math notranslate nohighlight">\(R(x,a,x')\)</span> that contains the rewards for every possible transition <span class="math notranslate nohighlight">\(x,a \rightarrow x'\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="nb">float</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">room_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">room_y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
            <span class="n">R</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mf">10.0</span> <span class="k">if</span> <span class="n">room_y</span> <span class="o">==</span> <span class="s2">&quot;Living Room&quot;</span> <span class="k">else</span> <span class="mf">0.0</span>

<span class="c1"># For example, taking action &quot;L&quot; in &quot;Kitchen&quot;:</span>
<span class="n">R</span><span class="p">[</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;Kitchen&quot;</span><span class="p">),</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;L&quot;</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([10.,  0.,  0.,  0.,  0.])
</pre></div>
</div>
</div>
</div>
<p>The result above is simple to interpret: if the robot starts in the kitchen and goes left, it obtains a reward of 10 <em>iff</em> it arrives to the living room (index 0), otherwise 0.</p>
<p>Observe that <span class="math notranslate nohighlight">\(R\)</span> is a <em>multidimensional</em> array, with shape <span class="math notranslate nohighlight">\((5, 4, 5)\)</span>, corresponding to the dimensionality of the arguments of the reward function <span class="math notranslate nohighlight">\(R(x,a,x')\)</span>.
We can do the same with the transition probabilities, creating a multidimensional array <span class="math notranslate nohighlight">\(T\)</span> with an entry for every possible transition <span class="math notranslate nohighlight">\(x \rightarrow a \rightarrow x'\)</span> with probability <span class="math notranslate nohighlight">\(P(x'|a,x)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First create a conditional with ids 0, 1, and 2 for state, action, and next state:</span>
<span class="n">conditional</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">DiscreteConditional</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)],</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_spec</span><span class="p">)</span>

<span class="c1"># Now use `enumerate` to iterate over all possible assignments to the conditional:</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="nb">float</span><span class="p">)</span>
<span class="k">for</span> <span class="n">assignment</span><span class="p">,</span> <span class="n">P_y_given_ax</span> <span class="ow">in</span> <span class="n">conditional</span><span class="o">.</span><span class="n">enumerate</span><span class="p">():</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">assignment</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">assignment</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">assignment</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">P_y_given_ax</span>

<span class="c1"># For example, taking action &quot;L&quot; in &quot;Kitchen&quot;:</span>
<span class="n">T</span><span class="p">[</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;Kitchen&quot;</span><span class="p">),</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;L&quot;</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.8, 0.2, 0. , 0. , 0. ])
</pre></div>
</div>
</div>
</div>
<p>As expected, if we execute the action L when the robot is in the kitchen,
we end up in the living room with 80% chance, while staying in place (the kitchen) 20% of the time.</p>
<p>Then, finally, calculating the expected reward <span class="math notranslate nohighlight">\(\bar{R}(x,a)\)</span> for a particular state-action pair <span class="math notranslate nohighlight">\((x,a)\)</span> becomes a simple dot product:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;Kitchen&quot;</span><span class="p">),</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;L&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected reward (Kitchen, L) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">],</span><span class="w"> </span><span class="n">R</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Expected reward (Kitchen, L) = 8.0
</pre></div>
</div>
</div>
</div>
<p>This makes total sense: if we stay in the kitchen, we get zero reward, but <em>if</em> we succeed to move to the living room, we receive a reward of 10. This happens only in 80% of the cases, though, so the <em>expected reward</em> is only 8.</p>
<!-- Note that this computation can also be realized using belief states.  In
particular, if encode the reward function as a vector (as above),
say $R = [10, 0, 0, 0, 0]^T$,
the the expected reward at time $k+1$ is given by

$$E[ R(X_k, a, X_{k+t})] = b_{k+1} R = \left( b_k M_{a}\right) R
$$

in which $M_{a}$ denotes the transition probability matrix associated
to action $ a$. Note that in this case, we the expected reward is computed for a given
prior distribution on $X_k$. If we know with certainty that $X_k =x$, the belief
vector would have a $1$ in the entry corresponding to the current room, and zero's for
all other entries.
Therefore, this form is somewhat more general than that given above. -->
<p>Equipped with the definition of expected reward, we can introduce a first, <strong>greedy planning</strong> algorithm:
Given the current state <span class="math notranslate nohighlight">\(x_k\)</span>, execute the action that maximizes the expected reward:</p>
<div class="math notranslate nohighlight">
\[
a_k^* = \arg  \max_{a \in {\cal A}} E[ R(x_k, a, X_{k+t})]
\]</div>
 <!-- = \arg \max_{a \in {\cal A}}\left( b_k M_{A}\right) R --><p>For example, in the kitchen, we can calculate the expected award for all actions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;Kitchen&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected reward (</span><span class="si">{</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="p">[</span><span class="n">a</span><span class="p">]</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">],</span><span class="w"> </span><span class="n">R</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Expected reward (Kitchen, L) = 8.0
Expected reward (Kitchen, R) = 0.0
Expected reward (Kitchen, U) = 0.0
Expected reward (Kitchen, D) = 0.0
</pre></div>
</div>
</div>
</div>
<p>It turns out that <span class="math notranslate nohighlight">\(8.0\)</span> is the best we can do by greedily choosing a single action, as the expected <em>immediate</em> reward for any other action is zero. Hence, whenever we find ourselves in the Kitchen, a good course of action is to always go left!</p>
</section>
<section id="utility-for-a-defined-sequence-of-actions">
<h2><span class="section-number">3.5.3. </span>Utility for a Defined Sequence of Actions<a class="headerlink" href="#utility-for-a-defined-sequence-of-actions" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Utility is the total discounted reward over finite or infinite horizons.</p>
</div></blockquote>
<p>The greedy strategy above focuses on the immediate benefit of applying an action.
For our robot, and for most robots operating in the real world, it is important to perform
effectively over a prolonged period of time, not merely for the next instant.
In order to maximize long-term benefit, instead of considering only the next-stage reward (as the greedy strategy does)
we could consider the sum of all rewards achieved in the future.
There are two immediate disadvantages to a direct implementation of this approach:</p>
<ul class="simple">
<li><p>First, because the effects of actions are uncertain, it is likely that the further we look
into the future, the more uncertain we will be about the robot’s anticipated state.
Therefore, it makes sense to <em>discount</em> our consideration of rewards that might occur far into the future,
say at times <span class="math notranslate nohighlight">\(X_{k+l}\)</span>, for increasing values of <span class="math notranslate nohighlight">\(l\)</span>.</p></li>
<li><p>Second, it is often convenient to reason with an infinite time horizon, e.g., consider the case when the robot will operate forever.
While this is certainly not a realistic expectation,
reasoning over an infinite time horizon often simplifies the mathematical complexities of planning into the future.
If we merely compute the sum of all future rewards, this sum will diverge to infinity as <span class="math notranslate nohighlight">\(l\)</span> approaches infinity.</p></li>
</ul>
<p>We can deal with both of these disadvantages by multiplying the reward at time step <span class="math notranslate nohighlight">\(k + l\)</span> by a discounting
factor <span class="math notranslate nohighlight">\(\gamma^l\)</span>, with <span class="math notranslate nohighlight">\(0 &lt; \gamma \leq 1\)</span>.
We refer to <span class="math notranslate nohighlight">\(\gamma\)</span> as the <strong>discount factor</strong>
and to the term <span class="math notranslate nohighlight">\(\gamma^l R(x_{k+l},a_{k+l},x_{k+l+1})\)</span> as a <strong>discounted reward.</strong>
Note that for <span class="math notranslate nohighlight">\(\gamma = 1\)</span>, there is no discount, and all future rewards are treated with equal weight.</p>
<p>Suppose the robot executes a sequence of actions, <span class="math notranslate nohighlight">\(a_1, \dots, a_n\)</span>,
starting in state  <span class="math notranslate nohighlight">\(X_1=x_1\)</span>, and passes through
state sequence <span class="math notranslate nohighlight">\(x_1,x_2,x_3\dots x_{n+1}\)</span>.
We define the utility function <span class="math notranslate nohighlight">\(U: {\cal A}^n \times {\cal X}^{n+1} \rightarrow \mathbb{R}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
U(a_1, \dots, a_n, x_1, \dots x_{n+1}) =
R(x_1,a_1, x_2) + \gamma R(x_2, a_2, x_3) + \dots \gamma^{n-1} R(x_{n}, a_{n}, x_{n+1})\]</div>
<p>We can write this more compactly as the summation</p>
<div class="math notranslate nohighlight">
\[
U(a_1, \dots, a_n, x_1, \dots x_{n+1}) =
\sum_{k=1}^{n} = \gamma^{k-1} R(x_k, a_k, x_{k+1}) 
\]</div>
<p>For <span class="math notranslate nohighlight">\(n &lt; \infty\)</span>, we refer to this as a finite-horizon utility function.
Note that influence of future rewards decreases exponentially with the time horizon,
and that the use of <span class="math notranslate nohighlight">\(\gamma^{k-1}\)</span> ensures that the sum will converge for the infinite horizon case (under mild assumptions about <span class="math notranslate nohighlight">\(R\)</span>).</p>
<p>The expression above is defined for a specific sequence of actions and a specific sequence of states.
When planning, as we have noted above, we are unable to know with certainty the future states.
We can, again, deal with this difficulty by computing the <em>expected</em> utility for a
given sequence of actions,
<span class="math notranslate nohighlight">\(E[U(a_1, \dots, a_n, X_1, \dots X_n)]\)</span>.
We can now formulate a slightly more sophisticated version of our planning problem. Choose the sequence of actions <span class="math notranslate nohighlight">\(a_{1:n}\)</span> that maximizes the expected utility:</p>
<div class="math notranslate nohighlight">
\[ a_1^*, \dots a_n^* = \arg  \max_{a_1 \dots a_n \in {\cal A}^n} E[U(a_1, \dots, a_n, X_1, \dots X_{n+1})]
\]</div>
<p>As formulated above, this problem could be solved by merely enumerating every possible action sequence,
and choosing the sequence that maximizes the expectation.
Obviously this is not a computationally tractable approach.
Not only does the number of possible action sequences grow exponentially with the time horizon <span class="math notranslate nohighlight">\(n\)</span>,
but the computation of the expectation for a specific action sequence is also computationally heavy.
We can, however, approximate this optimization process using the concept of rollouts, as we will now see.</p>
</section>
<section id="approximating-expected-utility-using-control-tape-rollouts">
<h2><span class="section-number">3.5.4. </span>Approximating Expected Utility Using Control Tape Rollouts<a class="headerlink" href="#approximating-expected-utility-using-control-tape-rollouts" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Rollin’, rollin’, rollin’…</p>
</div></blockquote>
<p>Consider the computation required to determine the expected utility for a sequence of only two actions:</p>
<div class="math notranslate nohighlight">
\[
E[U(a_1,a_2, X_1, X_2, X_3)] = E[R(X_1, a_1, X_2) + \gamma R(X_2, a_2, X_3)]
\]</div>
<p>Computing the expectation requires summing over all combinations of values for states
<span class="math notranslate nohighlight">\(X_1, X_2, X_3\)</span>.  Clearly, as <span class="math notranslate nohighlight">\(n\)</span> becomes large, this is not a tractable computation.
We can approximate this computation by recalling the relationship between
the expected value for a probability distribution and the average value over
many realizations of the underlying random process.
Namely, as we have seen before, the expected value of a random variable (in this case the value of the utility function)
corresponds to what we expect to observe as the average of that random variable over many trials.
This immediately suggests an approximation algorithm:</p>
<ul class="simple">
<li><p>Generate many sample trajectories for the action sequence <span class="math notranslate nohighlight">\(a_1, \dots, a_n\)</span>;</p></li>
<li><p>compute the average of the discounted rewards over these sample trajectories.</p></li>
</ul>
<p>A specific sequence of actions, <span class="math notranslate nohighlight">\(a_1, \dots, a_n\)</span> is sometimes called a <strong>control tape</strong>.
The process of computing a sample trajectory and
evaluating the corresponding utility is called a <strong>rollout</strong> for the control tape.
Each rollout produces one sample trajectory, which gives one corresponding utility value.
As an example, suppose the robot starts in the office executes the sequence
<span class="math notranslate nohighlight">\(a_1, a_2, a_3, a_4\)</span> = R,U,L,L (i.e., the robot executes actions <em>move right, move up, move left, move left</em>)
in an attempt to reach the living room.</p>
<p>Because we took care to specify the Markov chain above in reverse topological order, we can then use the GTSAM method <code class="docutils literal notranslate"><span class="pre">sample</span></code> to do the rollouts for us:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">markovChain</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">DiscreteBayesNet</span><span class="p">()</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)):</span>
    <span class="n">markovChain</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_spec</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: A Markov chain with 5 states and 4 actions. The actions are given.</span>
<span class="c1">#| label: fig:markov_chain_5</span>
<span class="n">show</span><span class="p">(</span><span class="n">markovChain</span><span class="p">,</span> <span class="n">hints</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;A&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">},</span> <span class="n">boxes</span><span class="o">=</span><span class="p">{</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f6ac5ae6510a1e97c809c529cdeaa6b98cae769f7028f901ea367cf9ed90a809.svg" src="_images/f6ac5ae6510a1e97c809c529cdeaa6b98cae769f7028f901ea367cf9ed90a809.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">perform_rollout</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Roll out states given actions as a dictionary&quot;&quot;&quot;</span>
    <span class="nb">dict</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="nb">dict</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">x1</span>
    <span class="n">given</span> <span class="o">=</span> <span class="n">VARIABLES</span><span class="o">.</span><span class="n">assignment</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">markovChain</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">given</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To execute a specific rollout for the control tape R,U,L,L, we can use the following code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="p">{</span><span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="s2">&quot;R&quot;</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span><span class="s2">&quot;L&quot;</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span><span class="s2">&quot;L&quot;</span><span class="p">}</span>
<span class="n">rollout</span> <span class="o">=</span> <span class="n">perform_rollout</span><span class="p">(</span><span class="s2">&quot;Office&quot;</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
<span class="n">pretty</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<table class='DiscreteValues'>
  <thead>
    <tr><th>Variable</th><th>value</th></tr>
  </thead>
  <tbody>
    <tr><th>A1</th><td>R</td></tr>
    <tr><th>A2</th><td>U</td></tr>
    <tr><th>A3</th><td>L</td></tr>
    <tr><th>A4</th><td>L</td></tr>
    <tr><th>X1</th><td>Office</td></tr>
    <tr><th>X2</th><td>Office</td></tr>
    <tr><th>X3</th><td>Office</td></tr>
    <tr><th>X4</th><td>Office</td></tr>
    <tr><th>X5</th><td>Office</td></tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>It is important to remember that any individual rollout corresponds to a sample trajectory from a stochastic process.
If you execute the above code several times, you should observe that the robot does not always arrive to the living room.</p>
<p>The code below executes the rollout and computes the corresponding utility for the sample trajectory.
Sample trajectories that do not arrive to the living room will have zero utility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">rollout</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return state, action, next_state triple for given rollout at time k.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">R</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">rollout_reward</span><span class="p">(</span><span class="n">rollout</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate reward for a given rollout&quot;&quot;&quot;</span>
    <span class="n">discounted_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">gamma</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">reward</span><span class="p">(</span><span class="n">rollout</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">horizon</span><span class="p">)]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">discounted_rewards</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">rollout_reward</span><span class="p">(</span><span class="n">rollout</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>The following code executes 20 rollouts for the action sequence R,U,L,L, and prints the utility for each sample trajectory.
You can see that in many cases the robot fails to arrive to the living room (thus earning zero utility)!
This is because each of the first two actions have a 0.2 probability of failure, and if either of these fail, the robot is unable to reach the living room using this control tape.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">control_tape_reward</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate reward given a dictionary of actions&quot;&quot;&quot;</span>
    <span class="n">rollout</span> <span class="o">=</span> <span class="n">perform_rollout</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rollout_reward</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>

<span class="nb">print</span><span class="p">([</span><span class="n">control_tape_reward</span><span class="p">(</span><span class="s2">&quot;Office&quot;</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 0.0, 30.0, 30.0, 0.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 0.0]
</pre></div>
</div>
</div>
</div>
<p>Finally, the expected utility of the action sequence R,U,L,L is approximated
by simply averaging over all 20 rollouts:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">control_tape_reward</span><span class="p">(</span><span class="s2">&quot;Office&quot;</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>19.5
</pre></div>
</div>
</div>
</div>
<section id="exercises">
<h3><span class="section-number">3.5.4.1. </span>Exercises:<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Execute both code segments above multiple times and observe the effect.</p></li>
<li><p>The rewards above seems to be always either zero or 30. Why is that?</p></li>
</ul>
</section>
</section>
<section id="policies">
<h2><span class="section-number">3.5.5. </span>Policies<a class="headerlink" href="#policies" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>A policy is a function that specifies which action to take in each state.</p>
</div></blockquote>
<p>In the example above, the control tape rollout for the action sequence R,U,L,L failed many times.
The reason for this failure may seem obvious: the robot executed the same sequence of actions,
regardless of the state trajectory.
If the robot had been able to choose its actions based on the current state,
it would have chosen to move right until it reached the hallway,
at which time it would have chosen to move up until reaching the living room.
Clearly, the robot could make better choices if it were allowed to dynamically
choose which action to execute based on its current state.</p>
<p>A <strong>policy</strong>, <span class="math notranslate nohighlight">\(\pi: {\cal X} \rightarrow {\cal A}\)</span> is a mapping from states to actions.
Specifying a policy instead of a control tape has the potential to significantly improve the robot’s performance, by adapting the action sequence based on the actual state trajectory that occurs during execution.</p>
<p>The code below defines a fairly intuitive policy. If in the office, move right. If in the dining room or kitchen, move left.
If in the hallway or living room, move up.
Note we implement policies as a simple list in python, so <code class="docutils literal notranslate"><span class="pre">pi[0]</span></code> is the the action taken in state with index 0 (the Living Room). We return an index into the <code class="docutils literal notranslate"><span class="pre">action_space</span></code> to make the code following it efficient:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">RIGHT</span> <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">)</span>
<span class="n">LEFT</span>  <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;L&quot;</span><span class="p">)</span>
<span class="n">UP</span>    <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">)</span>
<span class="n">DOWN</span>  <span class="o">=</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;D&quot;</span><span class="p">)</span>

<span class="n">reasonable_policy</span> <span class="o">=</span> <span class="p">[</span><span class="n">UP</span><span class="p">,</span> <span class="n">LEFT</span><span class="p">,</span> <span class="n">RIGHT</span><span class="p">,</span> <span class="n">UP</span><span class="p">,</span> <span class="n">LEFT</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Once we have a given policy, <span class="math notranslate nohighlight">\(\pi\)</span>, we can compute a policy rollout in a manner analogous to computing control tape rollouts described above.
In particular, at each state, instead of sampling from the distribution
<span class="math notranslate nohighlight">\(P(X_{k+1} | a_k, x_k)\)</span> we sample from the distribution
<span class="math notranslate nohighlight">\(P(X_{k+1} | \pi(x_k), x_k)\)</span>. In other words, instead of simulating a pre-specified action <span class="math notranslate nohighlight">\(a_k\)</span>, we choose <span class="math notranslate nohighlight">\(a_k = \pi(x_k)\)</span>.</p>
<p>Here is a function that computes a rollout given a policy, rather than a control tape:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_rollout</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="n">N</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Roll out states given a policy pi, for given horizon.&quot;&quot;&quot;</span>
    <span class="n">rollout</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">DiscreteValues</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x1</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">horizon</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="n">rollout</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">rollout</span><span class="p">[</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">a</span>
        <span class="n">next_state_distribution</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">DiscreteDistribution</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">next_state_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">rollout</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">horizon</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">rollout</span>

<span class="n">pretty</span><span class="p">(</span><span class="n">policy_rollout</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;Office&quot;</span><span class="p">),</span> <span class="n">reasonable_policy</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<table class='DiscreteValues'>
  <thead>
    <tr><th>Variable</th><th>value</th></tr>
  </thead>
  <tbody>
    <tr><th>A1</th><td>R</td></tr>
    <tr><th>A2</th><td>U</td></tr>
    <tr><th>A3</th><td>U</td></tr>
    <tr><th>A4</th><td>U</td></tr>
    <tr><th>X1</th><td>Office</td></tr>
    <tr><th>X2</th><td>Hallway</td></tr>
    <tr><th>X3</th><td>Living Room</td></tr>
    <tr><th>X4</th><td>Living Room</td></tr>
    <tr><th>X5</th><td>Living Room</td></tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Our policy-based rollouts seem to be much better!</p>
<p>In summary, we proposed two methods to plan an action sequence, a greedy approach that maximizes the expected reward for executing a single action,
and an optimization-based method that chooses a fixed action sequence to maximize the
expected utility of the action sequence.
We described how to implement the computations required for the latter approach using control tape rollouts.</p>
<p>With the introduction of policies, planning is reduced to the search for an appropriate
policy.
In the best case, this policy would maximize the expected utility.
Because policies are state-dependent, the combinatorics of enumerating all possible
policies precludes any approach that attempts to explicitly enumerate candidate policies.
Instead, we will focus on methods that explore the utility associated to
a policy.
As we will now see, this is somewhat more complicated than exploring the utility
for a fixed sequence of actions.</p>
</section>
<section id="the-value-function-for-a-given-policy">
<h2><span class="section-number">3.5.6. </span>The Value Function (for a given policy)<a class="headerlink" href="#the-value-function-for-a-given-policy" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>The value function <span class="math notranslate nohighlight">\(V^\pi\)</span> measures the expected utility from each state, under a given policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
</div></blockquote>
<p>We would now like to characterize the quality of any given policy.
Above, we defined the utility for a specific sequence of <span class="math notranslate nohighlight">\(n\)</span> actions as</p>
<div class="math notranslate nohighlight">
\[
U(a_1, \dots, a_n, x_1, \dots x_{n+1}) =
R(x_1,a_1, x_2) + \gamma R(x_2, a_2, x_3) + \dots \gamma^{n-1} R(x_{n}, a_{n}, x_{n+1})
\]</div>
<p>and used the expected utility, <span class="math notranslate nohighlight">\(E[U(a_1, \dots, a_n, X_1, \dots X_{n+1})]\)</span> as a quantitative measure
of the efficacy of the specific action sequence.
We can apply this same type of reasoning for a policy, <span class="math notranslate nohighlight">\(\pi\)</span>.
When evaluating policies, it is typical to use a discounted
reward over an infinite time horizon.
In this case, we define the <strong>value function</strong> <span class="math notranslate nohighlight">\(V^\pi:{\cal X} \rightarrow \mathbb{R}\)</span> for policy <span class="math notranslate nohighlight">\(\pi\)</span> as</p>
<div class="math notranslate nohighlight">
\[
V^\pi(x_1) \doteq E [R(x_1, \pi(x_1), X_2) + \gamma R(X_2, \pi(X_2), X_3) + \gamma^2 R(X_3, \pi(X_3), X_4) + \dots]
\]</div>
<p>where the expectation is taken over the possible values
of the states <span class="math notranslate nohighlight">\(X_2, X_3 \dots\)</span>.
Note that the policy <span class="math notranslate nohighlight">\(\pi\)</span> is deterministic, but that <span class="math notranslate nohighlight">\(\pi(X_i)\)</span> is the random action that results
from applying the deterministic policy to the stochastic state <span class="math notranslate nohighlight">\(X_i\)</span>.</p>
<!-- In the definition for $V^\pi$, the argument is given as $x_1$, the initial state.
We can easily generalize this to arbitrary time steps as

$$
V^\pi(x_k) \doteq E [R(x_k, \pi(x_k), X_{k+1}) + \gamma R(X_{k+1}, \pi(X_{k+1}), X_{k+2}) + \dots]
$$ --><p>The value function for a policy can be written in a nice recursive form
that can be obtained by the following derivation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
V^\pi(x_1) &amp;=
E [R(x_1, \pi(x_1), X_2) + \gamma R(X_2, \pi(X_2), X_3) + \gamma^2 R(X_3, \pi(X_3), X_4) + \dots] \\
&amp;= \sum_{x_2} P(x_2|x_1, \pi(x_1)) R(x_1, \pi(x_1), x_2) \\
&amp; + \sum_{x_2} P(x_2|x_1, \pi(x_1)) E [\gamma R(x_2, \pi(x_2), X_3) + \gamma^2 R(X_3, \pi(X_3), X_4) + \dots]\\
&amp;= \sum_{x_2} P(x_2|x_1, \pi(x_1)) \{R(x_1, \pi(x_1), x_2) + \gamma V^\pi(x_2)\}
\end{aligned}
\end{split}\]</div>
<p>where the second line is obtained by explicitly writing the summation for the expectation
taken with respect to the random state <span class="math notranslate nohighlight">\(X_2\)</span>,
and the third line is obtained by noticing that
<span class="math notranslate nohighlight">\(E [\gamma R(x_2, \pi(x_2), X_3) + \gamma^2 R(X_3, \pi(X_3), X_4) + \dots] = \gamma V^\pi(x_2)\)</span>.</p>
<p>We can apply the distributivity property to this expression to obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
V^\pi(x_1) &amp;= \sum_{x_2} P(x_2|x_1, \pi(x_1)) R(x_1, \pi(x_1), x_2) + \gamma  \sum_{x_2} P(x_2|x_1, \pi(x_1))  V^\pi(x_2) \\
&amp;= \bar{R}(x_1,\pi(x_1)) + \gamma \sum_{x_2} P(x_2|x_1, \pi(x_1)) V^\pi(x_2)
\end{aligned}
\end{split}\]</div>
<p>in which the term <span class="math notranslate nohighlight">\(\bar{R}(x_1,\pi(x_1))\)</span> is the expected reward for applying action <span class="math notranslate nohighlight">\(a = \pi(x_1)\)</span> in state <span class="math notranslate nohighlight">\(x_1\)</span>, which we already know how to compute from the reward function <span class="math notranslate nohighlight">\(R\)</span> and transition probabilities <span class="math notranslate nohighlight">\(T\)</span>.
By now substituting <span class="math notranslate nohighlight">\(x\)</span> for <span class="math notranslate nohighlight">\(x_1\)</span>, and <span class="math notranslate nohighlight">\(x'\)</span> for <span class="math notranslate nohighlight">\(x_{2}\)</span> we can generalize this expression
to apply to the state <span class="math notranslate nohighlight">\(x\)</span> at any arbitrary time:</p>
<div class="math notranslate nohighlight">
\[V^\pi(x) = \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^\pi(x')\]</div>
<p>This has a very nice interpretation: the value of a state under a given policy is the expected reward <span class="math notranslate nohighlight">\(\bar{R}(x,\pi(x))\)</span> under that policy, <em>plus</em> the discounted expected value of the next state.</p>
<section id="thought-exercise">
<h3><span class="section-number">3.5.6.1. </span>Thought exercise<a class="headerlink" href="#thought-exercise" title="Link to this heading">#</a></h3>
<p>Without a discount factor, we would always be hopeful that if we take one more step, we will find a pot of gold. Reflect on what various values for the discount factor mean in real life.</p>
</section>
</section>
<section id="approximating-the-value-function">
<h2><span class="section-number">3.5.7. </span>Approximating the Value Function<a class="headerlink" href="#approximating-the-value-function" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>We can use rollouts to approximate the value function as well.</p>
</div></blockquote>
<p>Just as we approximated the expected utility of an action sequence
using control tape rollouts,
we can approximate the value function by sampling over a number of policy rollouts.
This process provides a <em>Monte Carlo approximation</em> of the value function for a given policy.
Of course we cannot apply a policy rollout over an infinite time horizon, so we apply
the rollout only to some finite number of time steps, say <span class="math notranslate nohighlight">\(N_{\rm{ro}}\)</span>.</p>
<p>Note this is an approximation in <em>two</em> ways:</p>
<ol class="arabic simple">
<li><p>We approximate the expectation by averaging over <span class="math notranslate nohighlight">\(N_{\rm{Samples}}\)</span> sample trajectories.</p></li>
<li><p>We only roll out for  <span class="math notranslate nohighlight">\(N_{\rm{ro}}\)</span> steps.
We can improve on this by increasing the number of samples or by increasing the horizon, but at a cost linear in their product, i.e.,
<span class="math notranslate nohighlight">\(O(N_{\rm{ro}} N_{\rm{Samples}})\)</span>:</p></li>
</ol>
<p>You might worry that evaluating sample paths of length <span class="math notranslate nohighlight">\(N_{\rm{ro}}\)</span> could lead to significant errors
in our approximation.
In fact, it is easy to determine an upper bound on this error.
Since <span class="math notranslate nohighlight">\(R(x, a, x') \)</span> is finite,
we know that there is some upper bound <span class="math notranslate nohighlight">\(R_{\rm{max}}\)</span> such that <span class="math notranslate nohighlight">\(R(x, a, x') &lt; R_{\rm{max}}\)</span>, for all
possible <span class="math notranslate nohighlight">\(x,a,x'\)</span>.
We can use this fact to find a bound on <span class="math notranslate nohighlight">\(V^\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[V^\pi(x_k)
\leq \max_{x_k, x_{k+1}, \dots} \sum_{i=0}^\infty \gamma ^i R(x_{k+i},\pi(x_{k+i}), x_{k+i+1})
\leq \sum_{i=0}^\infty \gamma ^i R_{\rm{max}} = \frac{R_{\rm{max}}}{1 - \gamma}\]</div>
<p>in which the final term applies for <span class="math notranslate nohighlight">\(0 &lt; \gamma &lt; 1\)</span>.
This expression can then be used to bound the error</p>
<div class="math notranslate nohighlight">
\[
V^\pi(x) - \sum_{i=0}^{N_{\rm{ro}}}\gamma ^i R(x_i,\pi(xi), x_{i+1})\]</div>
<p>Because we have functions to sample a policy rollout <em>and</em> to calculate the value of a rollout, the code is simple enough:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">approximate_value_function</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">nr_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Approximate the value function by performing `nr_samples` rollouts</span>
<span class="sd">        starting from x1, and averaging the result.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rollouts</span> <span class="o">=</span> <span class="p">[</span><span class="n">policy_rollout</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">horizon</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nr_samples</span><span class="p">)]</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">rollout_reward</span><span class="p">(</span><span class="n">rollout</span><span class="p">,</span> <span class="n">horizon</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span> <span class="k">for</span> <span class="n">rollout</span> <span class="ow">in</span> <span class="n">rollouts</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">/</span><span class="n">nr_samples</span>

<span class="n">nr_samples</span><span class="o">=</span><span class="mi">10</span>
<span class="k">for</span> <span class="n">x1</span><span class="p">,</span> <span class="n">room</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
    <span class="n">V_x1</span> <span class="o">=</span> <span class="n">approximate_value_function</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">reasonable_policy</span><span class="p">,</span> <span class="n">nr_samples</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;V(</span><span class="si">{</span><span class="n">room</span><span class="si">}</span><span class="s2">) ~ </span><span class="si">{</span><span class="n">V_x1</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>V(Living Room) ~ 34.39
V(Kitchen) ~ 29.49
V(Office) ~ 16.00
V(Hallway) ~ 32.39
V(Dining Room) ~ 19.25
</pre></div>
</div>
</div>
</div>
<p>The above calculation was done with a horizon of <span class="math notranslate nohighlight">\(N=5\)</span> and 10 samples. Of course, we can use more samples and a longer horizon to obtain a much more accurate estimate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nr_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">horizon</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">VARIABLES</span><span class="o">.</span><span class="n">discrete_series</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">horizon</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">VARIABLES</span><span class="o">.</span><span class="n">discrete_series</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">horizon</span><span class="p">),</span> <span class="n">vacuum</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x1</span><span class="p">,</span> <span class="n">room</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
    <span class="n">V_x1</span> <span class="o">=</span> <span class="n">approximate_value_function</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">reasonable_policy</span><span class="p">,</span> <span class="n">nr_samples</span><span class="p">,</span> <span class="n">horizon</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;V(</span><span class="si">{</span><span class="n">room</span><span class="si">}</span><span class="s2">) ~ </span><span class="si">{</span><span class="n">V_x1</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>V(Living Room) ~ 99.43
V(Kitchen) ~ 97.64
V(Office) ~ 84.74
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>V(Hallway) ~ 97.37
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>V(Dining Room) ~ 85.89
</pre></div>
</div>
</div>
</div>
</section>
<section id="computing-the-value-function">
<h2><span class="section-number">3.5.8. </span>Computing The Value Function*<a class="headerlink" href="#computing-the-value-function" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>For any <em>fixed</em> policy we can compute the exact value of value function.</p>
</div></blockquote>
<p>We can compute the exact value of <span class="math notranslate nohighlight">\(V^\pi\)</span> by solving a system of linear equations.
Recall our recursive definition of the value function:</p>
<div class="math notranslate nohighlight">
\[V^\pi(x) = \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^\pi(x')\]</div>
<p>This equation holds for every possible value <span class="math notranslate nohighlight">\(x\)</span> for the state.
Hence, if there are <span class="math notranslate nohighlight">\(n\)</span> possible states, we obtain <span class="math notranslate nohighlight">\(n\)</span> linear equations in <span class="math notranslate nohighlight">\(n\)</span> unknowns.
Each of these <span class="math notranslate nohighlight">\(n\)</span> equations is obtained by evaluating <span class="math notranslate nohighlight">\(V^\pi(x)\)</span> for a specific value of <span class="math notranslate nohighlight">\(x\)</span>.
Collecting the unknown <span class="math notranslate nohighlight">\(V^\pi\)</span> terms on the left hand side and the known <span class="math notranslate nohighlight">\(\bar{R}(x,\pi(x))\)</span>
terms on the right hand side, we obtain</p>
<div class="math notranslate nohighlight">
\[V^\pi(x) - \gamma \sum_{x'} P(x'|x, \pi(x)) V^\pi(x') = \bar{R}(x,\pi(x)).\]</div>
<p>To make this explicit yet concise for our vacuum cleaning robot example,
let us define the <em>scalar</em> <span class="math notranslate nohighlight">\(T^\pi_{xy}\doteq P(y|x,\pi(x))\)</span> as the transition probability from state <span class="math notranslate nohighlight">\(x\)</span> to state <span class="math notranslate nohighlight">\(y\)</span> under policy <span class="math notranslate nohighlight">\(\pi\)</span>.
In addition, we use the abbreviations L,K,O,H, and D for the rooms, and use the shorthand <span class="math notranslate nohighlight">\(V^\pi_x\doteq V^\pi(x)\)</span> for the value of state <span class="math notranslate nohighlight">\(x\)</span> under policy <span class="math notranslate nohighlight">\(\pi\)</span>.
Using this notation, we can evaluate the above expression.
For <span class="math notranslate nohighlight">\(x = L\)</span>, we obtain</p>
<div class="math notranslate nohighlight">
\[ 
V^\pi_L - \gamma \sum_{x'\in {L,K,O,H,D}} T^\pi_{Lx'} V^\pi_{x'} = \bar{R}(L,\pi(L))
\]</div>
<p>or, after some algebra:</p>
<div class="math notranslate nohighlight">
\[ 
(1 - \gamma T^\pi_{LL}) V^\pi_{L}
- \gamma T^\pi_{LK} V^\pi_{K} 
- \gamma T^\pi_{LO} V^\pi_{O} 
- \gamma T^\pi_{LH} V^\pi_{H} 
- \gamma T^\pi_{LD} V^\pi_{D} 
= \bar{R}(L,\pi(L))
\]</div>
<p>If we apply this same process for each of the five rooms,
we obtain the following five equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
(1 - \gamma T^\pi_{LL}) V^\pi_{L}
- \gamma T^\pi_{LK} V^\pi_{K} 
- \gamma T^\pi_{LO} V^\pi_{O} 
- \gamma T^\pi_{LH} V^\pi_{H} 
- \gamma T^\pi_{LD} V^\pi_{D} 
&amp;= \bar{R}(L,\pi(L))
\\
- \gamma T^\pi_{KL} V^\pi_{L}
+ (1 - \gamma T^\pi_{KK}) V^\pi_{K} 
- \gamma T^\pi_{KO} V^\pi_{O} 
- \gamma T^\pi_{KH} V^\pi_{H} 
- \gamma T^\pi_{KD} V^\pi_{D} 
&amp;= \bar{R}(K,\pi(K))
\\
- \gamma T^\pi_{OL} V^\pi_{L}
- \gamma T^\pi_{OK} V^\pi_{K} 
+ (1 - \gamma T^\pi_{OO}) V^\pi_{O} 
- \gamma T^\pi_{OH} V^\pi_{H} 
- \gamma T^\pi_{OD} V^\pi_{D} 
&amp;= \bar{R}(O,\pi(O))
\\
- \gamma T^\pi_{HL} V^\pi_{L}
- \gamma T^\pi_{HK} V^\pi_{K} 
- \gamma T^\pi_{HO} V^\pi_{O} 
+ (1 - \gamma T^\pi_{HH}) V^\pi_{H} 
- \gamma T^\pi_{HD} V^\pi_{D} 
&amp;= \bar{R}(H,\pi(H))
\\
- \gamma T^\pi_{DL} V^\pi_{L}
- \gamma T^\pi_{DK} V^\pi_{K} 
- \gamma T^\pi_{DO} V^\pi_{O} 
- \gamma T^\pi_{DH} V^\pi_{H} 
+ (1 - \gamma T^\pi_{DD}) V^\pi_{D} 
&amp;= \bar{R}(D,\pi(D))
\end{aligned}
\end{split}\]</div>
<p>The unknowns in these equations are
<span class="math notranslate nohighlight">\(V^\pi_{L}, V^\pi_{K}, V^\pi_{O}, V^\pi_{H}, V^\pi_{D}\)</span>. All of the other terms are
either transition probabilities or expected rewards, whose values are either given,
or can easily be computed.</p>
<p>In code, this becomes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_value_system</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate A, b matrix of linear system for value computation.&quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">5</span><span class="p">,),</span> <span class="nb">float</span><span class="p">)</span>
    <span class="n">AA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">room</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="c1"># action under policy</span>
        <span class="n">b</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">@</span> <span class="n">R</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="c1"># expected reward under policy pi</span>
        <span class="n">AA</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">T</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">]</span>
        <span class="n">AA</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">AA</span><span class="p">,</span><span class="n">b</span>

<span class="k">def</span> <span class="nf">calculate_value_function</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate value function for given policy&quot;&quot;&quot;</span>
    <span class="n">AA</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">calculate_value_system</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">AA</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here is the linear system for the policy <code class="docutils literal notranslate"><span class="pre">reasonable_policy</span></code> (which is really almost always wrong):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">AA</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">calculate_value_system</span><span class="p">(</span><span class="n">reasonable_policy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;A =</span><span class="se">\n</span><span class="si">{</span><span class="n">AA</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b = </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A =
[[ 0.1  -0.   -0.   -0.   -0.  ]
 [-0.72  0.82 -0.   -0.   -0.  ]
 [-0.   -0.    0.82 -0.72 -0.  ]
 [-0.72 -0.   -0.    0.82 -0.  ]
 [-0.   -0.   -0.   -0.72  0.82]]
b = [10.  8.  0.  8.  0.]
</pre></div>
</div>
</div>
</div>
<p>When we calculate the value function <span class="math notranslate nohighlight">\(V^\pi\)</span> under the policy <code class="docutils literal notranslate"><span class="pre">reasonable_policy</span></code> we see that our <em>exact</em> value function was well approximated by the Monte Carlo estimate above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">value_for_pi</span> <span class="o">=</span> <span class="n">calculate_value_function</span><span class="p">(</span><span class="n">reasonable_policy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;V(reasonable_policy):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">room</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vacuum</span><span class="o">.</span><span class="n">rooms</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">room</span><span class="si">:</span><span class="s2">12</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value_for_pi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>V(reasonable_policy):
  Living Room : 100.00
  Kitchen     : 97.56
  Office      : 85.66
  Hallway     : 97.56
  Dining Room : 85.66
</pre></div>
</div>
</div>
</div>
<section id="exercise">
<h3><span class="section-number">3.5.8.1. </span>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">#</a></h3>
<p>Why is the value function in the living room <span class="math notranslate nohighlight">\(100\)</span> and not the immediate reward <span class="math notranslate nohighlight">\(10\)</span>?</p>
</section>
</section>
<section id="summary">
<h2><span class="section-number">3.5.9. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Markov Decision Processes or MDPs can be used to model decision making in a stochastic environment, albeit with complete knowledge of the state. It is a rich subject, and we introduced many new concepts in this section:</p>
<ul class="simple">
<li><p>The reward function, <span class="math notranslate nohighlight">\(R : {\cal X} \times {\cal A} \times {\cal X} \rightarrow \mathbb{R}\)</span>.</p></li>
<li><p>The expected reward, <span class="math notranslate nohighlight">\(\bar{R}(x,a)\)</span> for executing action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(x\)</span>, and a corresponding greedy planning algorithm.</p></li>
<li><p>The utility function, <span class="math notranslate nohighlight">\(U: {\cal A}^n \times {\cal X}^{n+1} \rightarrow \mathbb{R}\)</span>, as the sum of discounted rewards.</p></li>
<li><p>The notion of rollouts to approximate the expected utility of actions.</p></li>
<li><p>The policy <span class="math notranslate nohighlight">\(\pi: {\cal X} \rightarrow {\cal A}\)</span>, as a mapping from states to actions.</p></li>
<li><p>The value function, <span class="math notranslate nohighlight">\(V^\pi:{\cal X} \rightarrow \mathbb{R}\)</span>, associated with a given policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
<li><p>The use of policy rollouts to approximate the value function <span class="math notranslate nohighlight">\(V^\pi\)</span>.</p></li>
<li><p>Exact calculation of the value function for a fixed policy.</p></li>
</ul>
<p>We’ll leave the concepts of an optimal policy and how to compute it for the next section. There are two other extensions to MDPs that we did not cover:</p>
<ul class="simple">
<li><p>Partially Observable MDPs (or POMDPS) are appropriate when we cannot directly observe the state.</p></li>
<li><p>Reinforcement learning, a way to learn MDP policies from <em>data</em>. This will be covered in the next section as well.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="S34_vacuum_perception.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3.4. </span>Perception with Graphical Models</p>
      </div>
    </a>
    <a class="right-next"
       href="S36_vacuum_RL.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3.6. </span>Learning to Act Optimally</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-functions">3.5.1. Reward Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-reward">3.5.2. Expected Reward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-for-a-defined-sequence-of-actions">3.5.3. Utility for a Defined Sequence of Actions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-expected-utility-using-control-tape-rollouts">3.5.4. Approximating Expected Utility Using Control Tape Rollouts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">3.5.4.1. Exercises:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policies">3.5.5. Policies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-value-function-for-a-given-policy">3.5.6. The Value Function (for a given policy)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thought-exercise">3.5.6.1. Thought exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-the-value-function">3.5.7. Approximating the Value Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-value-function">3.5.8. Computing The Value Function*</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">3.5.8.1. Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.5.9. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank Dellaert and Seth Hutchinson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>