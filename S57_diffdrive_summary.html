
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5.7. Chapter Summary &#8212; Introduction to Robotics and Perception</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=51e3b7cf" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=c73c0f3e"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'S57_diffdrive_summary';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6. Autonomous Vehicles" href="S60_driving_intro.html" />
    <link rel="prev" title="5.6. Deep Learning" href="S56_diffdrive_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Introduction to Robotics and Perception - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Introduction to Robotics and Perception - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="S10_introduction.html">1. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S11_models.html">1.1. Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S12_reasoning.html">1.2. Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S13_math.html">1.3. The Mathematics of Robotics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S20_sorter_intro.html">2. A Trash Sorting Robot</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S21_sorter_state.html">2.1. Modeling the World State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S22_sorter_actions.html">2.2. Actions for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S23_sorter_sensing.html">2.3. Sensors for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S24_sorter_perception.html">2.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S25_sorter_decision_theory.html">2.5. Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="S26_sorter_learning.html">2.6. Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S27_sorter_summary.html">2.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S30_vacuum_intro.html">3. A Robot Vacuum Cleaner</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S31_vacuum_state.html">3.1. Modeling the State of the Vacuum Cleaning Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S32_vacuum_actions.html">3.2. Actions over time</a></li>
<li class="toctree-l2"><a class="reference internal" href="S33_vacuum_sensing.html">3.3. Dynamic Bayesian Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="S34_vacuum_perception.html">3.4. Perception with Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S35_vacuum_decision.html">3.5. Markov Decision Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="S36_vacuum_RL.html">3.6. Learning to Act Optimally</a></li>
<li class="toctree-l2"><a class="reference internal" href="S37_vacuum_summary.html">3.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S40_logistics_intro.html">4. Warehouse Robots in 2D</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S41_logistics_state.html">4.1. Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S42_logistics_actions.html">4.2. Moving in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="S43_logistics_sensing.html">4.3. Sensor Models with Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S44_logistics_perception.html">4.4. Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S45_logistics_planning.html">4.5. Planning for Logistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="S46_logistics_learning.html">4.6. Some System Identification</a></li>
<li class="toctree-l2"><a class="reference internal" href="S47_logistics_summary.html">4.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="S50_diffdrive_intro.html">5. A Mobile Robot With Simple Kinematics</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="S51_diffdrive_state.html">5.1. State Space for a differential-drive robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S52_diffdrive_actions.html">5.2. Motion Model for the Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S53_diffdrive_sensing.html">5.3. Cameras for Robot Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="S54_diffdrive_perception.html">5.4. Computer Vision 101</a></li>
<li class="toctree-l2"><a class="reference internal" href="S55_diffdrive_planning.html">5.5. Path Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S56_diffdrive_learning.html">5.6. Deep Learning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">5.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S60_driving_intro.html">6. Autonomous Vehicles</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S61_driving_state.html">6.1. Planar Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="S62_driving_actions.html">6.2. Kinematics for Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S63_driving_sensing.html">6.3. Sensing for Autonomous Vehicles</a></li>
<li class="toctree-l2"><a class="reference internal" href="S64_driving_perception.html">6.4. SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S65_driving_planning.html">6.5. Planning for Autonomous Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S66_driving_DRL.html">6.6. Deep Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S67_driving_summary.html">6.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S70_drone_intro.html">7. Autonomous Drones in 3D</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="S71_drone_state.html">7.1. Moving in Three Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S72_drone_actions.html">7.2. Multi-rotor Aircraft</a></li>
<li class="toctree-l2"><a class="reference internal" href="S73_drone_sensing.html">7.3. Sensing for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S74_drone_perception.html">7.4. Visual SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S75_drone_planning.html">7.5. Trajectory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S76_drone_learning.html">7.6. Neural Radiance Fields for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S77_drone_summary.html">7.7. Chapter Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">8. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="genindex.html">Index</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/S57_diffdrive_summary.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter Summary</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models">5.7.1. Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning">5.7.2. Reasoning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background-and-history">5.7.3. Background and History</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-summary">
<h1><span class="section-number">5.7. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Link to this heading">#</a></h1>
<a class="reference internal image-reference" href="_images/S50-Two-wheeled_Toy_Robot-02.jpg"><img alt="Splash image with steampunk differential-drive robot" class="align-center" src="_images/S50-Two-wheeled_Toy_Robot-02.jpg" style="width: 60%;" />
</a>
<p>In the first four chapters of this book, we have considered motion and sensor models, but we haven’t
really discussed the mechanics of motion or the implementation of sensors.
Instead, we proposed abstract models (e.g., sensor models of the form <span class="math notranslate nohighlight">\(z = h(x)\)</span>)
and various probability distributions to characterize uncertainty (e.g., conditional Gaussians),
but we have not bothered to relate these models to the actual devices that effect motion and sensing.
In this chapter, we began to remedy this deficiency, for motion models by studying the kinematics of differential
drive robots, and for sensor models by studying cameras.
We then developed appropriate planning, perception, and learning approaches for these models.
For both modeling and reasoning, the methods we introduced in this chapter were quite different from
what we have seen in previous chapters.</p>
<section id="models">
<h2><span class="section-number">5.7.1. </span>Models<a class="headerlink" href="#models" title="Link to this heading">#</a></h2>
<p>In previous chapters, our models have often been probabilistic in nature. Motion models have been expressed as conditional
probability distributions that depend on current state and applied action,
and sensor models have been expressed as conditional probability distributions that depend on state.
In this chapter, we develop geometric models without consideration of uncertainty. In general, geometric
models together with models for how uncertainty enters the system can be used to derive the kinds of conditional
probability models we have seen in previous chapters.</p>
<p>We began the chapter with a formal definition of a <em>configuration</em> and of the <em>configuration space</em> for robotic systems.
For the simple differential-drive robot of this chapter, we rigidly attach a coordinate frame to the robot,
with origin at the midpoint of the axle and <span class="math notranslate nohighlight">\(x\)</span>-axis parallel to the direction of motion.
The pose of this frame (position of its origin, and orientation of its axes) defines a configuration
of the robot, and the set of all configurations, <span class="math notranslate nohighlight">\({\cal Q} = \mathbb{R}^2 \times [0, 2\pi),\)</span> defines
the configuration space of the robot.
We then showed how it is possible to determine the location of any point on the robot using the
robot’s configuration.</p>
<p>The motion model for the differential-drive robot relates the angular velocities of the two
actuated wheels to the linear and angular velocity of the body-attached coordinate frame.
The <em>forward velocity kinematics</em> in the body-attached frame are given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-03291b1a-37b0-4461-b897-e11493d0c810">
<span class="eqno">(5.60)<a class="headerlink" href="#equation-03291b1a-37b0-4461-b897-e11493d0c810" title="Permalink to this equation">#</a></span>\[\begin{equation}
v_x = \frac{r}{2} (\dot{\phi}_R + \dot{\phi}_L)
\,\,\,\,\,\,\,\,
\omega = \frac{r}{L} (\dot{\phi}_R - \dot{\phi}_L)
\end{equation}\]</div>
<p>which can be expressed with respect to the world coordinate frame, as a function
of the configuration of the robot, as</p>
<div class="amsmath math notranslate nohighlight" id="equation-ac8da3fd-7b5a-4886-be35-2d7f3a05d6ca">
<span class="eqno">(5.61)<a class="headerlink" href="#equation-ac8da3fd-7b5a-4886-be35-2d7f3a05d6ca" title="Permalink to this equation">#</a></span>\[\begin{equation}
v^{\mathrm{world}}=
\begin{bmatrix} v_x \cos \theta \\ v_x \sin \theta \\ \dot{\theta} \end{bmatrix}
=\begin{bmatrix} \frac{r}{2} (\dot{\phi}_R + \dot{\phi}_L) \cos\theta  \\  \frac{r}{2} (\dot{\phi}_R + \dot{\phi}_L) \sin\theta  \\ \frac{r}{L} (\dot{\phi}_R - \dot{\phi}_L)\end{bmatrix}
\end{equation}\]</div>
<p>Using the differential kinematic equations, we can easily calculate either (a) the linear and angular velocity that will result from applied
wheel angular velocities, or (b) the wheel angular velocities that are required to achieve the desired angular and linear
velocities of the robot.</p>
<p>Cameras are sensors that compute a two-dimensional, discrete representation of a three-dimensional scene.
In this chapter, we studied the geometry of the image formation process.
There are essentially two aspects to image formation: projection and discretization.
Points in the world are projected onto the image plane, which contains a discrete array of sensing elements,
each of which corresponds to one pixel.</p>
<p>For most cameras, the imaging geometry can be modeled using perspective projection, which corresponds
to a pinhole camera model. In such systems, the physical lens apparatus implements an effective pinhole,
through which each light ray passes. The pinhole is located at the lens focal center,
at distance <span class="math notranslate nohighlight">\(F\)</span> in front of the image plane. For a 3D point
with coordinates <span class="math notranslate nohighlight">\((X,Y,Z)\)</span>, the corresponding image plane coordinates are given by
the perspective projection equations:</p>
<div class="amsmath math notranslate nohighlight" id="equation-135bcc57-8cca-4bee-afd8-8a11aca19035">
<span class="eqno">(5.62)<a class="headerlink" href="#equation-135bcc57-8cca-4bee-afd8-8a11aca19035" title="Permalink to this equation">#</a></span>\[\begin{equation}
X_I = - F \frac{X}{Z} ~~~~~~
Y_I = - F \frac{Y}{Z} 
\end{equation}\]</div>
<p>In the discretization stage, image plane coordinates are mapped to pixel coordinates by
incorporating the pixel size and the location of the image center (also called the <em>principal point</em>),
whose image coordinates are given as <span class="math notranslate nohighlight">\((u_0, v_0)\)</span>.
For the case of square pixels, such that there are <span class="math notranslate nohighlight">\(k\)</span> pixels per unit length, we combine
pixel size with focal length into a single constant, <span class="math notranslate nohighlight">\(f = Fk\)</span>,
and obtain the projection equations for pixel coordinates <span class="math notranslate nohighlight">\((r,c)\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-1f303386-a488-4f99-a174-c2dbf2d845ee">
<span class="eqno">(5.63)<a class="headerlink" href="#equation-1f303386-a488-4f99-a174-c2dbf2d845ee" title="Permalink to this equation">#</a></span>\[\begin{equation}
r = \left\lfloor v_0 + f \frac{Y}{Z} \right\rfloor ~~~~~~ c = \left\lfloor u_0 + f \frac{X}{Z} \right\rfloor
\end{equation}\]</div>
<p>These equations describe the <em>forward</em> imaging geometry, i.e., how points in the world project onto the imaging sensor.
In robotics, we often need to solve the inverse problem: computing the 3D coordinates of a point in the world
that corresponds to a certain pixel in the image.
While this inverse problem cannot be solved using only the information in a single image,
it is easily solved using a stereo pair of cameras, provided (a) the two cameras both view the relevant portion of the scene,
(b) the geometric relationship between the two cameras is known.
In the simplest stereo rig, the geometric relationship is restricted to a pure translation along the <span class="math notranslate nohighlight">\(x\)</span>-axis of the image
frame.
In this case, the <span class="math notranslate nohighlight">\(Z\)</span> coordinate of an image point can be computed using the <em>fundamental stereo equation</em>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f9d4849e-1494-459b-8eef-61c9a5a4bfe8">
<span class="eqno">(5.64)<a class="headerlink" href="#equation-f9d4849e-1494-459b-8eef-61c9a5a4bfe8" title="Permalink to this equation">#</a></span>\[\begin{equation}
Z = B \frac{f}{d}.
\end{equation}\]</div>
<p>in which <span class="math notranslate nohighlight">\(B\)</span> is the distance between the origins of the two camera coordinate frames,
and <span class="math notranslate nohighlight">\(d\)</span> is the disparity for the projected point in left and right images.</p>
</section>
<section id="reasoning">
<h2><span class="section-number">5.7.2. </span>Reasoning<a class="headerlink" href="#reasoning" title="Link to this heading">#</a></h2>
<p>In previous chapters, we have developed several methods to reason with uncertainty.
We frequently relied on the concept of expectation to optimize with respect
to average performance over long time horizons.
In this chapter, we focused on the deterministic aspects of computer vision and robot motion.
Rather than finding solutions that work well on average, we presented algorithms
that consistently find good solutions, whether for problems related to scene understanding using
computer vision or path planning for mobile robots.
A motion planning system that only avoids collision <em>on average</em> is not acceptable in many applications.
Likewise, computer vision systems that recognize objects well <em>on average</em> can fail
spectacularly and catastrophically when used to guide robots operating in the real world,
such as for self-driving cars (which we will see in Chapter 6).</p>
<p>Computer vision involves reasoning about images, and images are collections of pixels,
therefore, the first computer vision algorithms that we described were simple
image processing algorithms that operated on collections of pixels in an input image
to produce and output image.
Many of these image processing algorithms were merely variations on
convolution (e.g., smoothing, edge detection)
This motivated the introduction of convolutional neural networks (CNNs),
and led naturally to the introduction of <em>deep neural networks</em> (or <em>deep nets</em>),
which can be constructed using sequential layers of CNNs connected by <em>activation layers</em>.
We then saw how deep nets can be used to solve classical computer vision
problems such as semantic segmentation and depth estimation.</p>
<p id="index-0">Our introduction of deep nets focused on their use as simple computational units.
We provided the specific weights in the network that were required to implement specific, and known, operations.
However, the real power of deep nets is that they can be used trained to implement operations
that are not well understood, and that have not been successfully implemented using traditional
computer algorithms.
The secret to this power is that it is possible to <em>learn</em> the parameters of the network!
In this chapter, we saw how to do this using <em>supervised learning</em>.
With supervised learning, we are provided with a training set
and a loss function.
We train the network by choosing parameters that minimize this loss function with respect to the training set.
While there are numerous candidate loss functions, we confined our attention to <em>mean squared error</em> and
<em>cross entropy</em> loss functions.
The usual way to train the network is to compute a gradient of the loss function with respect
to the network’s parameters, and use some variation of gradient descent to find the optimal parameters.
In cases for which the loss function includes additive terms for each element of the training data,
the gradient computations can be extremely expensive,
but we showed that we can reduce the required computation by computing the gradient only at randomly
selected terms in the loss function, an approach known as <em>stochastic gradient descent</em>.
It is worth noting that the method of randomization used in stochastic gradient descent
is fundamentally different from our previous use of random variables to represent uncertain quantities.
Until this point, we have used probability theory to characterize uncertainty in the robot’s understanding
of the world, of its actions in the world, and of the data provided by its sensors.
When using randomized algorithms, the stochastic behavior is a property of the <em>algorithm</em>, and <em>not</em> of the
robot or its environment.</p>
<p>In previous chapters, we framed the planning problem in terms of maximizing an expected reward function.
In this chapter, we looked at a more specific problem: planning a collision-free path for the robot
from its initial configuration to a specified goal configuration.
We began by showing how this problem could be solved by using a special case of value iteration
(no stochastic component, negative reward for collision, high reward for reaching the goal),
but it was immediately clear that this approach would not scale to more complex robotic systems,
since the computational complexity of a dynamic programming approach scales exponentially with the
dimension of the configuration space.</p>
<p>Even though value iteration doesn’t scale to higher dimensions, the idea of following the gradient
of the reward function would be useful if there were a way to access this gradient without
incurring the cost of computing the value function.
Path planners that use artificial potential fields attempt to achieve this by constructing a function that behaves similarly
to the value function, but that is specified in closed form for each configuration,
thus eliminating the computational cost of dynamic programming.
These algorithms work by evaluating the gradient of the potential function at the current configuration,
and using gradient descent to make progress toward the goal, essentially focusing attention on computing
the potential function along a single path to the goal, rather than computing a global representation of the value function.
The problem with these methods is that the potential functions are almost never equivalent to the value function.
In particular, the value function has a single optimum at the goal,
while an artificial potential function typically has many local minima, which will trap gradient descent methods.
Thus, with artificial potential field methods, we trade computational complexity for completeness, i.e., we lose the
guarantee that a solution will be found when a solution exists, but we gain the possibility of finding solutions quickly.</p>
<p>In the same way that it was used to deal with the computational burden of computing gradients of
complex loss functions, randomization has also been used to solve complex path planning problems.
The basic idea is straightforward: generate random sample configurations,
and then use fast and efficient motion planning algorithms to find local paths
between these samples.
In order for these methods to be effective, two conditions must hold:
the random selection of configurations should ensure good coverage of the entire configuration
space (as the number of samples increases), and the local path planning problems should be easy to solve
(typically this is the case when the distance between two configurations is small).</p>
<p>Probabilistic road maps (PRMs) are graphs whose vertices correspond to sample configurations,
and whose edges correspond to local paths between sample configurations.
They are constructed by iteratively generating random samples and connecting each new sample
to nearby samples that are already represented in the graph. This is an offline process, intended to build an
approximate, global representation of the free configuration space.
Equipped with a PRM, path planning is reduced to (a) finding a path from the initial configuration to some
vertex in the graph, (b) finding a path from the goal configuration to a vertex in the graph, and (c) searching
the graph for a path that connects these two vertices.</p>
<p>Rapidly-Exploring Random Trees (RRTs) are constructed by iteratively adding vertices to a tree whose root
corresponds to the initial configuration, until a leaf is added that can be easily connected to the goal configuration.
The trick that makes RRTs work is that the randomly generated samples are not themselves
added to the tree; instead, they are used to determine which vertex in the existing tree should be expanded.
Specifically, the vertex <span class="math notranslate nohighlight">\(q_\mathrm{near}\)</span> in the existing tree that is nearest to the
randomly generated configuration <span class="math notranslate nohighlight">\(q_\mathrm{rand}\)</span> is selected for expansion,
and the expansion is achieved by constructing a local path from <span class="math notranslate nohighlight">\(q_\mathrm{near}\)</span> in the
direction of <span class="math notranslate nohighlight">\(q_\mathrm{rand}\)</span>, terminating at the configuration <span class="math notranslate nohighlight">\(q_\mathrm{new}\)</span>, which is added to the tree.
If <span class="math notranslate nohighlight">\(q_\mathrm{rand}\)</span> is generated by sampling from a uniform distribution, the RRT will tend to explore
the entire configuration space, ensuring (in an asymptotic, probabilistic sense) that a path will be found
if one exists.</p>
<p>Finally, it is worth noting that, apart from value iteration, all of the planning methods introduced
in this chapter can be considered as sampling-based algorithms for path planning.
For the case of PRMs and RRTs, the samples are randomly generated, while for potential fields methods,
we can consider each step in the gradient descent algorithm as a sample along the path
currently under exploration.
While the sampling strategies for these two approaches (random generation vs. gradient descent) are quite
different, both share the property that paths are constructed by generating sample configurations and building
a graph (a general graph for PRMs, a tree for RRTs, and a simple linear graph for potential fields).
The resulting graphs essentially represent one-dimensional subsets of the configuration space,
and the goal is to construct these sets so that their connectivity matches the connectivity of
the free configuration space with respect to the problem at hand.</p>
</section>
<section id="background-and-history">
<h2><span class="section-number">5.7.3. </span>Background and History<a class="headerlink" href="#background-and-history" title="Link to this heading">#</a></h2>
<p>The kinematics of differential-drive robots are described in detail in the <a class="reference external" href="https://mitpress.mit.edu/9780262015356/introduction-to-autonomous-mobile-robots/">Introduction to Autonomous Mobile Robots</a> book we mentioned earlier, by <span id="id1">Siegwart <em>et al.</em> [<a class="reference internal" href="bibliography.html#id53" title="Roland Siegwart, Illah Reza Nourbakhsh, and Davide Scaramuzza. Introduction to Autonomous Mobile Robots. MIT Press, 2011. ISBN 9780262015356. URL: https://mitpress.mit.edu/9780262015356/introduction-to-autonomous-mobile-robots/.">2011</a>]</span></p>
<p>The first mathematically rigorous book on robot motion planning was written by Latombe
in the early nineties <span id="id2">[<a class="reference internal" href="bibliography.html#id36" title="Jean-Claude Latombe. Robot Motion Planning. Kluwer Academic Publishers, USA, 1991. ISBN 0792391292.">Latombe, 1991</a>]</span>.
Brian Eno once remarked that only about 1,000 people bought the first Velvet Underground album, but every one of them formed a rock ‘n’ roll band.
Latombe’s book held this status in robotics; if you owned it, likely as not, you went on to become a researcher in robot motion planning.
In subsequent years, <a class="reference external" href="https://mitpress.mit.edu/9780262033275/principles-of-robot-motion/">Principles of Robot Motion</a> by <span id="id3">Choset <em>et al.</em> [<a class="reference internal" href="bibliography.html#id12" title="Howie Choset, Kevin M. Lynch, Seth Hutchinson, George Kantor, Wolfram Burgard, Lydia E. Kavraki, and Sebastian Thrun. Principles of Robot Motion. MIT Press, 2005. ISBN 9780262033275. URL: https://mitpress.mit.edu/9780262033275/principles-of-robot-motion/.">2005</a>]</span> and <a class="reference external" href="https://lavalle.pl/planning/">Planning Algorithms</a> by <span id="id4">LaValle [<a class="reference internal" href="bibliography.html#id37" title="Steven M. LaValle. Planning Algorithms. Cambridge University Press, 2006. ISBN 9780521862059. URL: https://lavalle.pl/planning/.">2006</a>]</span>
provided updated treatments of the rapidly expanding field.</p>
<p>Excellent introductions to the material on machine learning can be found in
<a class="reference external" href="https://www.deeplearningbook.org/">Deep Learning</a> by <span id="id5">Goodfellow <em>et al.</em> [<a class="reference internal" href="bibliography.html#id24" title="Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. ISBN 9780262035613. URL: https://www.deeplearningbook.org.">2016</a>]</span>
and <a class="reference external" href="https://d2l.ai/">Dive into Deep Learning</a> by <span id="id6">Zhang <em>et al.</em> [<a class="reference internal" href="bibliography.html#id66" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://d2l.ai.">2023</a>]</span>.
The historically important papers references in Section 5.4 are the Neocognitron paper by <span id="id7">Fukushima [<a class="reference internal" href="bibliography.html#id22" title="Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202, 1980. URL: https://en.wikipedia.org/wiki/Neocognitron.">1980</a>]</span>,
and the LeNet paper by <span id="id8">LeCun <em>et al.</em> [<a class="reference internal" href="bibliography.html#id39" title="Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. URL: https://ieeexplore.ieee.org/abstract/document/726791.">1998</a>]</span>.</p>
<p>The seminal reference for transformer-based architectures is the famous “Attention is all you need” paper by
<span id="id9">Vaswani <em>et al.</em> [<a class="reference internal" href="bibliography.html#id61" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Advances in Neural Information Processing Systems, 5998–6008. 2017. URL: https://en.wikipedia.org/wiki/Attention_Is_All_You_Need.">2017</a>]</span>, and for vision-transformers the equivalent is the “An Image is Worth 16x16 Words” paper by <span id="id10">Dosovitskiy <em>et al.</em> [<a class="reference internal" href="bibliography.html#id16" title="Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations. 2021. URL: https://openreview.net/forum?id=YicbFdNTTy.">2021</a>]</span>. A VIT architecture of note is the “Segment Anything” model by <span id="id11">Kirillov <em>et al.</em> [<a class="reference internal" href="bibliography.html#id34" title="Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment Anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1–10. 2023.">2023</a>]</span>.
Finally, a seminal reference for vision-language-action models is the RT-2 paper from Google <span id="id12">[<a class="reference internal" href="bibliography.html#id10" title="Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. In arXiv preprint arXiv:2307.15818. 2023. URL: https://robotics-transformer2.github.io/.">Brohan <em>et al.</em>, 2023</a>]</span>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="S56_diffdrive_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5.6. </span>Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="S60_driving_intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Autonomous Vehicles</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models">5.7.1. Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning">5.7.2. Reasoning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background-and-history">5.7.3. Background and History</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank Dellaert and Seth Hutchinson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>