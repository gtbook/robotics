

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>5.3. Robot Vision &#8212; Introduction to Robotics and Perception</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-312077-7"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'UA-312077-7');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'S53_diffdrive_sensing';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5.4. Computer Vision 101" href="S54_diffdrive_perception.html" />
    <link rel="prev" title="5.2. Motion Model for the Differential Drive Robot" href="S52_diffdrive_actions.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Introduction to Robotics and Perception - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Introduction to Robotics and Perception - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction to Robotics and Perception
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="S10_introduction.html">1. Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S11_intro_state.html">1.1. Representing State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S12_intro_actions.html">1.2. Robot Actions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S13_intro_sensing.html">1.3. Sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="S14_intro_perception.html">1.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S15_intro_decision.html">1.5. Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S16_intro_learning.html">1.6. Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S20_sorter_intro.html">2. A Trash Sorting Robot</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S21_sorter_state.html">2.1. Modeling the World State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S22_sorter_actions.html">2.2. Actions for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S23_sorter_sensing.html">2.3. Sensors for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S24_sorter_perception.html">2.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S25_sorter_decision_theory.html">2.5. Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="S26_sorter_learning.html">2.6. Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S30_vacuum_intro.html">3. A Robot Vacuum Cleaner</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S31_vacuum_state.html">3.1. Modeling the State of the Vacuum Cleaning Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S32_vacuum_actions.html">3.2. Actions over time</a></li>
<li class="toctree-l2"><a class="reference internal" href="S33_vacuum_sensing.html">3.3. Dynamic Bayes Nets</a></li>
<li class="toctree-l2"><a class="reference internal" href="S34_vacuum_perception.html">3.4. Perception with Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S35_vacuum_decision.html">3.5. Markov Decision Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="S36_vacuum_RL.html">3.6. Reinforcement Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S40_logistics_intro.html">4. Warehouse Robots in 2D</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S41_logistics_state.html">4.1. Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S42_logistics_actions.html">4.2. Moving in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="S43_logistics_sensing.html">4.3. Sensor Models with Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S44_logistics_perception.html">4.4. Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S45_logistics_planning.html">4.5. Planning for Logistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="S46_logistics_learning.html">4.6. Some System Identification</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="S50_diffdrive_intro.html">5. A Mobile Robot With Simple Kinematics</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="S51_diffdrive_state.html">5.1. State Space for a Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S52_diffdrive_actions.html">5.2. Motion Model for the Differential Drive Robot</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">5.3. Robot Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="S54_diffdrive_perception.html">5.4. Computer Vision 101</a></li>
<li class="toctree-l2"><a class="reference internal" href="S55_diffdrive_planning.html">5.5. Path Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S56_diffdrive_learning.html">5.6. Deep Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S60_driving_intro.html">6. Autonomous Vehicles</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S61_driving_state.html">6.1. Planar Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="S62_driving_actions.html">6.2. Kinematics for Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S63_driving_sensing.html">6.3. Sensing for Autonomous Vehicles</a></li>
<li class="toctree-l2"><a class="reference internal" href="S64_driving_perception.html">6.4. SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S65_driving_planning.html">6.5. Planning for Autonomous Driving.</a></li>
<li class="toctree-l2"><a class="reference internal" href="S66_driving_DRL.html">6.6. Deep Reinforcement Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S70_drone_intro.html">7. Autonomous Drones in 3D</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S71_drone_state.html">7.1. Moving in Three Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S72_drone_actions.html">7.2. Multi-rotor Aircraft</a></li>
<li class="toctree-l2"><a class="reference internal" href="S73_drone_sensing.html">7.3. Sensing for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S74_drone_perception.html">7.4. Visual SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S75_drone_planning.html">7.5. Trajectory Optimization</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gtbook/robotics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS53_diffdrive_sensing.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/S53_diffdrive_sensing.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Robot Vision</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cameras">5.3.1. Cameras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cameras-for-robot-vision">5.3.2. Cameras for Robot Vision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-imaging-geometry">5.3.3. Camera Imaging Geometry</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-calibration">5.3.4. Camera Calibration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pinhole-projection-equations">5.3.5. Pinhole Projection Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-calibration-in-gtsam">5.3.6. Camera Calibration in GTSAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-field-of-view">5.3.7. Camera Field of View</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stereo-vision">5.3.8. Stereo Vision</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S53_diffdrive_sensing.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p>In this section, we introduce cameras, which are among the most powerful sensors available to robots. We focus mainly on the geometric aspects of image formation, namely the relationships between 3D objects in the world and their 2D camera images. We will describe how to estimate the relevant parameters of the imaging process (camera calibration), and how two cameras can be used to estimate the 3D structure of the environment using two 2D images.</p>
<section class="tex2jax_ignore mathjax_ignore" id="robot-vision">
<h1><span class="section-number">5.3. </span>Robot Vision<a class="headerlink" href="#robot-vision" title="Permalink to this heading">#</a></h1>
<blockquote>
<div><p>A camera is a super-sensor.</p>
</div></blockquote>
<a class="reference internal image-reference" href="_images/S53-Two-wheeled_Toy_Robot-04.jpg"><img alt="Splash image with steampunk robot sporting a stereo camera" class="align-center" src="_images/S53-Two-wheeled_Toy_Robot-04.jpg" style="width: 40%;" /></a>
<section id="cameras">
<h2><span class="section-number">5.3.1. </span>Cameras<a class="headerlink" href="#cameras" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>The basic ideas behind cameras have been around for centuries.</p>
</div></blockquote>
<p>Everyone knows what a camera is these days, and you probably have between 1 and 5 on your phone, depending on what model you have.</p>
<p>Historically, a <strong>Camera Obscura</strong>, literally “dark room”, showed people that focused <em>upside-down</em> images can be formed on a surface, provided the light rays coming from outside the room were constricted to a small “pinhole”. If you have never experienced this in real-life, it is a worthwhile experience to see this with your own eyes. One of the surprising but obvious properties of a camera obscura is that the images <em>move</em>: it really is <em>video obscura</em>.</p>
<p>The question then is how to capture these fleeting images. Da Vinci wrote extensively about using the camera obscura for drawing, and several 17th century painters may have used it in their painting process, the most famous of them being <a class="reference external" href="https://en.wikipedia.org/wiki/Johannes_Vermeer">Johannes Vermeer</a>.
The invention of <strong>photography</strong> (painting with light!) is usually credited to <a class="reference external" href="https://en.wikipedia.org/wiki/Nic%C3%A9phore_Ni%C3%A9pce">Niépce</a>, who used a light-sensitive material to capture the light around 1825. However, it was his partner <a class="reference external" href="https://en.wikipedia.org/wiki/Louis_Daguerre">Daguerre</a> who introduced photography to the world on a large scale via his <em>Daguerreotype</em> process, released into the public domain in 1839.</p>
<p>Since the 1990s, <strong>digital cameras</strong> have replaced cameras based on chemical emulsions, using CCDs (charged-coupled devices) or CMOS sensors as the underlying technology. Both sensor types capture photons in an array of picture elements or <strong>pixels</strong>. We will not discuss in detail how these devices work, but in essence both sensor types count how many photons fall onto each pixel’s area over a given time period. Below we discuss the more practical matter of the format in which images come to us, and how they can be used for robot vision.</p>
</section>
<section id="cameras-for-robot-vision">
<h2><span class="section-number">5.3.2. </span>Cameras for Robot Vision<a class="headerlink" href="#cameras-for-robot-vision" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>A camera is two sensors in one.</p>
</div></blockquote>
<p>Cameras are amazing devices, and actually pack <em>two</em> sensors in one. First, a camera accurately measures the direction to points in space. Second, the 2D images formed on the sensor
contain photometric information about the scene.
This information can be analyzed by computer vision algorithms to recognize objects and analyze the scene in front of the robot. In this section we focus on the basics of image formation,
however, and leave algorithms for Section 5.4.</p>
<p>A pinhole by itself is rather amazing, as it renders the entire scene in front entirely <em>in focus</em>. However, it has a large drawback, in that it only lets in a tiny amount of light. The solution is to use a <strong>lens</strong>, which <em>collects</em> light over a larger diameter and <em>focuses</em> the light onto the image sensor. The upshot is that we can collect a lot more light (photons) in the same amount of time. The <em>downside</em> is that only part of the scene can be in focus at a given time - a phenomenon that leads to the “depth of field” of a camera: the (possibly narrow) area between where objects are too close or too far to be in focus.</p>
<p>The most important properties associated with a digital camera are its
<strong>resolution</strong>, typically specified as <span class="math notranslate nohighlight">\(W \times H\)</span> in pixels;
its <strong>focal length</strong>, which, as we will see below, can be measured either in meters or pixels;
and its <strong>field of view</strong> (FOV), typically specified in degrees (horizontal, vertical, or diagonal).
The resolution is a property of the <em>sensor</em>, whereas focal length and field of view depend on the lens. We will investigate the relationships between these quantities below, where we talk about the camera
imaging geometry.</p>
<p>In essence, we get access to images as multi-dimensional arrays. Expensive CCD cameras have three sensors, one per color channel (<strong>R</strong>ed, <strong>G</strong>reen, and <strong>B</strong>lue), and hence their raw output can be represented as three arrays of numbers that represent light levels in a specific frequency band, roughly corresponding to the same frequency bands that receptors in our eye are sensitive to. However, most cameras now have a <em>single</em> CMOS sensor with a color filter on top (called a Bayer pattern), and specialized algorithms that hallucinate three color channels. Actually, most cameras do a great deal more processing to improve the color and lighting; this sometimes gets in the way of algorithms that rely on measuring light exactly, but those are rather rare. In most cases, we are content to simply think of a (color) image as a <span class="math notranslate nohighlight">\(H \times W \times 3\)</span> array of numbers, where <span class="math notranslate nohighlight">\(H\)</span> is the height of the image, and <span class="math notranslate nohighlight">\(W\)</span> the width.</p>
<p>As an example, below we show an image on the left, taken by the differential drive robot on the right:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_name</span> <span class="o">=</span> <span class="s2">&quot;LL_color_1201754063.387872.bmp&quot;</span>
<span class="n">ROW</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;&lt;img src=&quot;</span><span class="si">{</span><span class="n">FIG5</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">image_name</span><span class="si">}</span><span class="s1">?raw=1&quot; alt=&quot;Outdoor, beaten down path&quot;&gt;&#39;</span><span class="p">,</span>
     <span class="sa">f</span><span class="s1">&#39;&lt;img src=&quot;</span><span class="si">{</span><span class="n">FIG5</span><span class="si">}</span><span class="s1">/lagr-robot.jpg?raw=1&quot; alt=&quot;LAGR robot&quot; height=&quot;359&quot;&gt;&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table width="100%" class={cls}>

<tr>
<td style="text-align:left;"><img src="https://raw.githubusercontent.com/gtbook/robotics/main/Figures5/LL_color_1201754063.387872.bmp?raw=1" alt="Outdoor, beaten down path"></td>
<td style="text-align:left;"><img src="https://raw.githubusercontent.com/gtbook/robotics/main/Figures5/lagr-robot.jpg?raw=1" alt="LAGR robot" height="359"></td>
</tr></table>
</div></div>
</div>
<p>A python library, the <em>Python Imaging Library</em> or PIL provides some basic capabilities to deal with digital images. We can load images using the <code class="docutils literal notranslate"><span class="pre">PIL.Image</span></code> class, examine its dimensions, and create a numpy array view (you can also use <code class="docutils literal notranslate"><span class="pre">display</span></code> in a notebook to show it):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">diffdrive</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span> <span class="c1"># locally: PIL.Image.open(image_name)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;resolution = </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">width</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">height</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">image_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;image_data.shape = </span><span class="si">{</span><span class="n">image_data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">image_data</span><span class="p">[</span><span class="mi">383</span><span class="p">,</span><span class="mi">511</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>resolution = 512x384
image_data.shape = (384, 512, 3)
[82 55 57]
</pre></div>
</div>
</div>
</div>
<p>We see that the image width and height are <span class="math notranslate nohighlight">\(512\)</span> and <span class="math notranslate nohighlight">\(384\)</span>, respectively. But when we access the array with numpy, the first (slowest changing) dimension is the <em>height</em>, followed by the width and then the color dimension. Hence, the numpy array has to be indexed using the <span class="math notranslate nohighlight">\((\text{row},\text{column})\)</span> convention, after which you get the RGB value in the array, as shown in the last line of code above.</p>
<p>It is customary to use variables <span class="math notranslate nohighlight">\((i,j)\)</span> or <span class="math notranslate nohighlight">\((r,c)\)</span> to index pixels, where the latter is slightly preferred as it emphasizes the <em>row</em> and <em>column</em> semantics of these <em>integer</em> coordinates.</p>
</section>
<section id="camera-imaging-geometry">
<h2><span class="section-number">5.3.3. </span>Camera Imaging Geometry<a class="headerlink" href="#camera-imaging-geometry" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>Points in the 3D environment project to points in a 2D image.</p>
</div></blockquote>
<p>In order to use a camera to infer the properties of the robot’s 3D environment,
we need to fully under stand the geometry of image formation.
We already did so at a superficial level, but the geometry involved needs more detail: exactly what light falls into what pixel?
The simplest model for geometric image formation is the <strong>pinhole camera model</strong>.
Imagine a three-dimensional, orthogonal coordinate frame centered at center of the lens.
Computer vision folks use a very specific camera convention which will make the math easy:</p>
<ul class="simple">
<li><p>the X-axis points to the <em>right</em>;</p></li>
<li><p>the Y-axis points <em>down</em>; and</p></li>
<li><p>the Z-axis points into the scene.</p></li>
</ul>
<p>When we express 3D points in the scene according to this convention, in a coordinate frame that is attached the the cameras, we speak of specifying an object in <em>camera coordinates</em>. For example, a 2 meter tall person, standing 5 meters away, and 3 meters to the left, would have be in between these two 3D coordinates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feet</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mf">1.7</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># point at the feet of the person, 5 meters in front of camera, 3 meters to the left</span>
<span class="n">head</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># point at the top of the head (note, Y = *minus* 2 meters)</span>
</pre></div>
</div>
</div>
</div>
<p>Note that we specify the location of the person’s feet in <em>camera coordinates</em>, and if we are holding the camera level at a height of 1.7 meters, the feet will be 1.7 meters <em>below</em> the pinhole position.</p>
<p>Thinking back to the camera obscura example, the pinhole camera model specifies that a 3D point <span class="math notranslate nohighlight">\((X,Y,Z)\)</span> in camera coordinates will be projected onto an image plane <em>behind</em> the camera:</p>
<div class="math notranslate nohighlight">
\[
X_I = - F \frac{X}{Z} ~~~~
Y_I = - F \frac{Y}{Z} ~~~~
Z_I = -F
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(F\)</span> denotes the focal length measured in meters,
which is defined as the distance from the image plane to the pinhole, i.e., the center of the lens.
The following figure shows the geometry:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">F</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># meter</span>
<span class="kn">from</span> <span class="nn">gtbook.diffdrive</span> <span class="kn">import</span> <span class="n">axes</span><span class="p">,</span> <span class="n">plane</span><span class="p">,</span> <span class="n">ray</span><span class="p">,</span> <span class="n">show_3d</span>
<span class="n">show_3d</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">plane</span><span class="p">(</span><span class="o">-</span><span class="n">F</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">ray</span><span class="p">(</span><span class="n">feet</span><span class="p">,</span> <span class="o">-</span><span class="n">F</span><span class="p">),</span> <span class="n">ray</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="o">-</span><span class="n">F</span><span class="p">)]</span> <span class="o">+</span> <span class="n">axes</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c365f37e251c1bc70b92b8797789688f0a9a091ec0790c3fa4d82523504015b0.png" src="_images/c365f37e251c1bc70b92b8797789688f0a9a091ec0790c3fa4d82523504015b0.png" />
</div>
</div>
<p>However, it is not easy to debug algorithms with a true <em>upside down</em>  pinhole image. Instead, we can define a <em>virtual image plane</em> at a distance <span class="math notranslate nohighlight">\(F\)</span> <em>in front</em> of the pinhole, which is non-physical, but has the advantage that the image now appears right-side up. We simply have to reflect the projected coordinates:</p>
<div class="math notranslate nohighlight">
\[
X_V = F \frac{X}{Z} ~~~~
Y_V = F \frac{Y}{Z} ~~~~
Z_V = F
\]</div>
<p>The virtual image geometry is shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">show_3d</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">plane</span><span class="p">(</span><span class="n">F</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">ray</span><span class="p">(</span><span class="n">feet</span><span class="p">,</span> <span class="n">F</span><span class="p">),</span> <span class="n">ray</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">F</span><span class="p">)]</span> <span class="o">+</span> <span class="n">axes</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a8323241096fdbf692f03e692a1a38dac55144e9dde1ca96ae7a7e194dc92a63.png" src="_images/a8323241096fdbf692f03e692a1a38dac55144e9dde1ca96ae7a7e194dc92a63.png" />
</div>
</div>
<p>The above has the disadvantage that we still have to take into account the focal length <span class="math notranslate nohighlight">\(F\)</span> when doing the projection. Dividing by the focal length yields the fundamental <em>pinhole projection equation</em>:</p>
<div class="math notranslate nohighlight">
\[
x = \frac{X}{Z} ~~~~ y = \frac{Y}{Z}
\]</div>
<p>The dimensionless <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> coordinates are called the <strong>intrinsic camera coordinates</strong>, and can be thought of as the image of the scene in a virtual image plane situated at a focal length of 1.0.
Note that the image origin at <span class="math notranslate nohighlight">\((x,y)=(0,0)\)</span> is the location where the <em>optical axis</em> (the blue Z-axis above) pierces the image plane.
This point is commonly refered to as the <strong>principal point</strong>.
The intrinsic coordinates are in essence measuring a direction in space, but parameterized by a location in the virtual image plane rather than two angles.</p>
</section>
<section id="camera-calibration">
<h2><span class="section-number">5.3.4. </span>Camera Calibration<a class="headerlink" href="#camera-calibration" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>From intrinsic to sensor coordinates.</p>
</div></blockquote>
<p>Intrinsic coordinates are dimensionless, but what <em>pixels</em> in an image do they correspond to?
Also, when we project real-valued 3D coordinates in an image, we get <em>real-valued</em> intrinsic coordinates <span class="math notranslate nohighlight">\((x,y)\)</span>. How does that relate to integer pixel coordinates?
To translate from intrinsic coordinates to pixel coordinates, we introduce real-valued <strong>sensor coordinates</strong> <span class="math notranslate nohighlight">\((u,v)\)</span>, with the following conventions (try to draw this out for a <span class="math notranslate nohighlight">\(4\times3\)</span> image!):</p>
<ul class="simple">
<li><p>the top-left of the sensor corresponds to <span class="math notranslate nohighlight">\((u, v)=(0.0, 0.0)\)</span>;</p></li>
<li><p>the bottom-right of the sensor corresponds to <span class="math notranslate nohighlight">\((u, v)=(W, H)\)</span>.</p></li>
</ul>
<p>Some things to note:</p>
<ul class="simple">
<li><p>the vertical <span class="math notranslate nohighlight">\(v\)</span>-axis points <em>down</em>;</p></li>
<li><p>the units are in pixels (<em>fractional</em> pixels, if being precise);</p></li>
<li><p>we swapped the convention from <span class="math notranslate nohighlight">\((r,c)\Leftrightarrow(\text{row},\text{column})\)</span> to <span class="math notranslate nohighlight">\((u,v)\Leftrightarrow(\text{horizontal}, \text{vertical})\)</span>;</p></li>
<li><p>the middle of pixel <span class="math notranslate nohighlight">\((r, c)=(0, 0)\)</span> has sensor coordinates <span class="math notranslate nohighlight">\((u, v)=(0.5, 0.5)\)</span>;</p></li>
<li><p>the middle of pixel <span class="math notranslate nohighlight">\((r, c)=(H-1, W-1)\)</span> has sensor coordinates <span class="math notranslate nohighlight">\((u, v)=(W-0.5, H-0.5)\)</span>.</p></li>
</ul>
<p>The simplest <strong>camera calibration model</strong> is just a linear mapping, which is most appropriate for lenses with a small field of view. For this we need four parameters <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>, <span class="math notranslate nohighlight">\(u_0\)</span>, and <span class="math notranslate nohighlight">\(v_0\)</span>, to convert from intrinsic coordinates <span class="math notranslate nohighlight">\((x,y)\)</span> to sensor coordinates <span class="math notranslate nohighlight">\((u,v)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
u &amp;= u_0 + \alpha x \\
v &amp;= v_0 + \beta y
\end{aligned}
\end{split}\]</div>
<p>As an example, consider the <a class="reference external" href="https://www.flir.com/products/firefly-s/?model=FFY-U3-04S2C-C">FireFly S</a> machine vision camera, which has the following specifications:</p>
<ul class="simple">
<li><p>sensor: <a class="reference external" href="https://www.phase1vision.com/userfiles/product_files/imx273_287_296_297_flyer.pdf">Sony IMX297</a> (CMOS)</p></li>
<li><p>resolution: 728 x 544</p></li>
<li><p>pixel size: 6.9 <span class="math notranslate nohighlight">\(\mu m\)</span> (H) x 6.9 <span class="math notranslate nohighlight">\(\mu m\)</span> (V)</p></li>
<li><p>sensor size: 6.3mm diagonally (sanity-check this!)</p></li>
</ul>
<p>We typically expect the <em>image center</em>, corresponding to <span class="math notranslate nohighlight">\((x,y)=(0.0,0.0)\)</span>, to be close to <span class="math notranslate nohighlight">\((u_0,v_0)=(W/2,H/2)\)</span>.
For the sensor above this would be <span class="math notranslate nohighlight">\((u_0,v_0)=(364.0, 272.0)\)</span>.
To compute <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> we have to take into account the lens focal length <span class="math notranslate nohighlight">\(F\)</span>. Since <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are expressed in pixels, and <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are dimensionless, it is clear that <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> must also be expressed in pixels. They can be computed as</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha = F k = 8mm/6.9\mu m \approx 1160px\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta = F l = 8mm/6.9\mu m \approx 1160px\)</span></p></li>
</ul>
<p>where</p>
<div class="math notranslate nohighlight">
\[k = 1px/6.9\mu~~~~~\mathrm{and}~~~~l = 1px/6.9\mu\]</div>
<p>are sensor-specific constants that indicated the number of pixels per unit of length.</p>
<p>Whenever <span class="math notranslate nohighlight">\(k=l\)</span>, the sensor has <em>square pixels</em>, and we can just use one proportionality constant, <span class="math notranslate nohighlight">\(f=\alpha=\beta\)</span>.
In this case, <span class="math notranslate nohighlight">\(f\)</span> again denotes the <em>focal length</em>, but this time, expressed in pixels. This is a slight abuse of terminology, as <span class="math notranslate nohighlight">\(f\)</span> is a property of both the lens <em>and</em> the image sensor plane, but it is in widespread and we will adopt it here as well.</p>
</section>
<section id="pinhole-projection-equations">
<h2><span class="section-number">5.3.5. </span>Pinhole Projection Equations<a class="headerlink" href="#pinhole-projection-equations" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>From 3D to pixel coordinates.</p>
</div></blockquote>
<p>Putting all of the above together,
we finally have the <strong>fundamental pinhole projection equations</strong>, projecting a point <span class="math notranslate nohighlight">\(P\)</span> in 3D camera coordinates <span class="math notranslate nohighlight">\(P=(X,Y,Z)\)</span>, to its 2D image projection <span class="math notranslate nohighlight">\(p=(u,v)\)</span> in sensor coordinates:</p>
<div class="math notranslate nohighlight">
\[
u = u_0 + f \frac{X}{Z} ~~~~ v = v_0 + f \frac{Y}{Z}.
\]</div>
<p>To obtain integer pixel coordinates <span class="math notranslate nohighlight">\((r,c)\)</span>, we simply need to use the <em>floor</em> function, truncating the fractional pixel sensor coordinates to a location in the image array. Note that in doing so we also flip horizontal and vertical:</p>
<div class="math notranslate nohighlight">
\[
(r,c) = (\lfloor v \rfloor, \lfloor u \rfloor)
\]</div>
<p>We can also go the other way, <em>calibrating</em> the sensor coordinates <span class="math notranslate nohighlight">\((u,v)\)</span> to the dimensionless intrinsic coordinates <span class="math notranslate nohighlight">\((x,y)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
x &amp;= (u-u_0)/f \\
y &amp;= (v-v_0)/f
\end{aligned}
\end{split}\]</div>
</section>
<section id="camera-calibration-in-gtsam">
<h2><span class="section-number">5.3.6. </span>Camera Calibration in GTSAM<a class="headerlink" href="#camera-calibration-in-gtsam" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>Everything above and more.</p>
</div></blockquote>
<p>In GTSAM you have access to several calibration models, with the simple one above corresponding to <code class="docutils literal notranslate"><span class="pre">gtsam.Cal3_S2</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cal_8mm_FireFlyS</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Cal3_S2</span><span class="p">(</span><span class="n">fx</span><span class="o">=</span><span class="mi">1160</span><span class="p">,</span> <span class="n">fy</span><span class="o">=</span><span class="mi">1160</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">u0</span><span class="o">=</span><span class="mi">364</span><span class="p">,</span> <span class="n">v0</span><span class="o">=</span><span class="mi">272</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The arguments <code class="docutils literal notranslate"><span class="pre">fx</span></code> and <code class="docutils literal notranslate"><span class="pre">fy</span></code> above correspond to <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, and for now you can ignore the extra <code class="docutils literal notranslate"><span class="pre">s</span></code> argument, denoting <em>skew</em> which is almost always zero for modern sensors.
We can then convert from integer pixel coordinates to intrinsic coordinates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calibration_demo</span><span class="p">(</span><span class="n">cal</span><span class="p">:</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Cal3_S2</span><span class="p">,</span> <span class="n">row</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">col</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert from integer pixel coordinates to sensor and then intrinsic coordinates.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">row</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">col</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">+</span><span class="n">col</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">+</span><span class="n">row</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">cal</span><span class="o">.</span><span class="n">calibrate</span><span class="p">([</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;image[</span><span class="si">{</span><span class="n">row</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s2">] -&gt; (u,v)=(</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px,</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px) -&gt; (x,y)=(</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">) &quot;</span><span class="p">)</span>

<span class="n">calibration_demo</span><span class="p">(</span><span class="n">cal_8mm_FireFlyS</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">calibration_demo</span><span class="p">(</span><span class="n">cal_8mm_FireFlyS</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">272</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">364</span><span class="p">)</span>
<span class="n">calibration_demo</span><span class="p">(</span><span class="n">cal_8mm_FireFlyS</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">543</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">727</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>image[0,0] -&gt; (u,v)=(0.5px,0.5px) -&gt; (x,y)=(-0.313,-0.234) 
image[272,364] -&gt; (u,v)=(364.5px,272.5px) -&gt; (x,y)=(0.0,0.0) 
image[543,727] -&gt; (u,v)=(727.5px,543.5px) -&gt; (x,y)=(0.313,0.234) 
</pre></div>
</div>
</div>
</div>
<p>Note that although the intrinsic coordinates are the dimensionless, you can interpret them as fractions of the focal length.
Also, the above was a “calibration” example where we go from pixel coordinates to intrinsic coordinates. The calibration objects in GTSAM also provide an <code class="docutils literal notranslate"><span class="pre">uncalibrate</span></code> method which goes the other way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">u</span><span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">cal_8mm_FireFlyS</span><span class="o">.</span><span class="n">uncalibrate</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(x,y)=(0,0) -&gt; (u,v)=(</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px,</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(x,y)=(0,0) -&gt; (u,v)=(364.0px,272.0px)
</pre></div>
</div>
</div>
</div>
</section>
<section id="camera-field-of-view">
<h2><span class="section-number">5.3.7. </span>Camera Field of View<a class="headerlink" href="#camera-field-of-view" title="Permalink to this heading">#</a></h2>
<p>The last concept we need to define the camera imaging geometry
is the camera’s <strong>field of view</strong> or <strong>FOV</strong>.
Because the <em>left-most</em> ray we can see has <span class="math notranslate nohighlight">\(u=0\)</span>, it corresponds to <span class="math notranslate nohighlight">\(x=-u_0/f\approx-W/2f\)</span>.
The horizontal FOV can then be calculated by</p>
<div class="math notranslate nohighlight">
\[\mathrm{HFOV} = 2 \arctan(W/2f)~~\mathrm{rad} = 360 \arctan(W/2f) / \pi~~\mathrm{degrees}\]</div>
<p>For the sensor-lens combination above we get a relatively narrow field of view of about 35 degrees:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="mi">1160</span>
<span class="n">hfov</span> <span class="o">=</span> <span class="mi">360</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="mi">728</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">f</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;HFOV for f=</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">hfov</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> degrees&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HFOV for f=1160 is 34.84 degrees
</pre></div>
</div>
</div>
</div>
<p>Field of view <em>increases</em> with decreasing focal length, e.g., a lens of 4mm will give us a bit less than double that HFOV, of around 64 degrees:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f_wide</span> <span class="o">=</span> <span class="mf">4e-3</span><span class="o">/</span><span class="mf">6.9e-6</span>
<span class="n">hfov_wide</span> <span class="o">=</span> <span class="mi">360</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="mi">728</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">f_wide</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;HFOV for f=</span><span class="si">{</span><span class="n">f_wide</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">hfov_wide</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> degrees&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HFOV for f=579.7 is 64.25 degrees
</pre></div>
</div>
</div>
</div>
<p>We can also ask the opposite question: what lens focal length should we choose to get a certain filed of view. For example, for a <em>diagonal</em> field of view we have</p>
<div class="math notranslate nohighlight">
\[\mathrm{DFOV} = 360 \arctan(\sqrt{W^2+H^2}/2f) / \pi~~\mathrm{degrees}\]</div>
<p>and hence</p>
<div class="math notranslate nohighlight">
\[f = \frac{\sqrt{W^2+H^2}}{2 \tan(\mathrm{DFOV} \pi/360)}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f45</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">728</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">544</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">)</span>
<span class="n">F45</span> <span class="o">=</span> <span class="n">f45</span><span class="o">*</span><span class="mf">6.9e-3</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;f45 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">f45</span><span class="p">)</span><span class="si">}</span><span class="s2"> pixels, F45 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">F45</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2"> mm&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>f45 = 454.0 pixels, F45 = 3.1 mm
</pre></div>
</div>
</div>
</div>
</section>
<section id="stereo-vision">
<h2><span class="section-number">5.3.8. </span>Stereo Vision<a class="headerlink" href="#stereo-vision" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>Given two cameras, we can calculate depth.</p>
</div></blockquote>
<p>When using two cameras, we can triangulate a feature that is seen in both cameras to calculate its location in space.
Given a projection <span class="math notranslate nohighlight">\(p=(u,v)\)</span> of a point <span class="math notranslate nohighlight">\(P=(X,Y,Z)\)</span> in a single camera we can only determine the <em>ray</em> on which the point <span class="math notranslate nohighlight">\(P\)</span> must lie.
However, if we see <em>two</em> projections of the same feature in two cameras, placed side by side, we can <em>triangulate</em> the location of <span class="math notranslate nohighlight">\(P\)</span>.
In particular, let us name the cameras “Left” and “Right”, abbreviated as “L” and “R”, and let the two projections be <span class="math notranslate nohighlight">\(p_L=(u_L,v_L)\)</span> and <span class="math notranslate nohighlight">\(p_R=(u_R,v_R)\)</span>. How could we recover the coordinates <span class="math notranslate nohighlight">\((X,Y,Z)\)</span> in, say, the <em>left</em> camera coordinate frame?</p>
<p>We can easily work out the answer <em>if</em> the cameras have the same calibration <em>and</em> the camera pair is in a “stereo” configuration. The latter means that the cameras have exactly the same orientation with respect to the world, and the right camera is displaced only horizontally with respect to the left camera. We call the displacement the <strong>stereo baseline</strong> <span class="math notranslate nohighlight">\(B\)</span>. In that case we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
u_L &amp;= u_0 + f \frac{X}{Z}, ~~~~~ &amp;v_L = v_0 + f \frac{Y}{Z} \\
\\
u_R &amp;= u_0 + f \frac{X-B}{Z}, ~~~~~ &amp;v_R = v_0 + f \frac{Y}{Z}
\end{aligned}
\end{split}\]</div>
<p>Two interesting things to note: (a) <span class="math notranslate nohighlight">\(u_L\)</span> and <span class="math notranslate nohighlight">\(u_R\)</span> differ only because the <span class="math notranslate nohighlight">\(X\)</span> coordinate of the point <span class="math notranslate nohighlight">\(P\)</span>, measured in the right camera, is <span class="math notranslate nohighlight">\(B\)</span> less than its value in the left camera.
and (b) <span class="math notranslate nohighlight">\(v_L\)</span> and <span class="math notranslate nohighlight">\(v_R\)</span> have the same value: <em>corresponding</em> points in a stereo pair lie on the same scanline in the images. We can use the first fact to calculate the <em>unknown depth</em> <span class="math notranslate nohighlight">\(Z\)</span>, by defining the <strong>disparity</strong> <span class="math notranslate nohighlight">\(d\)</span> as the difference of <span class="math notranslate nohighlight">\(u_L\)</span> and <span class="math notranslate nohighlight">\(u_R\)</span>,</p>
<div class="math notranslate nohighlight">
\[
d \doteq u_L - u_R = f \frac{X}{Z} - f \frac{X-B}{Z},
\]</div>
<p>and then performing some algebraic manipulation to obtain the <strong>fundamental stereo equation</strong>:</p>
<div class="math notranslate nohighlight">
\[
Z = B \frac{f}{d}.
\]</div>
<p>The fraction <span class="math notranslate nohighlight">\(f/d\)</span> is dimensionless, as both disparity <span class="math notranslate nohighlight">\(d\)</span> and focal length <span class="math notranslate nohighlight">\(f\)</span> are expressed in pixels, and hence the resulting depth <span class="math notranslate nohighlight">\(Z\)</span> is expressed in the units of the baseline <span class="math notranslate nohighlight">\(B\)</span>.
Using this together with the projection equations above,
we can now completely reconstruct the location of the point <span class="math notranslate nohighlight">\(P\)</span> in (left) camera coordinates:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}X\\Y\\Z\end{bmatrix}
= \begin{bmatrix}Z(u_L-u_0)/f\\Z(v_L-v_0)/f\\B f/d\end{bmatrix}
= B \frac{f}{d} \begin{bmatrix}(u_L-u_0)/f\\(v_L-v_0)/f\\1\end{bmatrix}
\end{split}\]</div>
<p>Stereo cameras are used very often on robotics platforms because of this ability to reconstruct the world in 3D, at least in principle. This is akin to our own (human) ability to perceive depth by virtue of having two eyes, a feature we have in common with many animals - primarily predators, who need accurate depth vision to hunt prey. In practice, using a stereo camera is not as easy, as it has to be carefully calibrated and finding <em>correspondences</em> between left and right cameras is not always straightforward. However, the latter has been alleviated quite a bit by recent advances in neural networks, which we will discuss below.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="S52_diffdrive_actions.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5.2. </span>Motion Model for the Differential Drive Robot</p>
      </div>
    </a>
    <a class="right-next"
       href="S54_diffdrive_perception.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5.4. </span>Computer Vision 101</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cameras">5.3.1. Cameras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cameras-for-robot-vision">5.3.2. Cameras for Robot Vision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-imaging-geometry">5.3.3. Camera Imaging Geometry</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-calibration">5.3.4. Camera Calibration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pinhole-projection-equations">5.3.5. Pinhole Projection Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-calibration-in-gtsam">5.3.6. Camera Calibration in GTSAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#camera-field-of-view">5.3.7. Camera Field of View</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stereo-vision">5.3.8. Stereo Vision</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank Dellaert and Seth Hutchinson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>