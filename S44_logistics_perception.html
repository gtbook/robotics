
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4.4. Localization &#8212; Introduction to Robotics and Perception</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=51e3b7cf" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-312077-7"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'UA-312077-7');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'UA-312077-7');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'S44_logistics_perception';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4.5. Planning for Logistics" href="S45_logistics_planning.html" />
    <link rel="prev" title="4.3. Sensor Models with Continuous State" href="S43_logistics_sensing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Introduction to Robotics and Perception - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Introduction to Robotics and Perception - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction to Robotics and Perception
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="S10_introduction.html">1. Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S11_models.html">1.1. Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S12_reasoning.html">1.2. Reasoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S13_math.html">1.3. The Mathematics of Robotics</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S20_sorter_intro.html">2. A Trash Sorting Robot</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S21_sorter_state.html">2.1. Modeling the World State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S22_sorter_actions.html">2.2. Actions for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S23_sorter_sensing.html">2.3. Sensors for Sorting Trash</a></li>
<li class="toctree-l2"><a class="reference internal" href="S24_sorter_perception.html">2.4. Perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="S25_sorter_decision_theory.html">2.5. Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="S26_sorter_learning.html">2.6. Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S27_sorter_summary.html">2.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S30_vacuum_intro.html">3. A Robot Vacuum Cleaner</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S31_vacuum_state.html">3.1. Modeling the State of the Vacuum Cleaning Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S32_vacuum_actions.html">3.2. Actions over time</a></li>
<li class="toctree-l2"><a class="reference internal" href="S33_vacuum_sensing.html">3.3. Dynamic Bayes Nets</a></li>
<li class="toctree-l2"><a class="reference internal" href="S34_vacuum_perception.html">3.4. Perception with Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="S35_vacuum_decision.html">3.5. Markov Decision Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="S36_vacuum_RL.html">3.6. Learning to Act Optimally</a></li>
<li class="toctree-l2"><a class="reference internal" href="S37_vacuum_summary.html">3.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="S40_logistics_intro.html">4. Warehouse Robots in 2D</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="S41_logistics_state.html">4.1. Continuous State</a></li>
<li class="toctree-l2"><a class="reference internal" href="S42_logistics_actions.html">4.2. Moving in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="S43_logistics_sensing.html">4.3. Sensor Models with Continuous State</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">4.4. Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S45_logistics_planning.html">4.5. Planning for Logistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="S46_logistics_learning.html">4.6. Some System Identification</a></li>
<li class="toctree-l2"><a class="reference internal" href="S47_logistics_summary.html">4.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S50_diffdrive_intro.html">5. A Mobile Robot With Simple Kinematics</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S51_diffdrive_state.html">5.1. State Space for a Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S52_diffdrive_actions.html">5.2. Motion Model for the Differential Drive Robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="S53_diffdrive_sensing.html">5.3. Robot Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="S54_diffdrive_perception.html">5.4. Computer Vision 101</a></li>
<li class="toctree-l2"><a class="reference internal" href="S55_diffdrive_planning.html">5.5. Path Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S56_diffdrive_learning.html">5.6. Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S57_diffdrive_summary.html">5.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S60_driving_intro.html">6. Autonomous Vehicles</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S61_driving_state.html">6.1. Planar Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="S62_driving_actions.html">6.2. Kinematics for Driving</a></li>
<li class="toctree-l2"><a class="reference internal" href="S63_driving_sensing.html">6.3. Sensing for Autonomous Vehicles</a></li>
<li class="toctree-l2"><a class="reference internal" href="S64_driving_perception.html">6.4. SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S65_driving_planning.html">6.5. Planning for Autonomous Driving.</a></li>
<li class="toctree-l2"><a class="reference internal" href="S66_driving_DRL.html">6.6. Deep Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="S67_driving_summary.html">6.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="S70_drone_intro.html">7. Autonomous Drones in 3D</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="S71_drone_state.html">7.1. Moving in Three Dimensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="S72_drone_actions.html">7.2. Multi-rotor Aircraft</a></li>
<li class="toctree-l2"><a class="reference internal" href="S73_drone_sensing.html">7.3. Sensing for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S74_drone_perception.html">7.4. Visual SLAM</a></li>
<li class="toctree-l2"><a class="reference internal" href="S75_drone_planning.html">7.5. Trajectory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="S76_drone_learning.html">7.6. Neural Radiance Fields for Drones</a></li>
<li class="toctree-l2"><a class="reference internal" href="S77_drone_summary.html">7.7. Chapter Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">8. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gtbook/robotics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS44_logistics_perception.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/S44_logistics_perception.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Localization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-running-example">4.4.1. A Running Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayes-filter">4.4.2. The Bayes Filter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-localization">4.4.3. Markov Localization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-1d-example">4.4.3.1. A 1D Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warehouse-example">4.4.3.2. Warehouse Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">4.4.3.3. Exercises</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-localization">4.4.4. Monte Carlo Localization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prediction-step">4.4.4.1. The Prediction Step</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-likelihood-in-the-update-step">4.4.4.2. Evaluating the Likelihood in the Update Step</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mcl-warehouse-example">4.4.4.3. MCL Warehouse Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-range-sensing">4.4.4.4. Adding Range Sensing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kalman-smoothing-and-filtering">4.4.5. Kalman Smoothing and Filtering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-graphs-and-least-squares">4.4.5.1. Factor Graphs and Least Squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-example">4.4.5.2. Numerical Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">4.4.5.3. Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-least-squares">4.4.5.4. Sparse Least-Squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-gps-like-measurements">4.4.5.5. Example with GPS-Like Measurements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gtsam-102">4.4.6. GTSAM 102</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S44_logistics_perception.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="localization">
<h1><span class="section-number">4.4. </span>Localization<a class="headerlink" href="#localization" title="Link to this heading">#</a></h1>
<blockquote>
<div><p>We introduce three variations of Bayes filtering to solve the robot localization problem: Markov localization, and Monte Carlo localization, and Kalman filtering.</p>
</div></blockquote>
<a class="reference internal image-reference" href="_images/S44-Warehouse_robots-08.jpg"><img alt="Splash image with yellowish warehouse robot pondering" class="align-center" src="_images/S44-Warehouse_robots-08.jpg" style="width: 40%;" /></a>
<p><strong>Localization</strong> is the process of estimating the robot’s position using sensor data
and the history of executed actions.
In this chapter, the robot’s position at time <span class="math notranslate nohighlight">\(k\)</span> is denoted by <span class="math notranslate nohighlight">\(x_k\)</span>, so the localization problem
is to estimate <span class="math notranslate nohighlight">\(x_k\)</span> at each time instant using the robot’s history of sensor observations,
<span class="math notranslate nohighlight">\(z_1, \dots z_k\)</span>, and commanded actions <span class="math notranslate nohighlight">\(u_1, \dots , u_{k-1}\)</span>.</p>
<p>As we have seen in Chapter 3, when we are able to exploit the Markov property, we need
not consider the entire sensor and action history at each time <span class="math notranslate nohighlight">\(k\)</span>; intstead, we can iteratively
compute a probability distribution for the state <span class="math notranslate nohighlight">\(x_k\)</span> using only our belief about the state <span class="math notranslate nohighlight">\(x_{k-1}\)</span>,
the action <span class="math notranslate nohighlight">\(u_{k-1}\)</span>, and the sensor observation <span class="math notranslate nohighlight">\(z_k\)</span>.
This kind of iterative, online updating is referred to as filtering.
The input to the filter is the belief at time <span class="math notranslate nohighlight">\(k-1\)</span>, the action <span class="math notranslate nohighlight">\(u_{k-1}\)</span>, and the observation <span class="math notranslate nohighlight">\(z_k\)</span>.
The output of the filter is the new belief, a probability distribution for <span class="math notranslate nohighlight">\(x_k\)</span>.
Note that when taking this approach we abandon computing the full joint distribution
<span class="math notranslate nohighlight">\(P(X_1, \dots X_k)\)</span>, and content ourselves to compute only the conditional distribution
for <span class="math notranslate nohighlight">\(X_k\)</span> at time <span class="math notranslate nohighlight">\(k\)</span>.
In this chapter, the filters that we develop will be derived from Bayes theorem,
and the result is known as <strong>Bayes filtering</strong>.</p>
<p>We begin the section with a general introduction to Bayes filters, and then develop
three specific algorithms, Markov localization, and Monte Carlo localization, and Kalman filtering.
These three algorithms reflect trade-offs in computational complexity versus accuracy and expressive
power. In particular, Markov localization relies on a grid-based representation that
can require significant computer memory as well as significant computation in the update
process.
Monte Carlo localization addresses each of these concerns by using a sampling-based approach,
at the expense of accuracy.
Kalman filters, on the other hand, provide an exact and optimal solution to the localization
problem, by only in the special case when the robot can be described as a linear system
and all uncertainties in motion and observation are Gaussian in nature.</p>
<section id="a-running-example">
<h2><span class="section-number">4.4.1. </span>A Running Example<a class="headerlink" href="#a-running-example" title="Link to this heading">#</a></h2>
<p>When introducing concepts related to localization, it will be useful to
have a short, ground truth trajectory for the robot that we can use to assess the quality of
solutions.
For this purpose,
consider a trajectory that starts at the bottom left of the map,
moves to the right, then up between the middle two shelves:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">left</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">10</span><span class="o">+</span><span class="n">i</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]</span>
<span class="n">up</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">50</span><span class="p">,</span><span class="mi">6</span><span class="o">+</span><span class="n">i</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">)]</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">left</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">up</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">gtsam</span><span class="o">.</span><span class="n">symbol</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">}</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">VectorValues</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">left</span><span class="o">+</span><span class="n">up</span><span class="p">):</span> <span class="n">values</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">state</span><span class="p">)</span>
<span class="n">values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<table class='VectorValues'>
  <thead>
    <tr><th>Variable</th><th>value</th></tr>
  </thead>
  <tbody>
    <tr><th>x1</th><td>10  6</td></tr>
    <tr><th>x2</th><td>15  6</td></tr>
    <tr><th>x3</th><td>20  6</td></tr>
    <tr><th>x4</th><td>25  6</td></tr>
    <tr><th>x5</th><td>30  6</td></tr>
    <tr><th>x6</th><td>35  6</td></tr>
    <tr><th>x7</th><td>40  6</td></tr>
    <tr><th>x8</th><td>45  6</td></tr>
    <tr><th>x9</th><td>50  6</td></tr>
    <tr><th>x10</th><td>50 11</td></tr>
    <tr><th>x11</th><td>50 16</td></tr>
    <tr><th>x12</th><td>50 21</td></tr>
    <tr><th>x13</th><td>50 26</td></tr>
    <tr><th>x14</th><td>50 31</td></tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Note above we used a <code class="docutils literal notranslate"><span class="pre">gtsam.VectorValues</span></code> to store the ground truth trajectory, which will come in handy again when we simulate the measurements. Below we show this “ground truth” trajectory overlaid on the warehouse map we introduced before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Ground truth trajectory displayed on the base map.</span>
<span class="c1">#| label: fig:logistics-ground-truth</span>
<span class="n">ground_truth</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/864b78ebf9223fa21c73ef5c41244b3e0d7b41999172546bb8642789b8dabf0b.png" src="_images/864b78ebf9223fa21c73ef5c41244b3e0d7b41999172546bb8642789b8dabf0b.png" />
</div>
</div>
</section>
<section id="the-bayes-filter">
<h2><span class="section-number">4.4.2. </span>The Bayes Filter<a class="headerlink" href="#the-bayes-filter" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>The Bayes filter is an iterative, one-step sum-product algorithm.</p>
</div></blockquote>
<p>The Bayes filter estimates the state of the robot at the <em>current time</em>
(denoted by index <span class="math notranslate nohighlight">\(k\)</span> in our discrete-time representation),
given all the measurements and action history up to and including the current time.
To simplify notation, we define
<span class="math notranslate nohighlight">\(\mathcal{Z}^{k}=\{z_{1},z_{2},\ldots z_{k}\}\)</span>, i.e., all measurements
up to and including stage <span class="math notranslate nohighlight">\(k\)</span>.
Similarly, we define the controls
<span class="math notranslate nohighlight">\(\mathcal{U}^{k}=\{u_{1},u_{2},\ldots u_{k-1}\}\)</span>.
Note there is always one less control variable than there are measurements.
The Bayes filter computes the conditional probability distribution
<span class="math notranslate nohighlight">\(P(X_k | \mathcal{Z}^{k},\mathcal{U}^{k})\)</span>, which is also called the <strong>filtering distribution</strong>.</p>
<p>Assuming that we are given a prior distribution for the initial state
of the robot, <span class="math notranslate nohighlight">\(P(X_1)\)</span>,
we can initialize the Bayes filter for time <span class="math notranslate nohighlight">\(k=1\)</span> by using
the sensor observation <span class="math notranslate nohighlight">\(z_1\)</span>. Note that no action has yet been applied.
Thus, we have</p>
<div class="math notranslate nohighlight">
\[
P(X_1|\mathcal{Z}^1=\{z_1\}, \mathcal{U}^1=\{\}) \propto L(X_1;z_1)P(X_1)
\]</div>
<p>Given this, recursively assuming that we have a probability distribution
<span class="math notranslate nohighlight">\(P(X_{k-1}|\mathcal{Z}^{k-1},\mathcal{U}^{k-1})\)</span> over the previous state <span class="math notranslate nohighlight">\(X_{k-1}\)</span>, we proceed in two
phases:</p>
<ol class="arabic">
<li><p>In the <strong>prediction phase</strong> we calculate a <strong>predictive distribution</strong>
<span class="math notranslate nohighlight">\(P(X_{k}|\mathcal{Z}^{k-1},\mathcal{U}^{k})\)</span> on the current state <span class="math notranslate nohighlight">\(X_{k}\)</span> given a control <span class="math notranslate nohighlight">\(u_{k-1}\)</span>. This is done by marginalizing out
the previous state <span class="math notranslate nohighlight">\(X_{k-1}\)</span>, by summing over all possible values
<span class="math notranslate nohighlight">\(x_{k-1}\)</span> for <span class="math notranslate nohighlight">\(X_{k-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    P(X_{k}|\mathcal{Z}^{k-1},\mathcal{U}^{k})=\sum_{x_{k-1}}P(X_{k}|x_{k-1},u_{k-1})P(x_{k-1}|\mathcal{Z}^{k-1},\mathcal{U}^{k-1}).
    \]</div>
</li>
<li><p>In the <strong>measurement phase</strong> we upgrade this predictive density to the <strong>filtering distribution</strong> <span class="math notranslate nohighlight">\(P(X_{k}|\mathcal{Z}^{k},\mathcal{U}^{k})\)</span>
via Bayes’ rule, given the measurement <span class="math notranslate nohighlight">\(z_{k}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    P(X_{k}| \mathcal{Z}^{k},\mathcal{U}^{k})
    \propto L(X_{k};z_{k})P(X_{k}|\mathcal{Z}^{k-1},\mathcal{U}^{k}).
    \]</div>
</li>
</ol>
<p>A keen observer will see that step 1 and step 2 above implement one step of the sum-product algorithm from Section 3.4, where in this case the previous state <span class="math notranslate nohighlight">\(X_{k-1}\)</span> is eliminated. Step 1 is a sum, and step 2 is a product.</p>
</section>
<section id="markov-localization">
<h2><span class="section-number">4.4.3. </span>Markov Localization<a class="headerlink" href="#markov-localization" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Markov localization computes the filtering distribution at each time <span class="math notranslate nohighlight">\(k\)</span> using a discrete grid.</p>
</div></blockquote>
<p>In principle, for a robot that translates in the plane, we take <span class="math notranslate nohighlight">\(x_k \in \mathbb{R}^2\)</span>,
and the belief associated to the state would be represented by a probability <em>density</em> function.
With <strong>Markov localization</strong> we approximate this continuous representation by using
a discrete grid to represent the workspace and assigning finite probability to each cell
in the grid.
In particular,
we assume a finite element discretization such that a density <span class="math notranslate nohighlight">\(p(x_k)\)</span> over continuous states <span class="math notranslate nohighlight">\(x_k\)</span> is approximated by a discrete probability distribution <span class="math notranslate nohighlight">\(P(X_k)\)</span>.
The only difference from the previous chapter, which also discussed discrete state spaces, is that in a finite element discretization the cardinality of the discrete states is typically much larger. Recall from section 4.1 that for our <span class="math notranslate nohighlight">\(100m \times 50m\)</span> warehouse example, even at a relatively course resolution of 1m by 1m cells, our state space had a cardinality of 5000 finite elements.
This makes reasoning over multiple time steps computationally challenging.</p>
<p>By using this finite element discretization we can apply the Bayes filter, as is, on the discrete grid.
When applied to robot localization, because we are using a discrete Markov chain representation, this approach has been called <strong>Markov Localization</strong>.</p>
<section id="a-1d-example">
<h3><span class="section-number">4.4.3.1. </span>A 1D Example<a class="headerlink" href="#a-1d-example" title="Link to this heading">#</a></h3>
<p>Figure
<a href="#fig:Markov-localization" data-reference-type="ref" data-reference="fig:Markov-localization">1</a> below
illustrates the measurement phase for a simple 1D example.</p>
<figure id="fig:Markov-localization">
<img src="https://github.com/gtbook/robotics/blob/main/Figures4/markov-localization.png?raw=1" style="width:14cm" alt="">
<figcaption>One-dimensional example of the measurement phase in Markov localization, the discrete version of the Bayes filter.</figcaption>
</figure>
<p>In this environment there are two doors, and the robot has a door sensor. The
predictive distribution <span class="math notranslate nohighlight">\(P(X)\)</span> encodes the belief that the robot is
near the left door. The likelihood <span class="math notranslate nohighlight">\(L(X;z)\)</span>, where measurement (or observation)
<span class="math notranslate nohighlight">\(z\)</span> indicates that the robot <em>did</em> perceive a door,
models the fact that the robot is more likely to be
near a door given <span class="math notranslate nohighlight">\(Z=z\)</span>, but also allows for the fact that the door
sensor could misfire at any location. Note that the likelihood is
un-normalized and there is no need for it to sum to 1. Finally, the
posterior <span class="math notranslate nohighlight">\(P(X|z)\)</span> is obtained, via Bayes’ rule, as the product of the
prediction <span class="math notranslate nohighlight">\(P(X)\)</span> and the likelihood <span class="math notranslate nohighlight">\(L(X;z)\)</span>, and is shown at the
bottom as a normalized probability distribution. As you can see, the
most probable explanation for the robot state is <span class="math notranslate nohighlight">\(X=5\)</span>, but there is a
second mode at <span class="math notranslate nohighlight">\(X=17\)</span> due to the bimodal nature of the likelihood.
However, that second mode is less probable because of the prior belief
over <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
<section id="warehouse-example">
<h3><span class="section-number">4.4.3.2. </span>Warehouse Example<a class="headerlink" href="#warehouse-example" title="Link to this heading">#</a></h3>
<p>Let us apply Markov localization to the warehouse example, using <em>just</em> the proximity sensor for now. We start by initializing the finite element density representation with a Gaussian prior, centered around the ground truth location for <span class="math notranslate nohighlight">\(k=1\)</span>, but with a relatively large standard deviation of 5 meters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior_mean</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">prior_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">])</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">logistics</span><span class="o">.</span><span class="n">gaussian</span><span class="p">(</span><span class="n">logistics</span><span class="o">.</span><span class="n">map_coords</span><span class="p">,</span> <span class="n">prior_mean</span><span class="p">,</span> <span class="n">prior_cov</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Prior distribution over the robot&#39;s initial position.</span>
<span class="c1">#| label: fig:logistics-prior</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="n">prior</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span><span class="o">+</span><span class="mf">0.1</span><span class="o">*</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/306e9a4b3063e502fa93843dbcb68ec9beebec555d30be87b77043f5783026ab.png" src="_images/306e9a4b3063e502fa93843dbcb68ec9beebec555d30be87b77043f5783026ab.png" />
</div>
</div>
<p>With respect to implementation, the hardest and most computationally demanding part of the Markov localization algorithm is the prediction step.
Recall the formula:</p>
<div class="math notranslate nohighlight">
\[
P(X_{k}|\mathcal{Z}^{k-1},\mathcal{U}^{k})=\sum_{x_{k-1}}P(X_{k}|x_{k-1},u_{k-1})P(x_{k-1}|\mathcal{Z}^{k-1},\mathcal{U}^{k-1}).
\]</div>
<p>Hence, for <em>every cell</em> in the predictive distribution grid, we need to sum over <em>all</em> cells in the previous image. Not only that, but for every one of these <span class="math notranslate nohighlight">\(5000^2\)</span>, i.e., <em>25 million</em> cell combinations <span class="math notranslate nohighlight">\((X_k,x_{k-1})\)</span> we need to evaluate the Gaussian motion model <span class="math notranslate nohighlight">\(P(X_{k}|x_{k-1},u_{k-1})\)</span>. With python for-loops, this will be rather expensive, so in the code below we build in two speed-ups:</p>
<ol class="arabic simple">
<li><p>We make the outer loop over the previous image, and threshold on the probability value
(i.e., if the probability value is less than a threshold, we set the value to zero in the output distribution);</p></li>
<li><p>We make use of the fact that the <code class="docutils literal notranslate"><span class="pre">logistics.gaussian</span></code> function is vectorized, i.e., we can process an entire row of the predictive image at a time.</p></li>
</ol>
<p>The fast code looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prediction_step</span><span class="p">(</span><span class="n">previous</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">motion_model_sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate predictive density given control and control stddev.&quot;&quot;&quot;</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">motion_model_sigma</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">predictive_density</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
            <span class="c1"># Speedup 1: threshold on previous[i,j]</span>
            <span class="k">if</span> <span class="n">previous</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">&gt;</span><span class="mf">1e-5</span><span class="p">:</span>
                <span class="n">previous_xy</span> <span class="o">=</span> <span class="n">logistics</span><span class="o">.</span><span class="n">map_coords</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
                <span class="n">mean</span> <span class="o">=</span> <span class="n">previous_xy</span> <span class="o">+</span> <span class="n">control</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
                    <span class="c1"># Speedup 1: vectorize Gaussian evaluation over predictive row:</span>
                    <span class="n">motion_model</span> <span class="o">=</span> <span class="n">logistics</span><span class="o">.</span><span class="n">gaussian</span><span class="p">(</span><span class="n">logistics</span><span class="o">.</span><span class="n">map_coords</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
                    <span class="n">predictive_density</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">motion_model</span> <span class="o">*</span> <span class="n">previous</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">predictive_density</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">predictive_density</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we have <em>no</em> measurements at all, but have a perfect measurement model, our “control tape” just pushes the Gaussian density along:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Predictive density after taking 14 actions.</span>
<span class="c1">#| label: fig:logistics-predictive</span>
<span class="n">motion_model_sigma</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">current_density</span> <span class="o">=</span> <span class="n">prior</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># prediction phase</span>
    <span class="n">control</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="c1"># ground truth control</span>
    <span class="n">current_density</span> <span class="o">=</span> <span class="n">prediction_step</span><span class="p">(</span><span class="n">current_density</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">motion_model_sigma</span><span class="p">)</span>
<span class="c1"># logistics.show_map(current_density/np.max(current_density) + 0.1*logistics.base_map)</span>
</pre></div>
</div>
</div>
</div>
<figure>
<img src="https://github.com/gtbook/robotics/blob/main/Figures4/predictive.gif?raw=1" id="fig:predictive" style="width:14cm" alt="">
<figcaption>Evolution of the predictive density (no measurements).</figcaption>
</figure>
<p>The figure above shows the result of purely predictive reasoning using a finite element discretization. As you can see, the density grows without bound, and also goes <em>inside</em> the shelves. Without any measurements, if we do not explicitly incorporate knowledge about the world (e.g., a map of obstacles),
the robot cannot rule out such situations.</p>
<p>The measurement update step takes the predictive density <span class="math notranslate nohighlight">\(P(X_{k}|\mathcal{Z}^{k-1},\mathcal{U}^{k})\)</span> and <em>upgrades</em> it to the posterior density:</p>
<div class="math notranslate nohighlight">
\[
P(X_{k}|\mathcal{Z}^{k},\mathcal{U}^{k}) \propto L(X_{k};z_{k})P(X_{k}|\mathcal{Z}^{k-1},\mathcal{U}^{k}).
\]</div>
<p>Note that this is so much simpler! We just have to code a pointwise multiplication and we are done. Because the ground truth trajectory never comes near any of the shelves, the proximity sensor is always <code class="docutils literal notranslate"><span class="pre">OFF</span></code>, and hence we multiply the predictive density with the corresponding <code class="docutils literal notranslate"><span class="pre">proximity_map_off</span></code> likelihood image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Posterior density after taking 14 actions.</span>
<span class="c1">#| label: fig:logistics-posterior</span>
<span class="n">predictive_density</span> <span class="o">=</span> <span class="n">prior</span>
<span class="n">posterior_density</span> <span class="o">=</span> <span class="n">predictive_density</span> <span class="o">*</span> <span class="n">logistics</span><span class="o">.</span><span class="n">proximity_map_off</span>
<span class="n">posterior_density</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">posterior_density</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># prediction phase</span>
    <span class="n">control</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="c1"># ground truth control</span>
    <span class="n">predictive_density</span> <span class="o">=</span> <span class="n">prediction_step</span><span class="p">(</span><span class="n">posterior_density</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">motion_model_sigma</span><span class="p">)</span>
    <span class="c1"># measurement update phase</span>
    <span class="n">posterior_density</span> <span class="o">=</span> <span class="n">predictive_density</span> <span class="o">*</span> <span class="n">logistics</span><span class="o">.</span><span class="n">proximity_map_off</span>
    <span class="n">posterior_density</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">posterior_density</span><span class="p">)</span>
<span class="c1"># logistics.show_map(posterior_density/np.max(posterior_density) + 0.1*logistics.base_map)</span>
</pre></div>
</div>
</div>
</div>
<figure>
<img src="https://github.com/gtbook/robotics/blob/main/Figures4/posterior.gif?raw=1" id="fig:posterior" style="width:14cm" alt="">
<figcaption>Markov localization in action.</figcaption>
</figure>
<p>The figure above shows Markov localization in action! The measurement information has <em>literally</em> squeezed the predictive density to a posterior that fuses predictive information with the knowledge
(from the proximity sensor) that we cannot be anywhere near the shelves, or the wall. Note that the density is still spreading in the direction that the sensor does <em>not</em> yield information.</p>
</section>
<section id="exercises">
<h3><span class="section-number">4.4.3.3. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In principle our motion model should have told us that we cannot move inside shelves. How would you change the prediction code to make this so?</p></li>
<li><p>The full Markov localization algorithm takes <em>less</em> time than the code <em>without</em> the measurement update. We do more work yet have faster inference. Why?</p></li>
</ul>
</section>
</section>
<section id="monte-carlo-localization">
<h2><span class="section-number">4.4.4. </span>Monte Carlo Localization<a class="headerlink" href="#monte-carlo-localization" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Localization with a particle filter is known as Monte Carlo Localization.</p>
</div></blockquote>
<p>The above finite element discretization of space is very costly, and most of the memory and computation is used to compute near-zero probabilities.
While there <em>are</em> ways to deal with this, switching to a sampling-based representation gets us more bang for the buck computation-wise. And, as we will see, it also leads to a very simple algorithm.
The sampling-based implementation of a Bayes filter is known as a <strong>particle filter</strong>.
Below we discuss it in a 2D context, but it is in fact rather general.
When used for robot localization, this technique is known as <a class="reference external" href="https://www.ri.cmu.edu/publications/monte-carlo-localization-for-mobile-robots/">“Monte Carlo Localization” or MCL</a>.</p>
<p>A particle filter approximates the Bayes filter by (a) replacing an explicit probability distribution by a set of samples, and (b) approximating the prediction step in the Bayes filter with a Monte Carlo approximation. We recursively assume that we have access to a sampling based approximation of the filtering density over the previous state <span class="math notranslate nohighlight">\(x_{k-1}\)</span>, as a set of <span class="math notranslate nohighlight">\(S\)</span> <em>weighted</em> samples:</p>
<div class="math notranslate nohighlight">
\[
p(X_{k-1}|\mathcal{Z}^{k-1},\mathcal{U}^{k-1}) \approx \{(x_{k-1}^{(s)}, w_{k-1}^{(s)})\}_{s =1 \dots S}
\]</div>
<p>in which <span class="math notranslate nohighlight">\(x_{k-1}^{(s)}\)</span> is the <span class="math notranslate nohighlight">\(s^{th}\)</span> sample, and <span class="math notranslate nohighlight">\(w_{k-1}^{(s)}\)</span> is its weight.</p>
<p>We then implement the process in two steps, called the <em>prediction</em> and <em>update</em> steps.:</p>
<ol class="arabic">
<li><p>In the prediction step we approximate the predictive distribution
<span class="math notranslate nohighlight">\(p(x_{k}|\mathcal{Z}^{k-1},\mathcal{U}^{k})\)</span> by drawing <span class="math notranslate nohighlight">\(S\)</span> <em>unweighted</em> samples from the following <strong>mixture density</strong> of motion model densities:</p>
<div class="math notranslate nohighlight">
\[
    \{ x_k^{(t)} \}_{t = 1 \dots S} \sim \sum_s w_{k-1}^{(s)} P(x_{k}|x_{k-1}^{(s)},u_{k-1})P(x_{k-1}^{(s)}|\mathcal{Z}^{k-1},\mathcal{U}^{k-1}).
    \]</div>
</li>
<li><p>In the update step, the measurement is used to convert this predictive density to a weighted sample approximation of the filtering distribution, by weighting each unweighted sample with the likelihood of <span class="math notranslate nohighlight">\(x_k^{(t)}\)</span> given the measurement <span class="math notranslate nohighlight">\(z_{k}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    p(X_{k}|\mathcal{Z}^{k},\mathcal{U}^{k}) \approx \{(x_k^{(t)}, L(x_k^{(t)};z_{k}))\}_{t = 1 \dots S}.
    \]</div>
</li>
</ol>
<p>We initialize the algorithm with a set of <span class="math notranslate nohighlight">\(S\)</span> samples from the prior, weighted by the likelihood on the first <span class="math notranslate nohighlight">\(x_1\)</span> given the first measurement <span class="math notranslate nohighlight">\(z_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P(X_1|\mathcal{Z}^1=\{z_1\}, \mathcal{U}^1=\{\}) \approx \{(x_1^{(t)}, L(x_1^{(t)};z_1))\}_{t = 1 \dots S}~\text{with}~x_1^{(t)}\sim p(X_1)
\]</div>
<p>In the “vanilla” particle filter outlined above, the number of samples stays constant over time, but variants exist that adapt the number of samples to the complexity of the density over time, or even to the available computation. Used in that way, a particle filter can be used as a “just in time” algorithm.</p>
<p>We will now describe the prediction and update steps in more detail, and illustrate the algorithm using our warehouse scenario.</p>
<section id="the-prediction-step">
<h3><span class="section-number">4.4.4.1. </span>The Prediction Step<a class="headerlink" href="#the-prediction-step" title="Link to this heading">#</a></h3>
<p>The prediction phase is the more complex of the two steps.
To illustrate how it works, we will us a simple example in which we represent
a Gaussian mixture density by a set of weighted particles.
A mixture density is simply a weighted sum of <em>component densities</em>, in this case a set of motion models. The weights define a probability mass function over the component indices and hence should sum to 1. Sampling from a mixture density proceeds in two steps:</p>
<ol class="arabic simple">
<li><p>Pick a component according to the weight distribution.</p></li>
<li><p>Sample from the component.</p></li>
</ol>
<p>As a toy example, let us assume a <em>Gaussian mixture density</em> with just two Gaussian components, with weights 0.9 and 0.1, respectively:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="p">[</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">20</span><span class="p">,</span><span class="mi">25</span><span class="p">),(</span><span class="mi">70</span><span class="p">,</span><span class="mi">40</span><span class="p">)]]</span>
<span class="n">covariances</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="n">sx</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">sy</span><span class="o">**</span><span class="mi">2</span><span class="p">])</span> <span class="k">for</span> <span class="n">sx</span><span class="p">,</span><span class="n">sy</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">),(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)]]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">go</span><span class="o">.</span><span class="n">Contour</span><span class="p">(</span><span class="n">z</span><span class="o">=</span><span class="n">logistics</span><span class="o">.</span><span class="n">gaussian</span><span class="p">(</span><span class="n">logistics</span><span class="o">.</span><span class="n">map_coords</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">),</span> <span class="n">contours_coloring</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span>
        <span class="n">line_width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">showscale</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">mean</span><span class="p">,</span><span class="n">cov</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span><span class="n">covariances</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Two Gaussian distributions that combine into a mixture density.</span>
<span class="c1">#| label: fig:logistics-mixture</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">);</span> <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/54c2c64c383070826996f9b8d4d1621945185093816c6f63eab66ce8d39f1ed3.png" src="_images/54c2c64c383070826996f9b8d4d1621945185093816c6f63eab66ce8d39f1ed3.png" />
</div>
</div>
<p>The following code generates 100 samples, rather inefficiently:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Samples from a mixture of two Gaussian distributions.</span>
<span class="c1">#| label: fig:logistics-samples</span>
<span class="n">component_indices</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.1</span><span class="p">])</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">covariances</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">component_indices</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2be3623062e67b877a1f45d073ed43670c05d448198e91118e964ef07dc87061.png" src="_images/2be3623062e67b877a1f45d073ed43670c05d448198e91118e964ef07dc87061.png" />
</div>
</div>
<p>As you can see the first component is chosen much more often, which is exactly what we expect.</p>
<p>Armed with this insight about sampling mixtures, we apply it below to code up the MCL prediction step:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_samples</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">motion_model_sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create predictive density from weighted samples given control and control stddev.&quot;&quot;&quot;</span>
    <span class="n">weights</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">component_indices</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">samples</span> <span class="o">+</span> <span class="n">control</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">motion_model_sigma</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">cov</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">component_indices</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluating-the-likelihood-in-the-update-step">
<h3><span class="section-number">4.4.4.2. </span>Evaluating the Likelihood in the Update Step<a class="headerlink" href="#evaluating-the-likelihood-in-the-update-step" title="Link to this heading">#</a></h3>
<p>As before, we know that the ground truth trajectory stays away from obstacles at all times. However, we only have access to a likelihood <em>image</em>, not a function that evaluates the likelihood for an arbitrary coordinate. The following function calculates the correct cell for looking up the likelihood, and returns 0 likelihood if out of bounds:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">likelihood_off</span><span class="p">(</span><span class="n">xy</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate likelihood by looking up value in proximity_map_off.&quot;&quot;&quot;</span>
    <span class="n">j</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">49</span> <span class="ow">or</span> <span class="n">j</span><span class="o">&lt;</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">j</span><span class="o">&gt;</span><span class="mi">99</span><span class="p">:</span> <span class="k">return</span> <span class="mf">0.0</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">logistics</span><span class="o">.</span><span class="n">proximity_map_off</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="mcl-warehouse-example">
<h3><span class="section-number">4.4.4.3. </span>MCL Warehouse Example<a class="headerlink" href="#mcl-warehouse-example" title="Link to this heading">#</a></h3>
<p>Now that we know how to do the prediction step and evaluate the likelihood function, we are able to implement the entire Monte Carlo localization method. First, let us obtain 500 samples from the prior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: 500 samples from the prior density at the start.</span>
<span class="c1">#| label: fig:logistics-prior-samples</span>
<span class="n">S</span><span class="o">=</span><span class="mi">500</span>
<span class="n">prior_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">prior_mean</span><span class="p">,</span> <span class="n">prior_cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="n">prior_samples</span><span class="p">,</span>
         <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/649bce0cda01a1b75381cacf38041937eaa01582b031f4be1878d3ddcd510ff6.png" src="_images/649bce0cda01a1b75381cacf38041937eaa01582b031f4be1878d3ddcd510ff6.png" />
</div>
</div>
<p>Compare this sampling-based representation with the Markov localization representation of the prior above, and note they represent the <em>same</em> density, but using vastly different representations. In fact, they also use vastly different resources: for Markov localization we used 5000 cells, and here we use 500 samples, each represented as a two-dimensional vector. In higher-dimensional state spaces this difference is even greater: what would happen if we wanted to represent orientation as well?</p>
<p>However, as discussed, we start off MCL with a set of <em>weighted</em> samples
<span class="math notranslate nohighlight">\(\{(x_1^{(t)}, L(x_1^{(t)};z_1))\}_{t = 1 \dots S}\)</span>, which represents the posterior for <span class="math notranslate nohighlight">\(k=1\)</span>. We do this by weighting each sample from the prior with the likelihood:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: By weighting the samples with the likelihood, we get a representation for the posterior.</span>
<span class="c1">#| label: fig:logistics-posterior-samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">prior_samples</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">likelihood_off</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span>
         <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="n">weights</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weights</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/573cd6acd99430e9f170d1a283dca6cdd5e8040bcacc7068bf215a9a1a7fe858.png" src="_images/573cd6acd99430e9f170d1a283dca6cdd5e8040bcacc7068bf215a9a1a7fe858.png" />
</div>
</div>
<p>Note that all samples close to the wall and/or the shelves have disappeared, as they have zero weight. All remaining samples have the same weight.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Monte Carlo Localization in action!</span>
<span class="c1">#| label: fig:mcl_in_action</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># prediction phase</span>
    <span class="n">control</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>  <span class="c1"># ground truth control</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">predict_samples</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">motion_model_sigma</span><span class="p">)</span>
    <span class="c1"># measurement update phase</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">likelihood_off</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
<span class="c1"># logistics.show_map(0.1*logistics.base_map, markers=samples,</span>
<span class="c1">#          marker=dict(color=&quot;red&quot;, opacity=0.1, size=10*weights/np.max(weights)))</span>
</pre></div>
</div>
</div>
</div>
<figure>
<img src="https://github.com/gtbook/robotics/blob/main/Figures4/samples.gif?raw=1" id="fig:samples" style="width:14cm" alt="">
<figcaption>Monte Carlo localization in action.</figcaption>
</figure>
<p>The figure above shows Monte Carlo localization in action! Comparing with Markov localization, we see that the results are consistent. Not only that, but if you look at the timing numbers, MCL runs at least an order of magnitude faster.</p>
<p>You might wonder about the samples that made it into the seemingly <em>wrong</em> aisles between the shelves. However, if we crank up the Markov localization visualization by a factor of 10, you will see that <em>in</em> fact the probability of being in those aisles is actually non-zero:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Markov Localization visualized with a heatmap.</span>
<span class="c1">#| label: fig:logistics-markov-heatmap</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">posterior_density</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">posterior_density</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d6e7d31794cbc66d47240bd68cc1d13f0d928d23ea4e53bbb6f412f44fb27c42.png" src="_images/d6e7d31794cbc66d47240bd68cc1d13f0d928d23ea4e53bbb6f412f44fb27c42.png" />
</div>
</div>
<p>MCL can be made to be <em>more</em> accurate than Markov localization, since the accuracy of the sample-based approximation can be increased by increasing the number of samples. Of course, we can also increase the resolution of the Markov localization representation, but at exponential cost. In contrast, the samples needed are proportional to the “volume” that the density occupies, an observation that underlies adaptive variants of the particle filter.</p>
</section>
<section id="adding-range-sensing">
<h3><span class="section-number">4.4.4.4. </span>Adding Range Sensing<a class="headerlink" href="#adding-range-sensing" title="Link to this heading">#</a></h3>
<p>Finally, and this is will be rather easy in MCL, let us investigate the effect of adding range sensing. This time we won’t have the luxury to know the entire measurement sequence in advance, as we did with proximity. Rather, the likelihood will have to be given the output of the RFID sensor. Remember that was either a beacon ID and a range <span class="math notranslate nohighlight">\((i,r)\)</span>, or <span class="math notranslate nohighlight">\((\text{None},\infty)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">likelihood_range</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">rfid_measurement</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate likelihood of xy given range measurement.&quot;&quot;&quot;</span>
    <span class="n">_id</span><span class="p">,</span> <span class="n">_range</span> <span class="o">=</span> <span class="n">rfid_measurement</span>
    <span class="k">if</span> <span class="n">_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">j</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">49</span> <span class="ow">or</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="mi">99</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">logistics</span><span class="o">.</span><span class="n">out_of_bound_map</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># In meters</span>
        <span class="n">range_for_xy</span> <span class="o">=</span> <span class="n">logistics</span><span class="o">.</span><span class="n">rfid_range</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">logistics</span><span class="o">.</span><span class="n">beacons</span><span class="p">[</span><span class="n">_id</span><span class="p">])</span>
        <span class="k">return</span> <span class="mf">0.0</span> <span class="k">if</span> <span class="n">range_for_xy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">range_for_xy</span> <span class="o">-</span> <span class="n">_range</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can also use this to illustrate how to <em>globally</em> localize, without any prior information at all, except that we are <em>somewhere</em> in the warehouse. We can encode this knowledge as samples by uniformly sampling over the entire range for <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This corresponds to assuming that the prior distribution on the initial state is
a uniform distribution, sometimes called the <em>uniform prior</em> assumption.
Because our knowledge is now so diffuse, we use 2000 samples instead of 500 to better approximate it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: 2000 samples from an *uninformative* prior density at the start, i.e., we don&#39;t know where we are.</span>
<span class="c1">#| label: fig:logistics-uninformative-prior-samples</span>
<span class="n">T</span><span class="o">=</span><span class="mi">2000</span>
<span class="n">prior_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">high</span><span class="o">=</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="n">prior_samples</span><span class="p">,</span>
         <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7c30b0f667e14dc66c653f7906ef2d2aa191a43bf9f64e4afe810c4637178c74.png" src="_images/7c30b0f667e14dc66c653f7906ef2d2aa191a43bf9f64e4afe810c4637178c74.png" />
</div>
</div>
<p>The first range measurement happens to be out of range, and we can see the effect on our estimate for the posterior <span class="math notranslate nohighlight">\(p(x_1|z_1)\)</span> quite clearly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Sample-based posterior after a single &quot;out of range&quot; measurement.</span>
<span class="c1">#| label: fig:logistics-posterior-samples-out-of-range</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">prior_samples</span>
<span class="n">range_measurement</span> <span class="o">=</span> <span class="n">logistics</span><span class="o">.</span><span class="n">rfid_measurement</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">likelihood_range</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">range_measurement</span><span class="p">)</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span>
         <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="n">weights</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weights</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/89e1f9911534a7fe3780192deaac94fff4b8165bbbeb600d07dd77eada5b6254.png" src="_images/89e1f9911534a7fe3780192deaac94fff4b8165bbbeb600d07dd77eada5b6254.png" />
</div>
</div>
<p>As expected, the likelihood has “punched holes” around the beacons, because we <em>know</em> we cannot have been near them. When we combine this with the proximity OFF measurement, we improve our posterior even more:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Upgrading the posterior with a single &quot;proximity=OFF&quot; measurement.</span>
<span class="c1">#| label: fig:logistics-posterior-samples-proximity-off</span>
<span class="n">weights</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">likelihood_off</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span>
         <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="n">weights</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weights</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0608d40c52435a3383f1ba869f91254c88d52cdfa276ca32aa6f748659fae38f.png" src="_images/0608d40c52435a3383f1ba869f91254c88d52cdfa276ca32aa6f748659fae38f.png" />
</div>
</div>
<p>We are now ready to bring it all together by multiplying <em>two</em> likelihoods in every measurement update phase:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Global localization using Monte Carlo Localization.</span>
<span class="c1">#| label: fig:logistics-mcl-global</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># prediction phase</span>
    <span class="n">control</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>  <span class="c1"># ground truth control</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">predict_samples</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">motion_model_sigma</span><span class="p">)</span>
    <span class="c1"># measurement update phase</span>
    <span class="n">range_measurement</span> <span class="o">=</span> <span class="n">logistics</span><span class="o">.</span><span class="n">rfid_measurement</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span>
        <span class="n">likelihood_range</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">range_measurement</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">likelihood_off</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
<span class="c1"># logistics.show_map(0.1*logistics.base_map, markers=samples,</span>
<span class="c1">#          marker=dict(color=&quot;red&quot;, opacity=0.1, size=10*weights/np.max(weights)))</span>
</pre></div>
</div>
</div>
</div>
<figure>
<img src="https://github.com/gtbook/robotics/blob/main/Figures4/global.gif?raw=1" id="fig:global" style="width:14cm" alt="">
<figcaption>Global localization using MCL aided by range sensing.</figcaption>
</figure>
<p>The figure above shows global localization in action! At the onset, the robot is very confused as to where it might be in the warehouse, but it still has <em>some</em> knowledge. Then it moves within range of beacon <span class="math notranslate nohighlight">\(0\)</span>, which immediately localizes it, acting a bit like a GPS sensor, especially in combination with what was sensed before. After that the particular shape of the range likelihoods makes for some interesting antics, and at the end we are quite a bit more certain that the robot is in the center aisle.</p>
<p>This example showed off <em>two</em> nice properties of Monte Carlo localization (and, for that matter, Markov localization):</p>
<ol class="arabic simple">
<li><p>We can reason about multi-model densities.</p></li>
<li><p>We can incorporate missing information/data, such as out of range measurements.</p></li>
</ol>
<p>To emphasize the latter point even more, note that for the last few time steps in the animation above we are <em>out</em> of beacon range, and you can see a bit of “squeezing” of the sample cloud at the very top, because of this.</p>
</section>
</section>
<section id="kalman-smoothing-and-filtering">
<h2><span class="section-number">4.4.5. </span>Kalman Smoothing and Filtering<a class="headerlink" href="#kalman-smoothing-and-filtering" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>For linear systems with Gaussian uncertainty, the Kalman filter is both efficient and optimal.</p>
</div></blockquote>
<p>The final and mathematically most challenging approach we will discuss is Kalman smoothing and filtering. Ironically, while the math is more difficult, the circumstances in which these methods can be applied are more limited.
The math is only exact for (a) linear measurement and motion models and (b) Gaussian additive noise.
In addition, although there are ways to deal with nonlinear models, a more serious limitation is that the density on the state is restricted to Gaussian distributions.</p>
<p>These methods are incredibly important in robotics. In spite of all their limitations, Kalman smoothers and filters are incredibly efficient, and are at the core of the navigation stack
(i.e., the software that handles everything from low-level sensor processing to higher-level localization)
of many systems, robots and otherwise.
In fact, the Kalman filter was implemented in the sixties on Apollo flight computers,
which had extremely limited computational hardware.</p>
<section id="factor-graphs-and-least-squares">
<h3><span class="section-number">4.4.5.1. </span>Factor Graphs and Least Squares<a class="headerlink" href="#factor-graphs-and-least-squares" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>When given measurements and actions, we can create a factor graph directly.</p>
</div></blockquote>
<p><strong>Kalman smoothing</strong> is the equivalent of the Viterbi algorithm for hidden Markov model, but for continuous state spaces.</p>
<p>In the continuous case, we build the factors directly from given control inputs and measurements.
A factor graph now encodes the negative log-posterior</p>
<div class="math notranslate nohighlight">
\[\Phi(X)=\sum_i \phi(X_i) = \frac{1}{2} \sum_i \|A_i X_i-b_i\|^2.\]</div>
<p>In the continuous case we use <em>minimization</em> of the log-likelihood rather than maximization over the probabilities. The main reason is because then inference becomes a linear least squares problem.</p>
<p>A least-squares problem is convex and can be solved in closed form. The linear terms within the square norm above are <em>sparse</em>, in the sense that <span class="math notranslate nohighlight">\(X_i\)</span> is only a <em>subset</em> of the continuous variables <span class="math notranslate nohighlight">\(X\)</span>. For example, a simple sensor fusion problem on two scalar variables <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> would be one where both have a simple prior on them, resp. with <span class="math notranslate nohighlight">\(\mu_1=3\)</span> and <span class="math notranslate nohighlight">\(\mu_2=7\)</span>, and we know that the difference between them is equal to <span class="math notranslate nohighlight">\(5\)</span>. This yields three factors that have to be minimized over <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\Phi(X)= \frac{1}{2} \|x_1-3\|^2 + \frac{1}{2} \|(x_2-x_1)-5\|^2 + \frac{1}{2} \|x_2-7\|^2.
\]</div>
<p>This corresponds to the following</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(X_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(A_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(b_i\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_1\)</span></p></td>
<td><p>[1]</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(x_1, x_2\)</span></p></td>
<td><p>[-1 1]</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_2\)</span></p></td>
<td><p>[1]</p></td>
<td><p>7</p></td>
</tr>
</tbody>
</table>
<p>We can collect all three matrices <span class="math notranslate nohighlight">\(A_1\)</span>, <span class="math notranslate nohighlight">\(A_2\)</span>, and <span class="math notranslate nohighlight">\(A_3\)</span> into one sparse matrix <span class="math notranslate nohighlight">\(A\)</span> and corresponding right-hand-side <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A=\begin{pmatrix} 1 &amp; 0 \\ -1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}~~\text{and}~~B=\begin{pmatrix} 3 \\ 5 \\ 7 \end{pmatrix}.\end{split}\]</div>
<p>You can convince yourself that the minimization problem can then be written as</p>
<div class="math notranslate nohighlight">
\[X^* = \arg \min_X \Phi(X) = \arg \min_X \frac{1}{2} \|A X-b\|^2\]</div>
<p>which is now a <em>sparse</em> linear least-squares problem.</p>
<p>Setting the derivative to zero and solving for <span class="math notranslate nohighlight">\(X^*\)</span> we obtain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
A^T(A X^* - b) &amp;= 0 \\
(A^T A) X^* &amp;= A^T b \\
X^* &amp;= (A^T A)^{-1} A^T b 
\end{aligned}\end{split}\]</div>
<p>This is known as a <strong>system of normal equations</strong>, and the last equation above is a closed from solution for it.</p>
<p>Solving a linear least-squares problem this way can be expensive. The matrix <span class="math notranslate nohighlight">\(A\)</span> can be very high dimensional, but luckily in many sensor fusion scenarios it is exceedingly <em>sparse</em>. Still, we have to proceed with some care, as in general the matrix <span class="math notranslate nohighlight">\((A^T A)^{-1}\)</span> is <em>not</em> sparse. The matrix <span class="math notranslate nohighlight">\(Q \doteq A^T A\)</span> is known as the <strong>Hessian</strong> or <strong>Information Matrix</strong> and has dimensions <span class="math notranslate nohighlight">\(n\times n\)</span> if there are <span class="math notranslate nohighlight">\(n\)</span> scalar unknowns. In general, it takes <span class="math notranslate nohighlight">\(O(n^3)\)</span> work to compute it, which is only 27 for <span class="math notranslate nohighlight">\(n=3\)</span>, but is already 1 <em>billion</em> for <span class="math notranslate nohighlight">\(n=1000\)</span>.</p>
</section>
<section id="numerical-example">
<h3><span class="section-number">4.4.5.2. </span>Numerical Example<a class="headerlink" href="#numerical-example" title="Link to this heading">#</a></h3>
<p>This method is trivial to implement in numpy for the small example above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">7.</span><span class="p">])</span>
<span class="n">X_optimal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_optimal</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2.66666667 7.33333333]
</pre></div>
</div>
</div>
</div>
<p>Note that in this simple sensor fusion example, a compromise solution is obtained: all factors are equally <em>unhappy</em>. We can compute the error vector as <span class="math notranslate nohighlight">\(A X^* - b\)</span> and find that the errors is equally divided between all factors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">X_optimal</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.33333333 -0.33333333  0.33333333]
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise">
<h3><span class="section-number">4.4.5.3. </span>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">#</a></h3>
<p>What would make the errors <em>not</em> divide equally between factors?</p>
</section>
<section id="sparse-least-squares">
<h3><span class="section-number">4.4.5.4. </span>Sparse Least-Squares<a class="headerlink" href="#sparse-least-squares" title="Link to this heading">#</a></h3>
<p>In practice we use <em>sparse factorization methods</em> to solve for <span class="math notranslate nohighlight">\(X^*\)</span>. In particular, <em>sparse Cholesky</em> factorization can efficiently decompose the sparse Hessian <span class="math notranslate nohighlight">\(Q\)</span> into its matrix square root <span class="math notranslate nohighlight">\(R\)</span></p>
<div class="math notranslate nohighlight">
\[A^T A=R^T R\]</div>
<p>where <span class="math notranslate nohighlight">\(R\)</span> is <em>upper-triangular</em> and sparse as well. There is one wrinkle: the sparsity of <span class="math notranslate nohighlight">\(R\)</span> depends (dramatically) on the column ordering chosen for <span class="math notranslate nohighlight">\(A\)</span>, and the optimal ordering is hard to find (NP-complete, in fact). Hence, many heuristic ordering methods exist that are applied in practice.</p>
<p>After we find <span class="math notranslate nohighlight">\(R\)</span>, which is where the heavy lifting occurs, we can efficiently solve for <span class="math notranslate nohighlight">\(X^*\)</span> in two steps. First, we solve for an auxiliary vector <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
(R^T R) X^* &amp;= A^T b \\
R^T (R X^*) &amp;= A^T b \\
R^T y &amp;= A^T b
\end{aligned}\end{split}\]</div>
<p>In the last line, <span class="math notranslate nohighlight">\(R^T\)</span> is lower-triangular, and hence <span class="math notranslate nohighlight">\(y\)</span> can be easily solved for proceeding from <span class="math notranslate nohighlight">\(y_1\)</span> to <span class="math notranslate nohighlight">\(y_n\)</span>, via <em>forward-substitution</em>. We then turn around use the computed value of <span class="math notranslate nohighlight">\(y\)</span> to solve for <span class="math notranslate nohighlight">\(X^*\)</span> by <em>back-substitution</em>:</p>
<div class="math notranslate nohighlight">
\[R X^*=y.\]</div>
<p>Both of these steps are <span class="math notranslate nohighlight">\(O(n^2)\)</span> for dense matrices, but typically closer to <span class="math notranslate nohighlight">\(O(n)\)</span> for sparse matrices.</p>
<p>There is a deep connection between sparse factorization methods and the sum-product algorithm discussed in the previous chapter. In fact, sparse factorization <em>is</em> sum-product, where one continuous variable is eliminated at a time. The factor graph corresponding to a sparse least-squares problem corresponds to its sparsity pattern. In this graphical framework, the <em>product</em> is implemented by collecting all factors connected to that variable, and the <em>sum</em> is implemented by expressing the variable to be eliminated as a linear combination of its neighbors in the graph.</p>
</section>
<section id="example-with-gps-like-measurements">
<h3><span class="section-number">4.4.5.5. </span>Example with GPS-Like Measurements<a class="headerlink" href="#example-with-gps-like-measurements" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>We build a factor graph and then simply call <code class="docutils literal notranslate"><span class="pre">optimize</span></code>.</p>
</div></blockquote>
<p>GTSAM is built around state-of-the-art sparse factorization methods, and hence can very efficiently solve large sensor fusion problems, like the localization problem with GPS-like measurements.
Below we illustrate this with an example in code for the GPS-like measurements, which is easily tackled using optimization. Range measurements can also be handled.</p>
<p>We simulate measurements using the ground truth trajectory but with noise added, by creating a quick Bayes net do this for us. Remember from the previous section that the measurements are in centimeters, so the number <span class="math notranslate nohighlight">\(30\)</span> below is the standard deviation of the measurement noise, in centimeters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">measurement_model_sigma</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">symbol</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">}</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">GaussianBayesNet</span><span class="p">()</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
    <span class="n">conditional_on_zk</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">GaussianConditional</span><span class="o">.</span><span class="n">FromMeanAndStddev</span><span class="p">(</span>
        <span class="n">z</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">C</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">measurement_model_sigma</span><span class="p">)</span>
    <span class="n">bn</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">conditional_on_zk</span><span class="p">)</span>
<span class="n">simulation</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;measurement on </span><span class="si">{</span><span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">simulation</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>measurement on [10.  6.] = [1038.61635761  632.63644323]
</pre></div>
</div>
</div>
</div>
<p>Notice how the measurement is scaled by 100, and is corrupted by noise. Now we are finally ready to create a GaussianFactorGraph that will fuse the information from the prior, the measurements, and the motion models:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gfg</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">GaussianFactorGraph</span><span class="p">()</span>

<span class="c1"># The prior</span>
<span class="n">p_x1</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">GaussianDensity</span><span class="o">.</span><span class="n">FromMeanAndStddev</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">gfg</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">p_x1</span><span class="p">)</span>

<span class="c1"># Create motion model factors from ground truth trajectory displacements</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">displacement</span> <span class="o">=</span> <span class="n">next_state</span> <span class="o">-</span> <span class="n">state</span>
    <span class="c1"># |next_state - (A*state + B*u)|</span>
    <span class="n">gfg</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">GaussianConditional</span><span class="o">.</span><span class="n">FromMeanAndStddev</span><span class="p">(</span>
        <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">displacement</span><span class="p">,</span> <span class="n">motion_model_sigma</span><span class="p">))</span>

<span class="c1"># Convert the conditionals in Bayes net from above into likelihood factors</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
    <span class="n">conditional_on_zk</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">measurement</span> <span class="o">=</span> <span class="n">simulation</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="n">likelihood_on_xk</span> <span class="o">=</span> <span class="n">conditional_on_zk</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">measurement</span><span class="p">)</span>
    <span class="n">gfg</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">likelihood_on_xk</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Below we show the factor graph, which looks exactly like the one for inference in an HMM, except we now have more states, and they are continuous. The two problems have essentially the same computational <em>structure</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">position_hints</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:(</span><span class="mf">0.35</span><span class="p">,</span><span class="mi">1</span><span class="p">)}</span>
<span class="n">pos</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:(</span><span class="n">k</span><span class="o">+</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]})</span> <span class="c1"># binary</span>
<span class="n">pos</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="o">+</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">:(</span><span class="n">k</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">})</span> <span class="c1"># unary</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: The factor graph for a Kalman smoother.</span>
<span class="c1">#| label: fig:logistics-factor-graph</span>
<span class="n">show</span><span class="p">(</span><span class="n">gfg</span><span class="p">,</span> <span class="n">hints</span><span class="o">=</span><span class="n">position_hints</span><span class="p">,</span> <span class="n">factor_positions</span><span class="o">=</span><span class="n">pos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2eab6ad88b3f096e1415ccf1c2c0e85f1d301ebab6b653d4c74673657db07151.svg" src="_images/2eab6ad88b3f096e1415ccf1c2c0e85f1d301ebab6b653d4c74673657db07151.svg" /></div>
</div>
<p>Now we can find the optimal solution, using the <code class="docutils literal notranslate"><span class="pre">optimize</span></code> method. In the Gaussian factor graph case, this solves a linear least-squares problem with a continuous variant of the max-product algorithm from Section 3.4:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">least_squares_solution</span> <span class="o">=</span> <span class="n">gfg</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Below we compare the estimated trajectory with the ground truth trajectory:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Estimated trajectory using a Kalman smoother.</span>
<span class="c1">#| label: fig:logistics-estimated</span>
<span class="n">estimated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">least_squares_solution</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
<span class="n">logistics</span><span class="o">.</span><span class="n">show_map</span><span class="p">(</span><span class="n">logistics</span><span class="o">.</span><span class="n">base_map</span><span class="p">,</span> <span class="n">estimated</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/434c80da4e8cc0f53b52e456e88bef8fff23191db5ae0037db0b56e06f4720a5.png" src="_images/434c80da4e8cc0f53b52e456e88bef8fff23191db5ae0037db0b56e06f4720a5.png" />
</div>
</div>
</section>
</section>
<section id="gtsam-102">
<h2><span class="section-number">4.4.6. </span>GTSAM 102<a class="headerlink" href="#gtsam-102" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>A deeper dive in the GTSAM concepts used above.</p>
</div></blockquote>
<p>Recall the <code class="docutils literal notranslate"><span class="pre">gtsam.GaussianFactorGraph</span></code> <code class="docutils literal notranslate"><span class="pre">gfg</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Factor graph for a Kalman smoother, again.</span>
<span class="c1">#| label: fig:logistics-factor-graph-again</span>
<span class="n">show</span><span class="p">(</span><span class="n">gfg</span><span class="p">,</span> <span class="n">hints</span><span class="o">=</span><span class="n">position_hints</span><span class="p">,</span> <span class="n">factor_positions</span><span class="o">=</span><span class="n">pos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2eab6ad88b3f096e1415ccf1c2c0e85f1d301ebab6b653d4c74673657db07151.svg" src="_images/2eab6ad88b3f096e1415ccf1c2c0e85f1d301ebab6b653d4c74673657db07151.svg" /></div>
</div>
<p>We can also visualize the sparse <span class="math notranslate nohighlight">\(A\)</span> matrix, which is called the <strong>sparse Jacobian</strong> of the system:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: The Jacobian matrix $A$ (and RHS $b$) corresponding to the above factor graph</span>
<span class="c1">#| label: fig:logistics_proximity_map2</span>
<span class="n">A</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">gfg</span><span class="o">.</span><span class="n">jacobian</span><span class="p">()</span>
<span class="n">px</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">color_continuous_scale</span><span class="o">=</span><span class="s1">&#39;Reds&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e240f0332e839150a6c36cc7b622450ab4ba0cfe7e45a8613a46c6a92e33dc66.png" src="_images/e240f0332e839150a6c36cc7b622450ab4ba0cfe7e45a8613a46c6a92e33dc66.png" />
</div>
</div>
<p>In fact, the deep connection between factor graphs and sparse linear algebra is this:</p>
<blockquote>
<div><p>Columns correspond to variables, and rows correspond to factors.</p>
</div></blockquote>
<p>You can make out, from the top:</p>
<ul class="simple">
<li><p>the first two rows correspond to the prior</p></li>
<li><p>the two darker diagonals correspond to the motion model factors</p></li>
<li><p>the single red diagonal corresponds to the measurements</p></li>
</ul>
<p>We can use the method <code class="docutils literal notranslate"><span class="pre">eliminateSequential</span></code> to use the linear-Gaussian sum-product algorithm (matrix factorization!) to turn it into a Bayes net, which encodes the multivariate Gaussian posterior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: Bayes net representing a multivariate Gaussian posterior. </span>
<span class="c1">#| label: fig:bayes-net-gaussian-posterior</span>
<span class="n">gaussian_posterior</span> <span class="o">=</span> <span class="n">gfg</span><span class="o">.</span><span class="n">eliminateSequential</span><span class="p">()</span>
<span class="n">show</span><span class="p">(</span><span class="n">gaussian_posterior</span><span class="p">,</span> <span class="n">hints</span><span class="o">=</span><span class="n">position_hints</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/93b8a35e6ae23603bee759ff1843f247c1304219a4641b9f31480f8d37ac79fe.svg" src="_images/93b8a35e6ae23603bee759ff1843f247c1304219a4641b9f31480f8d37ac79fe.svg" /></div>
</div>
<p>This Bayes net <em>is</em> the upper-triangular (DAG!) Cholesky factor <span class="math notranslate nohighlight">\(R\)</span> we discussed above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| caption: The upper triangular matrix corresponding to the above Bayes net. </span>
<span class="c1">#| label: fig:upper-triangular-gaussian-posterior</span>
<span class="n">R</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">gaussian_posterior</span><span class="o">.</span><span class="n">matrix</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">px</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">R</span><span class="p">),</span> <span class="n">color_continuous_scale</span><span class="o">=</span><span class="s1">&#39;Reds&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9dcbaf84a612f9a7ae02919a38fed0fe4a84d2e92b69544b60495ff74be1d399.png" src="_images/9dcbaf84a612f9a7ae02919a38fed0fe4a84d2e92b69544b60495ff74be1d399.png" />
</div>
</div>
<p>Indeed, here the correspondence is:</p>
<blockquote>
<div><p>Columns correspond to variables, and rows correspond to Gaussian conditionals.</p>
</div></blockquote>
<p>In general, the correspondence is about <em>block</em> columns, as variables in this case are two-dimensional, and <em>block</em> rows. For example, the first <code class="docutils literal notranslate"><span class="pre">GaussianConditional</span></code> in the <code class="docutils literal notranslate"><span class="pre">gaussian_posterior</span></code> Bayes net corresponds to the first two rows above, and can be shown like so:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gaussian_posterior</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;First two rows are: &quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First two rows are:  p(x1 | x2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  R = [ 3.91933       0 ]
      [       0 3.91933 ]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  S[x2] = [ -0.0637865          0 ]
          [          0 -0.0637865 ]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  d = [ 39.3312 24.0585 ]
  No noise model
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="S43_logistics_sensing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4.3. </span>Sensor Models with Continuous State</p>
      </div>
    </a>
    <a class="right-next"
       href="S45_logistics_planning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4.5. </span>Planning for Logistics</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-running-example">4.4.1. A Running Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayes-filter">4.4.2. The Bayes Filter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-localization">4.4.3. Markov Localization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-1d-example">4.4.3.1. A 1D Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warehouse-example">4.4.3.2. Warehouse Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">4.4.3.3. Exercises</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-localization">4.4.4. Monte Carlo Localization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prediction-step">4.4.4.1. The Prediction Step</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-likelihood-in-the-update-step">4.4.4.2. Evaluating the Likelihood in the Update Step</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mcl-warehouse-example">4.4.4.3. MCL Warehouse Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-range-sensing">4.4.4.4. Adding Range Sensing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kalman-smoothing-and-filtering">4.4.5. Kalman Smoothing and Filtering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-graphs-and-least-squares">4.4.5.1. Factor Graphs and Least Squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-example">4.4.5.2. Numerical Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">4.4.5.3. Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-least-squares">4.4.5.4. Sparse Least-Squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-gps-like-measurements">4.4.5.5. Example with GPS-Like Measurements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gtsam-102">4.4.6. GTSAM 102</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank Dellaert and Seth Hutchinson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>