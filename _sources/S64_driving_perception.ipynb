{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "tags": [
     "no-tex"
    ]
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/master/S64_driving_perception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q -U gtbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    import plotly.io as pio\n",
    "    pio.renderers.default = \"png\"\n",
    "\n",
    "import gtsam\n",
    "import gtsam.utils.plot as gtsam_plot\n",
    "\n",
    "from gtbook import driving\n",
    "from gtbook.display import show\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# SLAM\n",
    "\n",
    "> SLAM is *Simultaneous Localization and Mapping*, a key capability for mobile robots.\n",
    "\n",
    "<img src=\"Figures6/S64-Autonomous_Vehicle_with_LIDAR_and_cameras-03.jpg\" alt=\"Splash image with steampunk autonomous car\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an autonomous vehicle explores a new environment, it should be able to simultaneously build a map of this environment, and localize itself relative to that map. That capability is known as *Simultaneous Localization and Mapping* or **SLAM**. SLAM is a bit like a chicken and egg problem: if we knew the map, we could use the localization techniques we studied in Section 4.4, e.g., Monte Carlo Localization. Conversely, if we were already localized, it would not be too hard to create a map: if we have a LIDAR sensor, we could simply transform the points from the LIDAR frame into the world frame. But now we have to do *both* at the same time! Is that even possible? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we show that this *is* possible, and we expand upon two different SLAM techniques: *PoseSLAM* and *SLAM with landmarks*. In the former we concentrate on estimating the robot's motion from a rich sensor such as LIDAR, and building the map is generated as a side effect. In the latter, we truly optimize for both the location of the vehicle and the location of a observed landmarks, which leads to a sparse map of the environment. Before investigating these two SLAM algorithms, we review the math of vehicle poses in 2D, and discuss ICP, a seminal algorithm for aligning two point clouds, and which we use as a building block in PoseSLAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsB-xgTtlV-6"
   },
   "source": [
    "## Poses in 2D and 3D\n",
    "\n",
    "> SE(2) generalizes to SE(3).\n",
    "\n",
    "In both SLAM variants we need to optimize over the pose of a vehicle.\n",
    "To keep things simple, we will concentrate on poses in the plane for now. \n",
    "Recall from Section 6.1 that a 2D pose\n",
    "$T\\doteq(x,y,\\theta)$ is an element of the Special Euclidean group $SE(2)$,\n",
    "and\n",
    "can be represented by a $3\\times3$ matrix of the form\n",
    "\n",
    "$$\n",
    "T=\\left[\\begin{array}{cc|c}\n",
    "\\cos\\theta & -\\sin\\theta & x\\\\\n",
    "\\sin\\theta & \\cos\\theta & y\\\\\n",
    "\\hline 0 & 0 & 1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "R & t\\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "with the $2\\times1$ vector $t$\n",
    "representing the position of the vehicle, and $R$ the $2\\times2$\n",
    "rotation matrix representing the vehicle’s orientation in the plane.\n",
    "\n",
    "Below it is important to understand what coordinate frames are involved, and hence we insist on always annotating the transformation matrices with indices indicating the reference frame as superscript and the frame under consideration using subscript. This convention also\n",
    "provides a reminder to how points in one frame are transformed into the other. \n",
    "As an example, the following illustrates how points in the frame $b$ are transformed into the reference frame $a$ by referring to the pose $T^a_b$ of frame $b$ in $a$:\n",
    "\n",
    "$$\n",
    "P^a = \\left[\\begin{array}{cc}\n",
    "R^a_b & t^a_b\\\\\n",
    "0 & 1\n",
    "\\end{array}\\right] P^b = T^a_b P^b \n",
    "$$\n",
    "\n",
    "A good rule to keep in mind, illustrated above, is that the superscript $b$ of the point $P^b$ on the right-hand side is \"canceled\" by the subscript $b$ in the pose $T^a_b$, and so the left-hand side $P^a$ only retains the superscript $a$, indicating it now lives in the reference frame $a$. This rule is quite useful when implementing these types of transformations in code, as well. Also, keep in mind that we are talking about the *same* point $P$ in both cases: it is simply the *coordinates* that change by expressing them in a different frame.\n",
    "\n",
    "Note that this representation generalizes easily to three dimensions,\n",
    "but of course $t^a_b$ will be a three-vector, and $R^a_b$ will be a $3\\times3$\n",
    "rotation matrix representing the 3DOF orientation of the vehicle. The\n",
    "latter can be decomposed into roll, pitch, and yaw, if so desired, \n",
    "but we will not need that until the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpwKGbRylV-7"
   },
   "source": [
    "## The Iterative Closest Points Algorithm\n",
    "\n",
    "> ICP is a seminal method to align two point clouds.\n",
    "\n",
    "**Iterative closest points** or **ICP** is a method to align two point clouds, e.g., two successive LIDAR scans, and it is a crucial component in the PoseSLAM algorithm we will discuss next. Leaning into the notation re-introduced above, we use superscripts $a$ and $b$ to distinguish the two point clouds, and the points therein. Under the assumption that we have a good initial estimate $\\hat{T^a_b}$ for the relative pose $T^a_b$ between the two point clouds, ICP iterates between two steps:\n",
    "\n",
    "1. Find closest point correspondences between the two clouds.\n",
    "2. Use these point correspondences to re-estimate the relative pose $\\hat{T^a_b}$ between the two point clouds.\n",
    "\n",
    "These two steps are iterated until convergence, hence the name *iterated* closest points. Below we  explain both steps in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVRcg3fslV-7"
   },
   "source": [
    "### Finding Closest Points\n",
    "\n",
    "The first step is the easiest: for each point $P^a_j$ in the first point cloud, we define $P^b_j$ to be its closest point in the second point cloud. Stated formally, for each point $P^a_j$, we solve the following minimization problem:\n",
    "\n",
    "$$\n",
    "P^b_j = \\arg \\min_{P^b_k} \\| P^b_k - P^a_j\\|^2\n",
    "$$\n",
    "\n",
    "We prefer using indices $j$ for points as we use $i$ for poses below, making it easier to associate notation with geometric concepts. Two things about the above minimization are worth noting: \n",
    "(a) it can be solved using a linear search over all points $P^b$, \n",
    "and (b) rather than computing the distance we can save the computation of a square root by minimizing the square, which is just as good as they are monotonically related. \n",
    "\n",
    "The linear search problem above is known as the **nearest neighbor** problem, and solving this problem for all points is the **all nearest neighbors** problem.\n",
    "Iterating over all points in the second cloud can be quite slow, and indeed finding all nearest neighbors that way has quadratic complexity. However, very fast *approximate* nearest neighbor algorithms are available. Many of these use specialized data structures, such as \"KD-trees\" or \"Oct-trees\" (in 3D). While the details are beyond the scope of this book,\n",
    "intuitively these data structures  recursively divide up the point clouds into sub-clouds, such that sub-clouds unlikely to contain the nearest neighbor can be quickly excluded. We build this data structure once for the second cloud, and then use it for all nearest neighbor searches, leading to complexity which is approximately $O(N \\log N$),\n",
    "for point clouds of size $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTVEYMjdlV-8"
   },
   "source": [
    "### Estimating the Pairwise Transform\n",
    "\n",
    "The second step is the more interesting one: given a set of closest point pairs $\\{(P^a_j, P^b_j)\\}$, how can we estimate the relative pose $\\widehat{T^a_b}$ between the two point clouds? This is known as the **pose alignment** problem.\n",
    "\n",
    "Let us first assume that the two point clouds only differ by a rotation $R^a_b$. When this is the case, and assuming we have corresponding points $P^a$ and $P^b$, then each point $P^a$ in the first cloud can be expressed as a function of a point $P^b$ in the second cloud:\n",
    "\n",
    "$$\n",
    "P^a = R^a_b P^b\n",
    "$$\n",
    "\n",
    "One might be tempted to think that therefore \n",
    "\n",
    "$$\n",
    "R^a_b = P^a (P^b)^{-1}\n",
    "$$\n",
    "\n",
    "but, of course, there since $P^b$ is a vector, it does not have an inverse. So this would not work.\n",
    "Interestingly, though, if we form the matrix\n",
    "\n",
    "$$\n",
    "H = \\sum_j P^a_j (P^b_j)^T\n",
    "$$\n",
    "\n",
    "by summing over at least 3 point pairs $(P^a_j, P^b_j)$, it turns out that the rotation matrix $\\widehat{R^a_b}$ closest to $H$ in the least squares sense *is* the best possible estimate for the unknown rotation $R^a_b$. In addition, using the *singular value decomposition* from linear algebra, it is *very* easy to compute.\n",
    "The singular value decomposition of the matrix $H$ is defined by $H=U\\Lambda V^T$, in which $\\Lambda$ is a diagonal\n",
    "matrix of singular values, and both $U$ and $V$ are orthogonal matrices.\n",
    "The optimal estimate for the rotation matrix is given by\n",
    "\n",
    "$$\n",
    "\\widehat{R^a_b} = U V^T\n",
    "$$\n",
    "\n",
    "Interesting aside: this problem is known as the *orthogonal Procrustes problem* and its solution via SVD has been known since 1966, from [a paper by Peter Schönemann](https://doi.org/10.1007%2FBF02289451). Even more interesting is that Procrustes was a mythological greek villain who either stretched or cut his victims to size to fit their bed, analogous to how we make the rotation matrix above fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solves the problem when there is only rotation, but we must also consider relative translation between the two point clouds.\n",
    "Fortunately, it turns out that the best possible translation estimate will always align both *centroids* of the point clouds. Hence, when there is translation present, we simply compute the matrix $H$ from the *centered* points, \n",
    "\n",
    "$$\n",
    "H = \\sum_j (P^a_j-C^a) (P^b_j-C^b)^T\n",
    "$$\n",
    "\n",
    "where the point cloud centroids $C^a$ and $C^b$ are computed as \n",
    "\n",
    "$$\n",
    "C^a = \\frac{1}{N} \\sum_j P^a_j\\text{    and    }C^b = \\frac{1}{N} \\sum_j P^b_j.\n",
    "$$\n",
    "\n",
    "Given the estimated rotation $\\widehat{R^a_b}$, the translation estimate $\\widehat{t^a_b}$ can then be estimated from \n",
    "\n",
    "$$\n",
    "C^a = \\widehat{R^a_b} C^b + \\widehat{t^a_b},\n",
    "$$\n",
    "\n",
    "and the final relative pose estimate is given by $\\widehat{T^a_b} =(\\widehat{R^a_b}, \\widehat{t^a_b})$. By the way, all of the above math is identical for both the 2D and 3D case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc_L-6rElV-8"
   },
   "source": [
    "## PoseSLAM\n",
    "\n",
    "> PoseSLAM is SLAM with pose priors and relative pose constraints only. We can derive those from Iterative Closest Points (ICP).\n",
    "\n",
    "As we discussed at the beginning of this section, in SLAM the goal is to localize a robot using the information coming\n",
    "from the robot’s sensors, while simultaneously building the map.\n",
    "We have already covered the localization problem in chapter 4, \n",
    "using Markov localization, Monte Carlo localization, and the Kalman filter.\n",
    "The additional wrinkle in SLAM is that we do\n",
    "*not* know the map a priori, and hence we have to infer the unknown map\n",
    "while simultaneously estimating the robot's location with respect to the evolving map.\n",
    "\n",
    "**PoseSLAM** is a variant of SLAM that uses pose constraints (using an algorithm such as ICP) as a\n",
    "basic building block when optimizing over the unknown vehicle\n",
    "poses. \n",
    "In PoseSLAM we do *not* explicitly optimize over a map.\n",
    "Instead the map is reconstructed after the fact. \n",
    "This can only work if (a) the sensor is sufficiently rich to make the pose constraints very accurate, \n",
    "and (b) we can build a sufficiently good map using this sensor data. LIDAR satisfies both of these assumptions, \n",
    "making LIDAR-based PoseSLAM a very popular technique.\n",
    "\n",
    "In a nutshell, the PoseSLAM problem can be stated as:\n",
    "\n",
    "> Given a set of noisy relative measurements or **pose constraints**\n",
    "> $\\tilde{T}_{ij}$, recover the optimal set of poses $T_{i}^{*}$ that\n",
    "> maximizes the posteriori probability, i.e., recover the MAP solution.\n",
    "\n",
    "In the case of mapping for autonomous driving, these relative\n",
    "measurements can be derived by performing ICP between overlapping\n",
    "LIDAR scans. If available, we can additionally use GPS and/or IMU measurements \n",
    "to decide which scans overlap, so that we do not have to do this $O(n^{2})$ times. \n",
    "Depending on the situation, we can optimize for 3D or 2D poses, in the way we will\n",
    "discus below. Afterwards, we can reconstruct a detailed map by\n",
    "transforming the local LIDAR scans into the world frame, using the\n",
    "optimized poses $T_{i}^{*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iG14QP4lV-9"
   },
   "source": [
    "## The PoseSLAM Factor Graph\n",
    "\n",
    "> Factor graphs expose the sparse set of constraints tying absolute poses together.\n",
    "\n",
    "In our factor-graph-based view of the world, a pose constraint is\n",
    "represented as a factor. As before, the factor graph represent the\n",
    "posterior distribution over the unknown pose variables\n",
    "$\\mathcal{T}=\\{X_{1}\\dots X_{5}\\}$ given the known measurements:\n",
    "\n",
    "$$\n",
    "\\phi(\\mathcal{T})=\\prod_{i}\\phi_{i}(\\mathcal{T}_{i}),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{T}_{i}$ is the set of poses involved with factor $\\phi_i$. \n",
    "In this way, the factor graph encodes which factors are connected to which variables,\n",
    "exposing the sparsity pattern of the corresponding estimation problem.\n",
    "\n",
    "<figure id=\"fig:PoseSLAMFG\">\n",
    "<img src=\"https://github.com/gtbook/robotics/blob/main/Figures6/PoseSLAM-FG.png?raw=1\" style=\"width:60.0%\" /><figcaption>PoseSLAM factor graph example.</figcaption>\n",
    "</figure>\n",
    "\n",
    "An example is shown in Figure\n",
    "<a href=\"#fig:PoseSLAMFG\" data-reference-type=\"ref\" data-reference=\"fig:PoseSLAMFG\">1</a>.\n",
    "The example represents a vehicle driving around, and taking LIDAR scans\n",
    "at 5 different world poses, represented by $T_{1}$ to $T_{5}$.\n",
    "The factors $f_{1}$ to $f_{4}$ are binary factors representing the pose\n",
    "constraints obtained by matching successive LIDAR scans using ICP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJEBb4d6lV-9"
   },
   "source": [
    "The factor\n",
    "$f_{5}(T_{5},T_{2})$ is a so-called **loop closure** constraint:\n",
    "rather than derived from two successive scans, this one is derived from\n",
    "matching the scan taken at $T_{5}$ with the one at $T_{2}$.\n",
    "Detecting such loops can be done through a variety of means. The final,\n",
    "unary factor $f_{0}(T_{1})$ is there to “anchor” the solution to the\n",
    "origin: if it is not there, the solution will be undetermined. Another\n",
    "way to anchor the solution is to add unary factors at every time-step,\n",
    "derived from GPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0Jg1ajElV--"
   },
   "source": [
    "## MAP Inference via Least-Squares\n",
    "\n",
    "> Linear problems with zero-mean Gaussian noise lead to least-squares problems.\n",
    "\n",
    "When measurements are linear functions of continuous variables,\n",
    "finding the maximum a posteriori (MAP) solution can be done via\n",
    "least-squares optimization. \n",
    "Earlier in this book we have discussed MAP inference for discrete\n",
    "variables, and we have discussed probability distributions for\n",
    "continuous variables, but we have never put the two together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxUUPn_rlV--"
   },
   "source": [
    "In the case of linear measurements corrupted by zero-mean Gaussian noise, we can\n",
    "recover the MAP solution by minimizing a sum of squared differences. \n",
    "Recall that a univariate Gaussian density **with mean** $\\mu$ and **variance** $\\sigma^{2}$ is\n",
    "given by\n",
    "\n",
    "$$\n",
    "\\mathcal{N} (x; \\mu, \\sigma^2) =\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right\\} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqwl7FV2lV--"
   },
   "source": [
    "An example can help explain this more clearly.\n",
    "In particular, if we focus our attention in PoseSLAM on *just the x coordinates*, then we\n",
    "predict relative measurements $\\tilde{x}_{ij}$ by\n",
    "\n",
    "$$\n",
    "\\tilde{x}_{ij}\\approx h(x_{i,}x_{j})=x_{j}-x_{i}\n",
    "$$\n",
    " and each factor in\n",
    "Figure\n",
    "<a href=\"#fig:PoseSLAMFG\" data-reference-type=\"ref\" data-reference=\"fig:PoseSLAMFG\">1</a>\n",
    "could be written as\n",
    "\n",
    "$$\n",
    "\\phi(x_{i},x_{j})=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ -\\frac{1}{2}\\left(x_{j}-x_{i}-\\tilde{x}_{ij}\\right)^{2}\\right\\} ,\n",
    "$$\n",
    "\n",
    "where we assume $\\sigma=1$ for now. By taking the negative log,\n",
    "maximizing the posterior corresponds to minimizing the following sum of\n",
    "squares, where them sum ranges over all $(i,j)$ pairs for which we have a\n",
    "pairwise measurement:\n",
    "\n",
    "$$\n",
    "\\mathcal{X}^{*}=\\arg\\min_{\\mathcal{X}}\\sum_{k}\\frac{1}{2}\\left(h(x_{i},x_{j})-\\tilde{x}_{ij}\\right)^{2}=\\arg\\min_{\\mathcal{X}}\\sum_{k}\\frac{1}{2}\\left(x_{j}-x_{i}-\\tilde{x}_{ij}\\right)^{2}.\n",
    "$$\n",
    "\n",
    "Linear least squares problems like these are easily solved by numerical\n",
    "computing packages like MATLAB or numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUlUTv34lV-_"
   },
   "source": [
    "## PoseSLAM is Nonlinear !\n",
    "\n",
    "> Nonlinear problems need nonlinear solutions.\n",
    "\n",
    "Unfortunately, in the PoseSLAM case we cannot use linear least squares,\n",
    "because poses are not simply vectors, and the measurements are not\n",
    "simply linear functions of the poses. Indeed, in PoseSLAM both the\n",
    "prediction $h(T_{i},T_{j})$ and the measurement $\\tilde{T}_{ij}$\n",
    "are relative poses. The measurement prediction function $h(.)$ is given\n",
    "by \n",
    "$$\n",
    "h(T_{i},T_{j})=T_{i}^{-1}T_{j}\n",
    "$$\n",
    " and the\n",
    "measurement error to be minimized is\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\left\\Vert \\log\\left(\\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j}\\right)\\right\\Vert ^{2}\n",
    "$$\n",
    "\n",
    "where $\\log:SE(2)\\rightarrow\\mathbb{R}^3$ denotes a map from $SE(2)$ to a\n",
    "three-dimensional local coordinate vector $\\xi$, which will be defined\n",
    "in detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iVbbFVFlV-_"
   },
   "source": [
    "There are two ways out of the nonlinear quandary. The first is to\n",
    "realize that the only non-linearities stem from the $\\sin$ and $\\cos$\n",
    "terms in the poses, associated with the unknown orientations\n",
    "$\\theta_{i}$. Hence, one solution is to try and solve for the\n",
    "orientations first, and then solve for the translations using linear\n",
    "least squares, exactly as above. This approach is known as **rotation\n",
    "averaging** followed by linear translation recovery. Unfortunately it is\n",
    "sub-optimal as it does not consider the orientation and translation\n",
    "simultaneously. However, it can serve to provide a (very) good initial\n",
    "estimate for nonlinear optimization, discussed below.\n",
    "\n",
    "Indeed, we will prefer to take a second route, which is to use\n",
    "**nonlinear optimization**, as discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUtgFjvhlV_A"
   },
   "source": [
    "## Nonlinear Optimization for PoseSLAM\n",
    "\n",
    "> Linearize, solve, repeat...\n",
    "\n",
    "As discussed, the error expressions in PoseSLAM\n",
    "are *nonlinear*, and we cannot directly optimize over the poses\n",
    "$T_{i}$. Instead, we will locally linearize the problem and solve\n",
    "the corresponding linear problem using least-squares, and iterate this\n",
    "until convergence. We do this by, at each iteration, parameterizing a\n",
    "pose $T$ by \n",
    "\n",
    "$$\n",
    "T\\approx\\bar{T}\\Delta(\\xi)\n",
    "$$\n",
    "\n",
    "where $\\xi$ are local coordinates\n",
    "$\\xi\\doteq(\\delta x,\\delta y,\\delta\\theta)$ and the incremental pose\n",
    "$\\Delta(\\xi)\\in SE(2)$ is defined as\n",
    "\n",
    "$$\n",
    "\\Delta(\\xi)=\\left[\\begin{array}{cc|c}\n",
    "1 & -\\delta\\theta & \\delta x\\\\\n",
    "\\delta\\theta & 1 & \\delta y\\\\\n",
    "\\hline 0 & 0 & 1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "which you can recognize as a small angle\n",
    "approximation of the $SE(2)$ matrix.\n",
    "In 3D the local coordinates $\\xi$ are 6-dimensional, and the small angle\n",
    "approximation is defined as \n",
    "\n",
    "$$\n",
    "\\Delta(\\xi)=\\left[\\begin{array}{ccc|c}\n",
    "1 & -\\delta\\theta_{z} & \\delta\\theta_{y} & \\delta x\\\\\n",
    "\\delta\\theta_{z} & 1 & -\\delta\\theta_{x} & \\delta y\\\\\n",
    "-\\delta\\theta_{y} & \\delta\\theta_{x} & 1 & \\delta z\\\\\n",
    "\\hline 0 & 0 & 0 & 1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "With this new notation, we can approximate the\n",
    "nonlinear error by a linear approximation:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\left\\Vert \\log\\left(\\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j}\\right)\\right\\Vert ^{2}\\approx\\frac{1}{2}\\left\\Vert A_{i}\\xi_{i}+A_{j}\\xi_{j}-b\\right\\Vert ^{2}.\n",
    "$$\n",
    "\n",
    "For $SE(2)$ the matrices $A_{i}$ and $A_{j}$ are the $3\\times3$ **or\n",
    "Jacobian matrices** and $b$ is a $3\\times1$ bias term. The above\n",
    "provides a linear approximation of the term within the norm as a\n",
    "function of the incremental local coordinates $\\xi_{i}$ and $\\xi_{j}$.\n",
    "Deriving the detailed expressions for these Jacobians is beyond the\n",
    "scope of this document, but suffice to say that they exist and not too\n",
    "expensive to compute. In three dimensions, the Jacobian matrices are\n",
    "$6\\times6$ and $16\\times6$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "higUZ3jvlV_A"
   },
   "source": [
    "The final optimization will—in each iteration—minimize over the local\n",
    "coordinates of all poses by summing over all pose constraints. If we\n",
    "index those constraints by $k$, we have the following least squares\n",
    "problem:\n",
    "\n",
    "$$\n",
    "\\Xi^{*}=\\arg\\min_{\\Xi}\\sum_{k}\\frac{1}{2}\\Vert A_{ki}\\xi_{i}+A_{kj}\\xi_{j}-b_{k}\\Vert ^{2}\n",
    "$$\n",
    "\n",
    "where $\\Xi\\doteq \\{  \\xi_{i}\\}$, the set\n",
    "of all incremental pose coordinates.\n",
    "\n",
    "After solving for the incremental updates $\\Xi$, we update all poses\n",
    "using the update equation above and check for convergence. \n",
    "If the error does not decrease significantly\n",
    "we terminate, otherwise we linearize and solve again, until the error\n",
    "converges. While this is not guaranteed to converge to a global minimum,\n",
    "in practice it does so if there are enough relative measurements and a\n",
    "good initial estimate is available. For example, GPS can provide us with\n",
    "a good initial estimate. However, especially in urban environments GPS\n",
    "can be quite noisy, and it could happen that the map quality suffers by\n",
    "converging to a bad local minimum. Hence, a good quality control process\n",
    "is absolutely necessary in production environments.\n",
    "\n",
    "In summary, the algorithm for nonlinear optimization is\n",
    "\n",
    "-   Start with an initial estimate $\\mathcal{T}^{0}$\n",
    "\n",
    "-   Iterate:\n",
    "\n",
    "    1.  Linearize the factors\n",
    "$\\frac{1}{2}\\Vert \\log(\\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j})\\Vert ^{2}\\approx\\frac{1}{2}\\Vert A_{i}\\xi_{i}+A_{j}\\xi_{j}-b\\Vert ^{2}$\n",
    "\n",
    "    2.  Solve the least squares problem\n",
    "        $\\Xi^{*}=\\arg\\min_{\\Xi}\\sum_{k}\\frac{1}{2}\\Vert A_{ki}\\xi_{i}+A_{kj}\\xi_{j}-b_{k}\\Vert ^{2}$\n",
    "\n",
    "    3.  Update $X_{i}^{t+1}\\leftarrow X_{j}^{t}\\Delta(\\xi_{i})$\n",
    "\n",
    "-   Until the nonlinear error\n",
    "$J(\\mathcal{T})\\doteq\\sum_{k}\\frac{1}{2}\\Vert \\log(\\tilde{T}_{ij}^{-1}T_{i}^{-1}T_{j})\\Vert ^{2}$\n",
    "    converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6OG5ulXzTRe"
   },
   "source": [
    "## Optimization with GTSAM\n",
    "\n",
    "> GTSAM rocks PoseSLAM.\n",
    "\n",
    "For SLAM we typically use a specialized packages such as GTSAM that \n",
    "exploit the sparsity of the factor graphs to dramatically\n",
    "speed up computation. \n",
    "GTSAM exploits sparsity to be computationally efficient. Typically\n",
    "measurements only provide information on the relationship between a\n",
    "handful of variables, and hence the resulting factor graph will be\n",
    "sparsely connected. This is exploited by the algorithms implemented in\n",
    "GTSAM to reduce computational complexity. Even when graphs are too dense\n",
    "to be handled efficiently by direct methods, GTSAM provides iterative\n",
    "methods that are quite efficient regardless.\n",
    "\n",
    "The following code, included in GTSAM as an example, creates the\n",
    "factor graph from Figure\n",
    "<a href=\"#fig:PoseSLAMFG\" data-reference-type=\"ref\" data-reference=\"fig:PoseSLAMFG\">1</a>\n",
    "in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6gJFN6lzM3k"
   },
   "outputs": [],
   "source": [
    "graph = gtsam.NonlinearFactorGraph()\n",
    "prior_model = gtsam.noiseModel.Diagonal.Sigmas((0.3, 0.3, 0.1))\n",
    "graph.add(gtsam.PriorFactorPose2(1, gtsam.Pose2(0, 0, 0), prior_model))\n",
    "\n",
    "# Create odometry (Between) factors between consecutive poses\n",
    "odometry_model = gtsam.noiseModel.Diagonal.Sigmas((0.2, 0.2, 0.1))\n",
    "Between = gtsam.BetweenFactorPose2\n",
    "graph.add(Between(1, 2, gtsam.Pose2(2, 0, 0), odometry_model))\n",
    "graph.add(Between(2, 3, gtsam.Pose2(2, 0, np.pi/2), odometry_model))\n",
    "graph.add(Between(3, 4, gtsam.Pose2(2, 0, np.pi/2), odometry_model))\n",
    "graph.add(Between(4, 5, gtsam.Pose2(2, 0, np.pi/2), odometry_model))\n",
    "\n",
    "# Add the loop closure constraint\n",
    "graph.add(Between(5, 2, gtsam.Pose2(2, 0, np.pi/2), odometry_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Tt0j9DlV_B"
   },
   "source": [
    "Lines 1-4 create a nonlinear factor graph and add the unary factor\n",
    "$f_{0}(T_{1})$. As the vehicle travels through the world, it creates\n",
    "binary factors $f_{t}(T_{t},T_{t+1})$ corresponding to odometry,\n",
    "added to the graph in lines 6-12 (Note that M\\_PI\\_2 refers to pi/2).\n",
    "But line 15 models a different event: a **loop closure**. For example,\n",
    "the vehicle might recognize the same location using vision or a laser\n",
    "range finder, and calculate the geometric pose constraint to when it\n",
    "first visited this location. This is illustrated for poses $T_{5}$\n",
    "and $T_{2}$, and generates the (red) loop closing factor\n",
    "$f_{5}(T_{5},T_{2})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tN-kF1BwpXS"
   },
   "source": [
    "Before we can optimize, we need to create an initial estimate to start from. In GTSAM, this is done via the `gtsam.Values` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxyN7AKH0T-7",
    "outputId": "7bd94eee-b3e5-4304-9efe-acd1fdd4e315"
   },
   "outputs": [],
   "source": [
    "# Create the initial estimate\n",
    "initial_estimate = gtsam.Values()\n",
    "initial_estimate.insert(1, gtsam.Pose2(0.5, 0.0, 0.2))\n",
    "initial_estimate.insert(2, gtsam.Pose2(2.3, 0.1, -0.2))\n",
    "initial_estimate.insert(3, gtsam.Pose2(4.1, 0.1, np.pi/2))\n",
    "initial_estimate.insert(4, gtsam.Pose2(4.0, 2.0, np.pi))\n",
    "initial_estimate.insert(5, gtsam.Pose2(2.1, 2.1, -np.pi/2))\n",
    "print(initial_estimate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WNeRXHurTzX"
   },
   "source": [
    "We can use this initial estimate to show the factor graph below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "qkkjavZTl6nT",
    "outputId": "7ddf8253-e311-480c-c473-76c06c969a62"
   },
   "outputs": [],
   "source": [
    "show(graph, initial_estimate, binary_edges=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNip0x6GniL_"
   },
   "source": [
    "Optimization is done using non-linear minimization, as explained above. In GTSAM, this is done via a `NonlinearOptimizer` class. The specific optimizer we use below is `GaussNewtonOptimizer`, which exactly implements the pseudo-code given above, but exploiting sparsity in the factor graph to do this very efficiently. The optimizer only needs a graph and an initial estimate, both of which we already created, and hence the code below is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaXSwH2fm1Zc",
    "outputId": "17874473-7408-482a-babb-0720c25e1888"
   },
   "outputs": [],
   "source": [
    "# Optimize the initial values using a Gauss-Newton nonlinear optimizer\n",
    "optimizer = gtsam.GaussNewtonOptimizer(graph, initial_estimate)\n",
    "result = optimizer.optimize()\n",
    "print(\"Final Result:\\n{}\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQ2j-vBvoYEo"
   },
   "source": [
    "We can also inspect the result graphically. Looking at the result as printed above only gets us so far, and more importantly: it only shows us the maximum a posteriori (MAP) solution, but not the uncertainty around it. Luckily, GTSAM can also compute the **posterior marginals**, which show the uncertainty on each recovered pose as a Gaussian density $P(T_i|Z)$, taking into account all the measurements $Z$.\n",
    "\n",
    "In code, we do this via the `gtsam.Marginals` object, and we can plot marginals with a special function `plot_pose2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "TUJSyFCFmOLf",
    "outputId": "3ea084d8-829a-4104-a4a7-38bbc7c9ce09"
   },
   "outputs": [],
   "source": [
    "marginals = gtsam.Marginals(graph, result)\n",
    "for i in range(1, 6):\n",
    "    gtsam_plot.plot_pose2(0, result.atPose2(i), 0.5,\n",
    "                          marginals.marginalCovariance(i))\n",
    "plt.axis('equal'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oyymz-A0e4D"
   },
   "source": [
    "The result is shown graphically in Figure\n",
    "<a href=\"#fig:example\" data-reference-type=\"ref\" data-reference=\"fig:example\">2</a>,\n",
    "along with covariance ellipses shown in green. These covariance ellipses\n",
    "in 2D indicate the marginal over position, *over all possible\n",
    "orientations*, and show the area which contain 99% of the probability\n",
    "mass (in 1D this would correspond to one standard deviation). The graph\n",
    "shows in a clear manner that the uncertainty on pose $T_{5}$ is now\n",
    "much less than if there would be only odometry measurements. The pose\n",
    "with the highest uncertainty, $T_{4}$, is the one furthest away from\n",
    "the unary constraint $f_{0}(T_{1})$, which is the only factor tying\n",
    "the graph to a global coordinate frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb32b85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SLAM with Landmarks\n",
    "\n",
    "> Take PoseSLAM, add landmarks.\n",
    "\n",
    "So far we optimized over one type of variable, but often we build a landmark map *simultaneously* with the trajectory, i.e., this is *true* SLAM. In the next chapter, we will more thoroughly examine the full 3d case, whereas here we will model landmarks with 2D points in the plane. That does not mean that they cannot be standings for real 3-D entities in the environment: they can be the location of trees, poles, building corners, the sides of windows, the location of a stop sign in traffic, even moving pedestrians in more advanced SLAM/tracking applications.\n",
    "\n",
    "How do we measure such landmarks? The most typical *type* of measurements are either **range** measurements, **bearing** measurements, or **bearing-range** measurements of coverage combine the former two. The details on how to obtain them are typically application-dependent, and below we will abstract away the sensor pre-processing details. For example, in the case of a LIDAR sensors, bearing range measurements can be obtained by pre-processing the every LIDAR scan, detecting prominent vertical structures for example. A real-life example that we will discuss below involves detecting and measuring the bearing/range to trees. Another often-used sensors in autonomous driving includes radar, which can often be modeled or idealized to give bearing range measurements as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d82d22",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To illustrate SLAM with landmarks, we build a small toy example with 3 bearing-range measurements to two different landmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f12785",
   "metadata": {},
   "outputs": [],
   "source": [
    "slam_graph = gtsam.NonlinearFactorGraph()\n",
    "slam_graph.add( gtsam.PriorFactorPose2(1, gtsam.Pose2(0.0, 0.0, 0.0), prior_model))\n",
    "slam_graph.add(Between(1, 2, gtsam.Pose2(2.0, 0.0, 0.0), odometry_model))\n",
    "slam_graph.add(Between(2, 3, gtsam.Pose2(2.0, 0.0, 0.0), odometry_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6943c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Range-Bearing measurements to two different landmarks L1 and L2\n",
    "measurement_model = gtsam.noiseModel.Diagonal.Sigmas(np.array([0.1, 0.2]))\n",
    "BR = gtsam.BearingRangeFactor2D\n",
    "l = {k:gtsam.symbol('l',k) for k in [1,2]} # name landmark variables\n",
    "slam_graph.add(BR(1, l[1], gtsam.Rot2.fromDegrees(45),np.sqrt(4.0 + 4.0), measurement_model)) # pose 1 -*- landmark 1\n",
    "slam_graph.add(BR(2, l[1], gtsam.Rot2.fromDegrees(90), 2.0,measurement_model)) # pose 2 -*- landmark 1\n",
    "slam_graph.add(BR(3, l[2], gtsam.Rot2.fromDegrees(90), 2.0,measurement_model)) # pose 3 -*- landmark 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858637d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we have an initial estimate, we can look at the structure of this factor graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f365542",
   "metadata": {},
   "outputs": [],
   "source": [
    "slam_initial = gtsam.Values()\n",
    "slam_initial.insert(1, gtsam.Pose2(-0.25, 0.20, 0.15))\n",
    "slam_initial.insert(2, gtsam.Pose2(2.30, 0.10, -0.20))\n",
    "slam_initial.insert(3, gtsam.Pose2(4.10, 0.10, 0.10))\n",
    "slam_initial.insert(l[1], gtsam.Point2(1.80, 2.10))\n",
    "slam_initial.insert(l[2], gtsam.Point2(4.10, 1.80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10793f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(slam_graph, slam_initial, binary_edges=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfeb0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We optimize again with LM, and show the marginals on both robot position and landmarrs, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16128fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = gtsam.LevenbergMarquardtOptimizer(slam_graph, slam_initial)\n",
    "slam_result = optimizer.optimize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginals = gtsam.Marginals(slam_graph, slam_result)\n",
    "for k in [1,2,3]:\n",
    "    gtsam_plot.plot_pose2(0, slam_result.atPose2(k), 0.5, marginals.marginalCovariance(k))\n",
    "for j in [1,2]:\n",
    "    gtsam_plot.plot_point2(0, slam_result.atPoint2(l[j]), 'g', P=marginals.marginalCovariance(l[j]))\n",
    "plt.axis('equal'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651f559",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Larger SLAM Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baffa99",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Below we optimize a piece of the (old) [Victoria park dataset](http://www-personal.acfr.usyd.edu.au/nebot/victoria_park.htm), which involves a truck driving through a park in Sydney, extracting the position of trees in the park from LIDAR scans, just as we discussed above. The factor graph for this example is created from file and shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f76695",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = gtsam.findExampleDataFile('example.graph')\n",
    "model = gtsam.noiseModel.Diagonal.Sigmas([0.05,0.05,2*math.pi/180])\n",
    "[graph,initial] = gtsam.load2D(datafile, model)\n",
    "show(graph,initial, binary_edges=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8246d94a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a much larger factor graph than any we have encountered before, and we can distinguish several features:\n",
    "\n",
    "- There is a prominent backbone of truck poses, connected via odometry measurements.\n",
    "- There are about 20 landmarks, some of which are seen briefly, others are seen for longer periods of time.\n",
    "- The graph is very sparsely connected, and hence optimization will still be quite fast.\n",
    "\n",
    "Optimizing with `gtsam.LevenbergMarquardtOptimizer`, again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90abaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_poses = gtsam.utilities.extractPose2(initial)\n",
    "for i in range(initial_poses.shape[0]):\n",
    "    initial.update(i, initial.atPose2(i).retract(np.random.normal(scale=0.5,size=(3,))))\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial)\n",
    "result = optimizer.optimize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the plot both the initial estimate, which was created by adding random noise on top of the ground truth, and the optimized trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_poses = gtsam.utilities.extractPose2(initial)\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=initial_poses[:,0], y=initial_poses[:,1], name=\"initial\", marker=dict(color='orange'))\n",
    "final_poses = gtsam.utilities.extractPose2(result)\n",
    "fig.add_scatter(x=final_poses[:,0], y=final_poses[:,1], name=\"optimized\", marker=dict(color='green'))\n",
    "fig.update_yaxes(scaleanchor = \"x\",scaleratio = 1); fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTSAM 101\n",
    "\n",
    "> A deeper dive in the GTSAM concepts used above.\n",
    "\n",
    "\n",
    "help(gtsam.Point3)\n",
    "\n",
    "```Point3(x=nan, y=nan, z=nan)\n",
    "    Shim for the deleted Point3 type.\n",
    "```\n",
    "help(gtsam.Rot3.Ypr)\n",
    "\n",
    "```Ypr(...) method of builtins.PyCapsule instance\n",
    "    Ypr(y: float, p: float, r: float) -> gtsam.gtsam.Rot3\n",
    "```\n",
    "\n",
    "help(gtsam.Pose3)\n",
    "\n",
    "```Align(...) from builtins.PyCapsule\n",
    " |      Align(*args, **kwargs)\n",
    " |      Overloaded function.\n",
    " |      \n",
    " |      1. Align(abPointPairs: std::vector<std::pair<Eigen::Matrix<double, 3, 1, 0, 3, 1>, Eigen::Matrix<double, 3, 1, 0, 3, 1> >, std::allocator<std::pair<Eigen::Matrix<double, 3, 1, 0, 3, 1>, Eigen::Matrix<double, 3, 1, 0, 3, 1> > > >) -> Optional[gtsam.gtsam.Pose3]\n",
    " |      \n",
    " |      2. Align(a: numpy.ndarray[numpy.float64[m, n]], b: numpy.ndarray[numpy.float64[m, n]]) -> Optional[gtsam.gtsam.Pose3]\n",
    " |  \n",
    " |  Expmap(...) from builtins.PyCapsule\n",
    " |      Expmap(v: numpy.ndarray[numpy.float64[m, 1]]) -> gtsam.gtsam.Pose3\n",
    " |  \n",
    " |  ExpmapDerivative(...) from builtins.PyCapsule\n",
    " |      ExpmapDerivative(xi: numpy.ndarray[numpy.float64[m, 1]]) -> numpy.ndarray[numpy.float64[6, 6]]\n",
    " |  \n",
    " |  Logmap(...) from builtins.PyCapsule\n",
    " |      Logmap(pose: gtsam.gtsam.Pose3) -> numpy.ndarray[numpy.float64[6, 1]]\n",
    " |  \n",
    " |  LogmapDerivative(...) from builtins.PyCapsule\n",
    " |      LogmapDerivative(xi: gtsam.gtsam.Pose3) -> numpy.ndarray[numpy.float64[6, 6]]\n",
    " |  \n",
    " |  adjoint(...) from builtins.PyCapsule\n",
    " |      adjoint(xi: numpy.ndarray[numpy.float64[m, 1]], y: numpy.ndarray[numpy.float64[m, 1]]) -> numpy.ndarray[numpy.float64[6, 1]]\n",
    " |  \n",
    " |  adjointMap(...) from builtins.PyCapsule\n",
    " |      adjointMap(xi: numpy.ndarray[numpy.float64[m, 1]]) -> numpy.ndarray[numpy.float64[6, 6]]\n",
    " |  \n",
    " |  adjointMap_(...) from builtins.PyCapsule\n",
    " |      adjointMap_(xi: numpy.ndarray[numpy.float64[m, 1]]) -> numpy.ndarray[numpy.float64[6, 6]]\n",
    " |  \n",
    " |  adjointTranspose(...) from builtins.PyCapsule\n",
    " |      adjointTranspose(xi: numpy.ndarray[numpy.float64[m, 1]], y: numpy.ndarray[numpy.float64[m, 1]]) -> numpy.ndarray[numpy.float64[6, 1]]\n",
    " |  \n",
    " |  adjoint_(...) from builtins.PyCapsule\n",
    " |      adjoint_(xi: numpy.ndarray[numpy.float64[m, 1]], y: numpy.ndarray[numpy.float64[m, 1]]) -> numpy.ndarray[numpy.float64[6, 1]]\n",
    " |  \n",
    " |  identity(...) from builtins.PyCapsule\n",
    " |      identity() -> gtsam.gtsam.Pose3\n",
    " |  \n",
    " |  wedge(...) from builtins.PyCapsule\n",
    " |      wedge(wx: float, wy: float, wz: float, vx: float, vy: float, vz: float) -> numpy.ndarray[numpy.float64[m, n]]\n",
    " |  \n",
    " |  ----------------------------------------------------------------------\n",
    " |  Static methods inherited from pybind11_builtins.pybind11_object:\n",
    " |  \n",
    " |  __new__(*args, **kwargs) from pybind11_builtins.pybind11_type\n",
    " |      Create and return a new object.  See help(type) for accurate signature.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(driving.read_lidar_points)\n",
    "\n",
    "help(driving.visualize_clouds)\n",
    "help(driving.visualize_clouds_animation)\n",
    "\n",
    "help(gtsam.Values.deserialize)\n",
    "\n",
    "help(gtsam.Pose3.transformFrom)\n",
    "\n",
    "help(gtsam.noiseModel.Diagonal)\n",
    "\n",
    "help(gtsam.NonlinearFactorGraph())\n",
    "\n",
    "help(gtsam.PriorFactorPose3)\n",
    "help(gtsam.BetweenFactorPose3)\n",
    "\n",
    "help(gtsam.GaussNewtonParams)\n",
    "\n",
    "help( gtsam.GaussNewtonOptimizer)\n",
    "\n",
    " help(gtsam.Marginals)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S64_driving_perception.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gtbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f7376ced4243bb13dfcffa8a3ba834e0602aa8334cd3a1d8ba8d285f4628083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
