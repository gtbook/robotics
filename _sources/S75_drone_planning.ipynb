{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "tags": [
     "no-tex"
    ]
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S75_drone_decision_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q -U gtbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.subplots as pls\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import gtsam\n",
    "from gtsam import Point2\n",
    "\n",
    "from gtbook.drone import axes\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# try:\n",
    "#     import google.colab\n",
    "# except:\n",
    "#     import plotly.io as pio\n",
    "#     pio.renderers.default = \"png\"\n",
    "\n",
    "rng = np.random.default_rng()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_random_map(W, H, num_obstacles=50, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a random occupancy map with the given dimensions and number of obstacles.\n",
    "    Optionally, a seed can be provided to make the map reproducible.\n",
    "    \"\"\"\n",
    "    # Set the seed for reproducibility:\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Map dimensions in pixels (10 cm per pixel)\n",
    "    pixel_W, pixel_H = 10*W, 10*H\n",
    "\n",
    "    # Initialize an empty occupancy map\n",
    "    obstacles = torch.zeros((pixel_H, pixel_W), dtype=torch.float32)\n",
    "\n",
    "    # Add obstacles\n",
    "    for _ in range(num_obstacles):\n",
    "        # Random position for the obstacle\n",
    "        x, y = rng.integers(0, pixel_W), rng.integers(0, pixel_H)\n",
    "\n",
    "        # Random value for the obstacle (higher than empty space)\n",
    "        obstacle_value = rng.random() * 0.5 + 0.5  # Random value between 0.5 and 1.0\n",
    "\n",
    "        # Assign the value to the obstacle position\n",
    "        obstacles[y, x] = obstacle_value\n",
    "\n",
    "    # Define a box filter (kernel)\n",
    "    kernel_size = 5  # The size of the box filter\n",
    "    kernel = torch.ones((kernel_size, kernel_size), dtype=torch.float32)\n",
    "\n",
    "    # Apply 2D convolution to \"fatten\" the objects\n",
    "    batch = obstacles[None, None, ...]  # Add batch and channel dimensions\n",
    "    return torch.conv2d(batch, kernel[None, None, ...], padding='same')[0, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def displaced_gaussian(\n",
    "    sigma: float, K: int, uv: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns a Gaussian kernel of size K x K with standard deviation sigma.\n",
    "    The kernel is centered at uv, a float coordinate in xy convention.\n",
    "    Also returns the indices of the kernel in the image.\n",
    "    \"\"\"\n",
    "    # Create meshgrid around uv, in ij convention\n",
    "    j0, i0 = np.round(uv).astype(int)\n",
    "    kk = np.arange(-(K // 2), K // 2 + 1)\n",
    "    ii, jj = np.meshgrid(i0 + kk, j0 + kk, indexing=\"ij\")\n",
    "\n",
    "    # Calculate the kernel\n",
    "    dd = (ii - uv[1]) ** 2 + (jj - uv[0]) ** 2\n",
    "    kernel = np.exp(-dd / (2 * sigma**2))\n",
    "    kernel = kernel / kernel.sum()\n",
    "\n",
    "    # Return the kernel and the indices of the kernel in the image\n",
    "    return kernel, ii, jj\n",
    "\n",
    "\n",
    "def gaussian_kernel(sigma: float, K: int = 9) -> torch.Tensor:\n",
    "    \"\"\"Generates a Gaussian kernel of shape (1, 1, K, K) with standard deviation sigma.\"\"\"\n",
    "    uv = np.array([K // 2, K // 2])\n",
    "    numpy_kernel, _, _ = displaced_gaussian(sigma, K, uv)\n",
    "    return torch.tensor(numpy_kernel, dtype=torch.float32)[None, None, ...]\n",
    "\n",
    "\n",
    "def gaussian_filter(sigma, uv, image, K=9):\n",
    "    \"\"\"Applies a Gaussian filter at uv on the given image.\"\"\"\n",
    "    kernel, ii, jj = displaced_gaussian(sigma, K, uv)\n",
    "    \n",
    "    # Dimensions of the image\n",
    "    height, width = image.shape[-2:]\n",
    "\n",
    "    # Find the valid range for ii and jj\n",
    "    valid_mask = (ii >= 0) & (ii < height) & (jj >= 0) & (jj < width)\n",
    "    valid_ii = ii[valid_mask]\n",
    "    valid_jj = jj[valid_mask]\n",
    "\n",
    "    # Adjust the kernel to match the valid range\n",
    "    valid_kernel = kernel[valid_mask]\n",
    "\n",
    "    # Extract the relevant portion of the image\n",
    "    image_excerpt = image[..., valid_ii, valid_jj].numpy()\n",
    "\n",
    "    # Apply the valid kernel to the image excerpt\n",
    "    return (valid_kernel * image_excerpt).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sobel_kernels(dtype=torch.float32, **kwargs):\n",
    "    \"\"\"Return Sobel gradient kernels sobel_u of shape (1, 1, 1, 3) and sobel_v of shape (1, 1, 3, 1).\"\"\"\n",
    "    sobel_u = torch.tensor([[-1, 0, 1]], dtype=dtype, **kwargs)\n",
    "    sobel_v = sobel_u.T\n",
    "    return sobel_u[None, None, ...], sobel_v[None, None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "ChebyshevFactor = gtsam.VectorEvaluationFactorChebyshev2D3\n",
    "constrained3 = gtsam.noiseModel.Constrained.All(3)\n",
    "\n",
    "class SmoothTrajectory:\n",
    "    def __init__(self, trajectory: np.array, N:int=5, a:float=-1, b:float=1, sample:bool = False, boundaries:bool = False):\n",
    "        \"\"\"\n",
    "        Construct a polynomial fit of degree N-1, e.g. N=4 is a cubic.\n",
    "        Args:\n",
    "            trajectory: An array of shape (K, 3) representing a 3D trajectory.\n",
    "            N: The number of control points.\n",
    "            a: The lower bound of the interval.\n",
    "            b: The upper bound of the interval.\n",
    "            sample: If True, sample the trajectory at N Chebyshev nodes of the second kind.\n",
    "            boundaries: If True, constrain the start and end points to be constant velocity.\n",
    "        \"\"\"\n",
    "        self.N, self.a, self.b = N, a, b\n",
    "        K = len(trajectory)\n",
    "        assert trajectory.shape[1] == 3, \"Trajectory must be 3D\"\n",
    "\n",
    "        if sample:\n",
    "            # Calculate N-2 Chebyshev nodes of the second kind in the [0,K-1] interval,\n",
    "            # and interpolate points at these Chebyshev nodes:\n",
    "            nodes = [gtsam.Chebyshev2.Point(N, i, 0, K-1) for i in range(1, N - 1)]\n",
    "            middle = [trajectory[int(t)] * (int(t) + 1 - t) + trajectory[int(t) + 1] * (t - int(t)) for t in nodes]\n",
    "\n",
    "            # Combine start point, middle points, and end point:\n",
    "            self.points = np.stack([trajectory[0], *middle, trajectory[-1]], axis=0)\n",
    "        else:\n",
    "            # Fit using factor graph and adjusted weighting:\n",
    "            graph = gtsam.NonlinearFactorGraph()\n",
    "            t = np.linspace(a, b, K)\n",
    "            x = np.linspace(-1.0, 1.0, K)\n",
    "            key = 77\n",
    "            for k, xyz in enumerate(trajectory):\n",
    "                t_k = t[k]\n",
    "                if k == 0 or k == K - 1:\n",
    "                    model = constrained3 # constrained at end points\n",
    "                else:\n",
    "                    precision = 1.0 / (1 - x[k]*x[k]) # high at ends, low in middle\n",
    "                    model = gtsam.noiseModel.Isotropic.Precision(3, precision)\n",
    "                graph.add(ChebyshevFactor(key, xyz, model, N, t_k, a, b))\n",
    "            if boundaries:\n",
    "                # Constrain start and end points to be constant velocity:\n",
    "                dt = t[1] - t[0]\n",
    "                v = (trajectory[1] - trajectory[0])/dt\n",
    "                graph.add(ChebyshevFactor(key, trajectory[0] - v*dt, constrained3, N, t[0]-dt, a, b))\n",
    "                graph.add(ChebyshevFactor(key, trajectory[0] - v*2*dt, constrained3, N, t[0]-2*dt, a, b))\n",
    "                v = (trajectory[-1] - trajectory[-2])/dt\n",
    "                graph.add(ChebyshevFactor(key, trajectory[-1] + v*dt, constrained3, N, t[-1]+dt, a, b))\n",
    "                graph.add(ChebyshevFactor(key, trajectory[-1] + v*2*dt, constrained3, N, t[-1]+2*dt, a, b))\n",
    "            initial = gtsam.Values()\n",
    "            initial.insert(key, gtsam.ParameterMatrix3(N))\n",
    "            optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial)\n",
    "            result = optimizer.optimize()\n",
    "            self.points = result.atParameterMatrix3(key).matrix().T\n",
    "\n",
    "        # Cache differentiation matrices for velocities and accelerations:\n",
    "        self.D = gtsam.Chebyshev2.DifferentiationMatrix(self.N, self.a, self.b)\n",
    "        self.DD = self.D @ self.D\n",
    "\n",
    "    def evaluate(self, t: np.ndarray):\n",
    "        \"\"\"Evaluate the polynomial at t, which should a 1D array.\"\"\"\n",
    "        weights = gtsam.Chebyshev2.WeightMatrix(self.N, t, self.a, self.b)\n",
    "        return weights @ self.points\n",
    "\n",
    "    def velocities(self, t: np.ndarray):\n",
    "        \"\"\"Evaluate the polynomial at t, which should a 1D array.\"\"\"\n",
    "        derivatives = self.D @ self.points\n",
    "        weights = gtsam.Chebyshev2.WeightMatrix(self.N, t, self.a, self.b)\n",
    "        return weights @ derivatives\n",
    "\n",
    "    def accelerations(self, t: np.ndarray):\n",
    "        \"\"\"Evaluate the polynomial at t, which should a 1D array.\"\"\"\n",
    "        second_derivatives = self.DD @ self.points\n",
    "        weights = gtsam.Chebyshev2.WeightMatrix(self.N, t, self.a, self.b)\n",
    "        return weights @ second_derivatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Trajectory Optimization\n",
    "\n",
    "> We can use factor graphs to optimize over future trajectories, as well.\n",
    "\n",
    "<img src=\"Figures7/S75-Autonomous_camera_drone-05.jpg\"  alt=\"Splash image with steampunk drone in aggressive maneuver\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw how do use factor graphs for visual SLAM and structure from motion. These perception algorithms are typically run after we have gathered some visual information, and inform us about what happened in the past. But how can we plan for the future? We already saw that RRTs are useful paradigm to plan in a continuous, potentially high dimensional state space. \n",
    "\n",
    "In this section we will discuss a different method, which is optimal control. In particular, we will use **trajectory optimization**, to minimize the cost associated with the trajectory that we would like to execute in the future. These costs can be associated with staying away from obstacles, or other desirable properties like minimizing power consumption to preserve battery life. Finally, RRTs and trajectory optimization can be combined, where RRT finds a good \"broad strokes\" trajectory, and trajectory optimization smooths out and fine-tunes the final trajectory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Objective\n",
    "\n",
    "We explain trajectory optimization using a very simple problem: how to get from point A to point B. This seems like an easy problem, as we all know the answer should be a straight line, right? But, it is not as easy for drones, as we have to account for the drone's attitude, and we might have to accelerate and decelerate at the start and end points. Come to think of it, maybe we are already flying at a certain velocity at point A, and want to have a certain velocity at point B! Clearly, this is not as easy as it appears.\n",
    "\n",
    "There are many ways to go about this:\n",
    "- MPC with collocated polynomials\n",
    "- discrete poses and controls + dynamics factors\n",
    "- differential flatness + controller: this is what we'll do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing for Position\n",
    "\n",
    "It turns out we can optimize for position only, without worrying about attitude, at least at first. This is not a trivial statement, and it is also very fortunate. Let us assume that we want to fly in a forest, full of trees, and we want a trajectory for the drone avoid the trees. A quadrotor-style drone trajectory has the potential to be rather complicated: the drone lives in the 6DOF space of position (3DOF) and attitude (3DOF). And we know from Section 7.2 that attitude will *affect* position: if we pitch or role, that will affect our velocity, and that will integrate into position. That very mechanism is also a source of complexity: a drone is *under-actuated*: we cannot directly control the 6DOF, we have to do it through the action only four rotors. It seems like a very complex problem indeed.\n",
    "\n",
    "When given a continuous trajectory of position that also behaves well in terms of velocity, we can obtain the pitch, roll, and thrust trajectories to obtain it. Of course, it can't be a totally crazy trajectory: a drone can't stop on a dime, or teleport, or go faster than certain limits. But if we have a 3DOF translational trajectory that is well behaved, we can \"upgrade\" it to a full position/attitude in two different ways: in advance, by solving a large optimization problem, or physically, in real-time, by using a tracking controller. The latter is what we will do, after obtaining a trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a suitable translational trajectory that avoids obstacles, we will use optimization. There are many ways one can do this, some better than others, but here strive for simplicity: we discretize the time interval in which we expect to fly and solve for a set of corresponding set of positions over time that satisfy our objectives. These objectives can include:\n",
    "\n",
    "- a desired starting position (and velocity, if so desired);\n",
    "- a desired goal position (and velocity, if desired);\n",
    "- a desired distance from the closest obstacle at any time;\n",
    "- a smoothness objective that bounds our velocity and/or acceleration.\n",
    "\n",
    "If these objectives are in conflict, we can assign them weights such that we trade off some against others. Constrained optimization techniques allow to set hard constraints and/or bounds, but we do not use those here. Instead, we opt for a simple non-linear least squares scheme that minimizes the following objective:\n",
    "\n",
    "$$\n",
    "X^* = \\arg \\min \\sum_{k=1}^K \\phi_k(X_k)^2  + \\sum_{k=1}^{K-1} \\psi_k(X_k, X_{k+1})^2\n",
    "$$\n",
    "\n",
    "where $X_k$ is the position at time, and $\\phi_k(X_k)$ and $\\psi_k(X_k, X_{k+1})$ are unary and binary objectives, respectively, that we want to minimize. Desired start and goal, as well as obstacle avoidance are examples of unary objectives, while smoothness is a binary objective, as it evaluates the distance/velocity between successive positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will not come as any surprise that we can also use factor graph optimization to find our optimal trajectories. The objective above is exactly the expression of a nonlinear factor graph, except that the factors now derive from objectives we want to minimize, rather than measurement errors. In the next section we work in detail how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy and Cost Maps\n",
    "\n",
    "A very popular approach to represent environments with different obstacles is to use an **occupancy map**. An occupancy map is essentially a 2D array of cells, each representing the environment at a certain resolution (for example, 10 cm by 10 cm). Each cell in this map contains a probability for the presence of an obstacle. This concept is further extendable to distinguish between various types of obstacles, each with different levels of lethality. This extended version can then be transformed into a **cost map**, where areas with less hazardous obstacles (like tall grass) are considered less costly compared to more dangerous ones (like flying into a tree). For our purposes, we will start directly with cost maps instead of creating true occupancy maps.\n",
    "\n",
    "To illustrate, let us consider creating a simple example cost map. In the `gtbook` library, we have a function named `create_random_map`. This function requires as parameters the width $W$, height $H$, and the number of obstacles to create a random arrangement of obstacles within the map. Width and height are given in meters, and the resolution is hardcoded at 10 cm. An optional parameter is a random seed, which allows for the reproduction of the exact experiment setup. The following code snippet demonstrates how to generate a random map with 50 obstacles using a random seed of 42:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random cost map\n",
    "W, H = 30, 10  # 30m x 10m\n",
    "cost_map = create_random_map(W, H, num_obstacles=50, seed=7)\n",
    "px.imshow(cost_map, color_continuous_scale='Reds').show()\n",
    "#| caption: Cost map with randomly generated obstacles, at 10cm resolution.\n",
    "#| label: fig:obstacles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create differentiable factors $\\phi_k(X_k)$ that avoid high cost areas, we can smooth the cost map with a differentiable kernel, like a Gaussian. The cost map above is a torch tensor, representing a grayscale image 100 pixels tall and 300 pixels wide. Convolving an image with a kernel is relatively easy in pytorch, though we need to juggle some tensor dimensions because by convention, `conv2d` operates on $\\text{batch} \\times \\text{channel} \\times \\text{height} \\times \\text{width}$ tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5  # 0.5m standard deviation for the Gaussian kernel\n",
    "K = 21 # 21x21 kernel is big enough to accommodate that standard deviation \n",
    "kernel = gaussian_kernel(sigma*10, K) # multiply by 10 as map is 10cm resolution\n",
    "batch = cost_map[None, None, ...]  # Add batch and channel dimensions\n",
    "blurred = torch.conv2d(batch, kernel, padding='same')[0, 0, ...]\n",
    "px.imshow(blurred, color_continuous_scale='Reds').show()\n",
    "#| caption: Cost map blurred with 0.5 meter Gaussian kernel.\n",
    "#| label: fig:blurred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get truly differentiable factors, however, it is better to construct a custom Gaussian kernel at the exact location we want to evaluate the cost. Remember we are trying to minimize a trajectory, by moving around a set of positions $X_k$ at discrete time stamps $t_k$. In non-linear optimization, the story is always the same: evaluate the cost *and* its derivatives at a current guess, and then use the derivatives to move to a lower cost solution. However, if we smooth a discretized cost map, the result is still discretized, however smooth it looks to us. To get the *exact* value of the cost, and its derivatives, we can create a Gaussian kernel centered *exactly* at our current *continuous* position.\n",
    "\n",
    "The function `gaussian_filter` does just that: it evaluates a single Gaussian kernel operator given a standard deviation, a continuous map location, and the map to operate on. The code below shows that this gives the exact same result as a lookup in the blurred image when given an integer coordinate, but it *also* works for non-integer coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = Point2(7.5, 5.3) # point in map, in meters\n",
    "uv = 10*xy # continuous position in image\n",
    "local_result = gaussian_filter(sigma*10, uv, cost_map, K)\n",
    "print(f\"Local cost at {xy} is {local_result:.3f}\")\n",
    "\n",
    "# When uv are at integer values, blurred image gives the same result:\n",
    "assert np.allclose(local_result, blurred[int(uv[1]), int(uv[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the derivatives we blur the vertical and horizontal gradient images, respectively. Remember that we need the derivative of the exact cost with respect to x and y, if we are to optimize over those positions. Because convolution is a linear operation, we can take the derivative of the Gaussian kernel with respect to x and y, *or* we can apply the convolution on a derivative image. We choose the latter, by first using the Sobel operator we encountered in Section 5.4 to create gradient images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients:\n",
    "sobel_u, sobel_v = sobel_kernels()\n",
    "grad_u = torch.conv2d(batch, sobel_u, padding='same')[0,0,...]\n",
    "grad_v = torch.conv2d(batch, sobel_v, padding='same')[0,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#| caption: Obstacle cost map, blurred cost map, and both vertical and horizontal sobel gradients.\n",
    "#| label: fig:obstacles-2x2\n",
    "fig = pls.make_subplots(rows=2, cols=2)\n",
    "fig.add_trace(go.Heatmap(z=cost_map, colorscale='reds', showscale=False), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=blurred, colorscale='reds', showscale=False), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=grad_u, colorscale='rdbu', showscale=False), row=2, col=1)\n",
    "fig.add_trace(go.Heatmap(z=grad_v, colorscale='rdbu', showscale=False), row=2, col=2)\n",
    "for i in [1,2]:\n",
    "    for j in [1,2]:\n",
    "        fig.update_xaxes(title_text=\"\", showticklabels=False, row=i, col=j)\n",
    "        fig.update_yaxes(title_text=\"\", showticklabels=False, row=i, col=j)\n",
    "fig.update_layout(margin=dict(l=10, r=10, t=10, b=10))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure <a href=\"#fig:obstacles-2x2\" data-reference-type=\"ref\" data-reference=\"fig:obstacles-2x2\">3</a> shows both cost map and its blurred version, as well as the (un-blurred) gradient images `grad_u` and `grad_v`. We can then combined these into one rank-3 tensor and evaluate the cost *and* derivatives in one operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = torch.stack([cost_map, grad_u, grad_v], dim=0)\n",
    "print(np.round(gaussian_filter(sigma*10, uv, combined, K),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Graphs for Trajectory Optimization\n",
    "\n",
    "Now that we can evaluate cost its derivatives at any location, we can create factors. Since occupancy maps or cost maps are not built into GTSAM, we use its facility to create a custom factor from arbitrary python code. A `gtsam.CustomFactor` just has a constructor that can take an arbitrary python callback function, along with a `Key` (and a noise model). At evaluation, the callback function gets a handle to the factor and a `gtsam.Values` object, and it is supposed to do three things:\n",
    "\n",
    "- check which variable is involved, by asking the factor;\n",
    "- with that key, extract the current estimate from the passed in `Values`;\n",
    "- calculate the cost, and if asked, its derivatives.\n",
    "\n",
    "The fact that we can do this for arbitrary python code is very convenient, and is used below to create cost factors that interrogate the cost map, using our localized Gaussian filter, and if asked, its derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_func(factor: gtsam.CustomFactor, v: gtsam.Values, H: list[np.ndarray]):\n",
    "    \"\"\"Error function for the custom factor.\"\"\"\n",
    "    # Extract the Point2 variable using the key in the factor\n",
    "    point = v.atPoint2(factor.keys()[0])\n",
    "\n",
    "    uv = 10 * point # Convert to map coordinates\n",
    "\n",
    "    if H is None:\n",
    "        cost = gaussian_filter(sigma * 10, uv, cost_map, K)\n",
    "    else:\n",
    "        # If Jacobian is needed, calculate them here:\n",
    "        cost, cost_x, cost_y = gaussian_filter(sigma * 10, uv, combined, K)\n",
    "        H[0] = np.array([[cost_x, cost_y]])\n",
    "\n",
    "    return np.array([cost]).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us discretize the trajectory with M factors, using the keys $k\\in 1...M$. We can define a start and goal point, and initialize the trajectory as straight line. Who knows, we might be lucky and hit no obstacles along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100 # Number of points\n",
    "start, goal = Point2(2, 5), Point2(28, 5)\n",
    "initial = gtsam.Values()\n",
    "for k in range(1,M+1):\n",
    "    alpha = (k-1)/(M-1)\n",
    "    initial.insert(k, (1-alpha)*start + alpha*goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| caption: Initial trajectory estimate plotted on the cost map.\n",
    "#| label: fig:initial\n",
    "initial_path = 10*gtsam.utilities.extractPoint2(initial)\n",
    "fig = px.imshow(cost_map, color_continuous_scale='Reds')\n",
    "fig.add_trace(go.Scatter(x=initial_path[:,0], y=initial_path[:,1], mode='lines+markers', line=dict(color='gray')))\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, margin=dict(l=10, r=10, t=10, b=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this trajectory on the cost map shows that we are not lucky, however: we are definitely going through some obstacles. We can immediately create all obstacle cost factors now, and evaluate the cost to conform this is not a zero-cost path. Since these factors are non-linear, we store them in a `gtsam.NonlinearFactorGraph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacle_factors = gtsam.NonlinearFactorGraph()\n",
    "for k in range(1,M+1):\n",
    "    custom_factor = gtsam.CustomFactor(None, [k], error_func)\n",
    "    obstacle_factors.add(custom_factor)\n",
    "print(f\"Graph has {obstacle_factors.size()} factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error profile along the trajectory clearly corresponds with the obstacles we encounter along the way. We can evaluate both the individual errors and the sum-of-squares error $\\sum\\psi_k(X_k)^2$ of a factor graph. The former gives us an error profile along the way, and the latter represents a loss we would like to minimize. We show the former as a bar graph below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| caption: Errors for the initial trajectory estimate.\n",
    "#| label: fig:initial-errors\n",
    "errors = -obstacle_factors.linearize(initial).jacobian()[1]\n",
    "fig = px.bar(errors, labels={'x': 'Factor', 'y': 'Error'}, range_y=[0, 0.2])\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, height=300, margin=dict(l=10, r=10, t=10, b=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete trajectory optimization problem needs more than just obstacle factors, however. We also need to add start and goal factors, as well as smoothness factors. Start and goal factors make sure that we start at our current position, and tell our optimizer where we want to go. They can both be implemented as simple priors on the variables with keys $k=1$ and $k=100$. Smoothness factors are implemented using a `gtsam.BetweenFactor`, and make sure that we obtain a smooth, continuous trajectory.\n",
    "\n",
    "Both types of factors are added in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by copying the obstacle factors:\n",
    "graph = gtsam.NonlinearFactorGraph()\n",
    "graph.push_back(obstacle_factors)\n",
    "\n",
    "constrained2 = gtsam.noiseModel.Constrained.All(2) # We *insist* on the start and goal points!\n",
    "graph.addPriorPoint2(1, start, constrained2)\n",
    "graph.addPriorPoint2(M, goal, constrained2)\n",
    "\n",
    "# The strength of the smoothness factors is controlled by a `precision` value: higher is stronger.\n",
    "# Feel free to change the precision value below and see how the optimizer behaves:\n",
    "smoothness_model = gtsam.noiseModel.Isotropic.Precision(2, 0.01)\n",
    "for k in range(1, M):\n",
    "    graph.add(gtsam.BetweenFactorPoint2(k, k+1, gtsam.Point2(0.0, 0.0), smoothness_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize, we use a Levenberg Marquardt non-linear optimizer. This optimizer, which we have encountered before, linearizes the factors at every iteration and takes a cautious second-order minimization step. The \"cautiousness\" is governed by a parameter $\\lambda$ that essentially selects between gradient descent ($\\lambda$ high) and a second-order Gauss-Newton step ($\\lambda$ low).  In our potentially very non-linear cost landscape it is good practice to start of the optimizer with a relatively high value for $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial error: {graph.error(initial):.3f}\")\n",
    "params = gtsam.LevenbergMarquardtParams()\n",
    "params.setlambdaInitial(100)\n",
    "# params.setVerbosityLM(\"SUMMARY\") # uncomment this line to see the LM progress\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial, params)\n",
    "result = optimizer.optimize()\n",
    "print(f\"Final error: {graph.error(result):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is very fast, on the order of milliseconds, so it can be used in an online planner that continuously re-plans as the cost map is updated with new information. When used for flying a drone, we can execute on the initial part of the trajectory and then immediately re-plan. This scheme is known as \"model-predictive\" control, and is frequently used in actual autonomous vehicles and drones.\n",
    "\n",
    "In our case, the optimizer found a very nice trajectory that navigates some tricky obstacle configurations, especially in the second half of the trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| caption: Initial and final trajectories plotted on the blurred cost map.\n",
    "#| label: fig:final\n",
    "result_path = 10*gtsam.utilities.extractPoint2(result)\n",
    "fig = px.imshow(blurred, color_continuous_scale='Reds')\n",
    "fig.add_trace(go.Scatter(x=initial_path[:,0], y=initial_path[:,1], mode='lines+markers', line=dict(color='gray')))\n",
    "fig.add_trace(go.Scatter(x=result_path[:,0], y=result_path[:,1], mode='lines+markers', line=dict(color='green')))\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, margin=dict(l=10, r=10, t=10, b=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error profile along this final trajectory is not entirely down to zero, as we have to navigate some tight obstacle configurations along the way. However, when plotted on the same scale it looks much better than for the initial trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| caption: Errors for the final trajectory result.\n",
    "#| label: fig:final-errors\n",
    "errors = -obstacle_factors.linearize(result).jacobian()[1]\n",
    "fig = px.bar(errors, labels={'x': 'Factor', 'y': 'Error'}, range_y=[0, 0.2])\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, height=300, margin=dict(l=10, r=10, t=10, b=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "The above discretized trajectory is not great, as we cannot obtain velocities and accelerations easily. Below we use a class `SmoothTrajectory` which takes a discretized trajectory and fits a polynomial approximation, with N control points. Before we do, we add a constant height of 1.5m. Also, we set the flag `boundaries` to True, which enforces constant velocity (zero acceleration) at the endpoints.\n",
    "\n",
    "A key parameter here is the total trajectory time T, which turns our optimized path into a trajectory: a path in time. If we choose T to be small, we are asking the drone to fly fast and we might hit thrust limits. If we choose T to be high, the drone will fly more slowly. T=20 is a good compromise in our case, as we expect the average velocity to be a bit above 1 m/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add height of 1.5 meters\n",
    "xy = gtsam.utilities.extractPoint2(result)\n",
    "trajectory = np.hstack([xy, np.full((len(xy),1), 1.5)])\n",
    "\n",
    "T = 20.0\n",
    "smooth = SmoothTrajectory(trajectory, N=20, a=0, b=T, boundaries=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate and visualize the smoothed trajectory at *any* time t between 0 and T:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| caption: Discretized and smoothed trajectory.\n",
    "#| label: fig:smoothed\n",
    "\n",
    "t = np.linspace(0.0 , T, 250)\n",
    "smoothed = smooth.evaluate(t)\n",
    "\n",
    "fig = px.imshow(blurred, color_continuous_scale='Reds')\n",
    "fig.add_trace(go.Scatter(x=result_path[:,0], y=result_path[:,1], mode='lines', line=dict(color='green')))\n",
    "fig.add_trace(go.Scatter(x=10*smoothed[:,0], y=10*smoothed[:,1], mode='lines', line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=10*smooth.points[:,0], y=10*smooth.points[:,1], mode='markers', line=dict(color='blue')))\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, margin=dict(l=10, r=10, t=10, b=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is instructive to inspect the velocity and acceleration profiles as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| caption: smoothed trajectory as time series.\n",
    "#| label: fig:smoothed-series\n",
    "velocities = smooth.velocities(t)\n",
    "accelerations = smooth.accelerations(t)\n",
    "fig = pls.make_subplots(rows=4, cols=1, shared_xaxes=True)\n",
    "fig.add_trace(go.Scatter(x=t, y=velocities[:,0], mode='lines', line=dict(color='blue'), name='vx'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=t, y=accelerations[:,0], mode='lines', line=dict(color='blue'), name='ax'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=t, y=velocities[:,1], mode='lines', line=dict(color='blue'), name='vy'), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=t, y=accelerations[:,1], mode='lines', line=dict(color='blue'), name='ay'), row=4, col=1)\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, margin=dict(l=80, r=60, t=10, b=10), height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, for our chosen T=20, the drone flies at velocities between 1 and 1.6 m/s, which is what we expected for T=20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translational Control\n",
    "\n",
    "> Control is getting what you want.\n",
    "\n",
    "We control the position of the drone using a 3DOF vectored thrust input: thrust magnitude, roll, and pitch. The idea outlined here is a simplified version of the geometric controller in\n",
    "\n",
    "> K. Gamagedara, M. Bisheban, E. Kaufman, T. Lee, \"Geometric Controls of a Quadrotor UAV with Decoupled Yaw Control,\" in *2019 American Control Conference (ACC)*, Philadelphia, PA, USA, July 10-12, 2019.\n",
    "\n",
    "### A Virtual Vectored Thrust\n",
    "\n",
    "The idea is rather simple. Remember the translational equations of motion from Section 7.2:\n",
    "\n",
    "$$\n",
    "\\dot r^n = v^n \\\\\n",
    "m \\dot v ^n = R^n_b f + m g^n\n",
    "$$\n",
    "\n",
    "where $m$ is the mass. Let us concentrate on the dynamics part: we *know* the acceleration $\\dot v^n_d$ we want from our planned trajectory, and gravity is a given. Hence, to make the drone do what we want, we can only hope to change the quantity $R^n_b f$. Let is call that the **vectored thrust** $T^n$, defined in the navigation frame. Then, to obtain the desired acceleration, the vectored thrust has to satisfy\n",
    "\n",
    "$$\n",
    "T^n = m \\dot v^n_d - m g^n.\n",
    "$$\n",
    "\n",
    "You can think of $T^n$, which is a 3D vector, as a thrust vector that pushes the drone exactly where it needs to be. It can move the drone forwards or backwards by pitching appropriately, and left or right by rolling. But of course, at all times we need to make sure it cancels gravity as well, which explains the second term above. Even after gravity compensation, to gain or lose altitude, the thrust magnitude can be adjusted to make either happen. In control-theoretic terms, $T^n$ can be thought of as a virtual control input.\n",
    "\n",
    "All of this is implemented in the `vectored_thrust` function below, and they are shown below that, sampled at regular intervals along our optimized trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the vectored thrust:\n",
    "def vectored_thrust(accelerations: np.ndarray, mass: float, g: float = 9.81):\n",
    "    \"\"\"Calculate the vectored thrust for the desired accelerations.\"\"\"\n",
    "    gn = np.array([0, 0, -g]) # gravity vector in navigation frame.\n",
    "    return mass * (accelerations - gn)\n",
    "\n",
    "t = np.linspace(0.0 , T, 50)\n",
    "a_d = smooth.accelerations(t)\n",
    "\n",
    "# now calculate all the vectors:\n",
    "vectors = vectored_thrust(a_d, mass=1.0)\n",
    "print(f\"vectors.shape = {vectors.shape}\")\n",
    "print(f\"vectors[0] = {vectors[0]}\")\n",
    "print(f\"vectors[-1] = {vectors[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| caption: Smoothed trajectory with vectored thrust vectors.\n",
    "#| label: fig:vectored-thrust\n",
    "# Draw vectors with plotly as a set of vectors on top of a down-sampled smoothed trajectory (every 10 steps), using scatter3d:\n",
    "X, Y, Z = smoothed[::5, 0], smoothed[::5, 1], np.zeros_like(smoothed[::5, 0])\n",
    "\n",
    "fig = px.scatter_3d(x=X, y=Y, z=Z, color=t, color_continuous_scale='Viridis')\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "# Add thrust vectors as 3d arrows\n",
    "Tx, Ty, Tz = 0.2*vectors[:, 0], 0.2*vectors[:, 1], 0.2*vectors[:, 2]\n",
    "for i in range(len(X)):\n",
    "    fig.add_trace(go.Scatter3d(x=[X[i], X[i] + Tx[i]], y=[Y[i], Y[i] + Ty[i]], z=[Z[i], Z[i] + Tz[i]],\n",
    "                               mode='lines', line=dict(color='blue', width=2)))\n",
    "\n",
    "# Set aspect ratio based on W and Height of the map\n",
    "fig.update_layout(scene=dict(aspectratio=dict(x=1.2, y=H/W, z=2/W)))\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "# Apply the camera settings to the layout\n",
    "fig.update_layout(scene_camera=dict(eye=dict(x=-1, y=0.2, z=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the desired vectored thrust for the optimized trajectory from before. If you execute this section's notebook on colab you would be able to rotate the figure and ascertain that all thrust vectors, even if they are of different lengths, all have exactly the same vertical projection magnitude. This is of course by design, to cancel out gravity in the absence of any requested vertical height changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectoring using Fast Rotational Dynamics\n",
    "\n",
    "While we cannot *instantaneously* produce the desired vectored thrust $T^n$ at any given time, the dynamics of a quadrotor are fast enough that we can get rather close. Remember that the inertial matrix, which resists rotational acceleration, is typically small around the roll and pitch axes. Let us recall the the (approximate) rotational equations of motion:\n",
    "\n",
    "$$\n",
    "\\dot{R}^n_b\t= R^n_b\\hat{\\omega}^b \\\\\n",
    "I \\dot{\\omega}^b \\approx \\tau^b\n",
    "$$\n",
    "\n",
    "For typical drone configurations the inertial matrix $I$ can be well approximated as $\\text{diag}(I_{xy},I_{xy},I_z)$, with the roll-pitch component $I_{xy} < I_z$, the yaw component. Since $I_{xy}$ is small, and we have four powerful motors that can provide differential thrust very quickly, those dynamics are quite fast. Hence, if the attitude $R^n_b$ does not yet align the total thrust $f$ with the desired thrust vector $T^n$, we can quickly make it so by rotating.\n",
    "\n",
    "To figure out how to make the drone tilt, think of a video game where you are given a two axis controller and the screen displays the *current* orientation of the drone's Z-axis $\\hat k^n_b$ and the *desired* thrust vector $T^n$. Your job is to align the two by rolling (rotate around the body X-axis $\\hat x^n_b$) and pitching (rotating around the body Y-axis $\\hat y^n_n$). Looking at the drone from the outside, in the navigation frame, this will actually be rather hard: typically the controls are fixed to the *body* frame, and at every step you need to perform a mental rotation to find the right action. That is why racing drone pilots much prefer to use a \"first person view\" or FPV display and controller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical equivalent of FPV for a control *algorithm* is to rotate everything into the body frame. Taking the desired thrust vector $T^n$ and multiplying it with the transpose of the attitude $R^n_b$ yields the desired thrust vector $T^b$ in the body frame:\n",
    "\n",
    "$$\n",
    "T^b = (R^n_b)^T T^n.\n",
    "$$\n",
    "\n",
    "Let us examine each component of this three-vector $T^b$, in the body frame, in turn. Let us name the 3 components $T_F$,  $T_L$, and $T_U$, as we are using the XYZ = Forward-Left-Up convention. Then\n",
    "\n",
    "- when $T_F$ is *positive*, the desired thrust vector is ahead of the drone, and we should *pitch* forward. Looking from the tip of the Y/Left axis, this corresponds to a counter-clockwise, i.e., positive angular velocity $\\omega^b_y>0$ around the Y/Left axis. Likewise, if $T_F$ is *negative*, we should rotate clockwise around the Y-axis, i.e., $\\omega^b_y>0$. This inspires a control law for the torque around Y as follows:\n",
    "\n",
    "    $$\n",
    "    \\tau^b_y = K_F T_F;\n",
    "    $$\n",
    "\n",
    "- when $T_L$ is positive, the desired thrust vector is to the left, and we should *roll*. Looking from the tip of the X/Forward axis, this corresponds to a clock-wise, i.e., *negative* angular velocity $\\omega^b_x$. This inspires a control law for the torque around X as follows:\n",
    "\n",
    "    $$\n",
    "    \\tau^b_x = - K_L T_L;\n",
    "    $$\n",
    "\n",
    "- the component $T_U$ along the body Up/Z axis is a good approximation of the total thrust we need, and in fact is exactly correct if the thrust vector is already aligned correctly. So, we set\n",
    "\n",
    "    $$\n",
    "    f = K_U\n",
    "    $$\n",
    "\n",
    "The three control laws above constitute a simple yet effective controller for the drone, with $K_F$ and $K_L$ constants that govern how aggressive the controller is.The torque control laws above correspond to simple **proportional** controllers in control theory, with $-T_F$ and $T_L$ acting as errors to be driven to zero.  The higher the value of $K$, the faster the controller will try to remove those errors, but this could also lead to oscillations and even instabilities. While this is out of scope for the current text, we can obtain a more stable aggressive controller by adding a derivative term as well that is a function of the angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function implements the control law above:\n",
    "def control_law(desired_thrust: np.ndarray, nRb: gtsam.Rot3, K: float = 1.0):\n",
    "    \"\"\"Calculate the vectored thrust for the desired thrust vector, given in nav frame.\"\"\"\n",
    "    # Rotate the desired thrust vector to body frame:\n",
    "    T_F, T_L, T_U = nRb.matrix() @ desired_thrust\n",
    "\n",
    "    # Calculate the desired roll and pitch torques:\n",
    "    tau_x = K * T_F\n",
    "    tau_y = - K * T_L\n",
    "\n",
    "    # Return alongside total thrust to be commanded:\n",
    "    return tau_x, tau_y, T_U\n",
    "\n",
    "nRb = gtsam.Rot3.Ypr(0, 0, 0) # Assume the robot is initially level\n",
    "print(f\"control[0] = {control_law(vectors[0], nRb)}\")\n",
    "print(f\"control[-1] = {control_law(vectors[-1], nRb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Motor Thrust Vectors\n",
    "\n",
    "Creating the requested torque and thrust by the control law is done by adjusting the individual motor thrusts $f_i$. Recalling the force-torque equation in Section 7.2, if we ensure $f_{1} + f_{3} = f_{2} + f_{4}$ we obtain :\n",
    "\n",
    "$$\n",
    "\\tau_{xy}^b = l \\begin{bmatrix}\n",
    "f_{1}-f_{2}-f_{3}+f_{4}\\\\\n",
    "f_{1}+f_{2}-f_{3}-f_{4}\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The thrust vector magnitude $F^b_z = f$ can be adjusted quickly as well:\n",
    "\n",
    "$$\n",
    "f = f_1 + f_2 + f_3 + f_4.\n",
    "$$\n",
    "\n",
    "and hence we now have four equations in four unknowns. Solving for the forces yields\n",
    "\n",
    "$$\n",
    "f_1 = 0.25 \\left( f + \\tau_x/l + \\tau_y/l \\right) \\\\\n",
    "f_2 = 0.25 \\left( f - \\tau_x/l + \\tau_y/l \\right) \\\\\n",
    "f_3 = 0.25 \\left( f + \\tau_x/l - \\tau_y/l \\right) \\\\\n",
    "f_4 = 0.25 \\left( f - \\tau_x/l - \\tau_y/l \\right)\n",
    "$$\n",
    "\n",
    "where $l$ is the arm length, and $\\tau_x$ and $\\tau_y$ are the roll and pitch moment, respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code below implements this. Although we will not actually use it below, this is what you would have to do on a drone. However, this simplified code does not make any guarantees on not hitting torque limits. Hence, in practice more sophisticated approaches are used return a solution subject to actual control limits, while still trying to deliver some desired torques and/or thrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_to_motor_speeds(tau_x: float, tau_y: float, f: float, L: float = np.sqrt(0.002)):\n",
    "    \"\"\"Convert the desired torques and thrust to motor speeds.\"\"\"\n",
    "    # Calculate the motor speeds:\n",
    "    f_1 = 0.25 * (f + tau_x/L + tau_y/L)\n",
    "    f_2 = 0.25 * (f - tau_x/L + tau_y/L)\n",
    "    f_3 = 0.25 * (f + tau_x/L - tau_y/L)\n",
    "    f_4 = 0.25 * (f - tau_x/L - tau_y/L)\n",
    "\n",
    "    # Return as a tuple\n",
    "    return f_1, f_2, f_3, f_4\n",
    "\n",
    "print(f\"motor_speeds[-1] = {control_to_motor_speeds(*control_law(vectors[-1], nRb))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example\n",
    "\n",
    "**Jan '24: This section is not yet finalized, please check back later.**\n",
    "\n",
    "The proposed controller is relatively easy to implement in code. \n",
    "\n",
    "However, To make this work, we need to also correct for potential tracking errors. The above will work will *if* we are perfectly tracking the planned trajectory. Indeed, the trajectory optimization result tells us exactly what to do there. However, inevitably small errors will occur, and more importantly the pitch/roll controller that is supposed to deliver the thrust vector direction will have a small but consequential lag. Finally, motors might hit their thrust limits and not be able to deliver the desired thrust completely. All this means that errors *will* occur and we have to deal with them. To do so, we add two terms to reduce both positional and velocity errors:\n",
    "\n",
    "$$\n",
    "T^n = m \\dot v^n_d - m g^n - K_x (X - X_d) - K_v (\\dot X - \\dot X_d)\n",
    "$$\n",
    "\n",
    "This now requires tuning two additional parameters, $K_x$ and $K_v$, for the *proportional* and *derivative* control terms respectively. However, without these terms we have no hopes of staying on track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 250\n",
    "# Calculate moment of inertia for pitch and roll, values copied from Section 7.2\n",
    "g = 9.81\n",
    "mass = 1.0\n",
    "I_xy = 4 * 0.15 * mass * 0.1**2  # All 4 motors contribute to I_xy\n",
    "\n",
    "delta_t = T / len(smoothed)\n",
    "print(f\"Trajectory has {K} time steps, with a time step of {delta_t:.3f} seconds.\")\n",
    "\n",
    "# allocate storage for time series\n",
    "rn = np.zeros((3, K + 1), float)\n",
    "nRb = np.zeros((3, 3, K + 1), float)\n",
    "\n",
    "# We take the original position/velocity from the smoothed trajectory:\n",
    "rn[:, 0] = smooth.evaluate([[0]])\n",
    "vn = smooth.velocities([[0]]).flatten()\n",
    "\n",
    "# and have the drone face east (X-axis in navigation frame) initially:\n",
    "nRb0 = gtsam.Rot3.Ypr(0, 0, 0)\n",
    "nRb[:, :, 0] = nRb0.matrix()\n",
    "wb = np.zeros(3, float) # Initial angular velocity to zero\n",
    "\n",
    "# integrate forward\n",
    "for k in range(K):\n",
    "    t_k = np.array([[k * delta_t]])\n",
    "\n",
    "    # Get desired position, velocity, acceleration in nav frame:\n",
    "    rn_d = smooth.evaluate(t_k)\n",
    "    vn_d = smooth.velocities(t_k)\n",
    "    an_d = smooth.accelerations(t_k)\n",
    "\n",
    "    # Calculate vectored thrust:\n",
    "    nT = vectored_thrust(an_d, mass, g)\n",
    "    # print(f\"nT = {nT}\")\n",
    "\n",
    "    # Correct for tracking errors:\n",
    "    nT -= 0.1 * (rn[:, k] - rn_d)\n",
    "    # print positional error:\n",
    "    # print(f\"rn_d = {np.round(rn_d,2)}, rn = {np.round(rn,2)[:, k]}, error={np.round(rn[:, k] - rn_d,2)}\")\n",
    "    nT -= 0.1 * (vn - vn_d)\n",
    "    # print(f\"nT corrected = {np.round(nT,2)}\")\n",
    "\n",
    "    # Calculate controls:\n",
    "    nRb_k = nRb[:, :, k]\n",
    "    tau_x, tau_y, T_U = control_law(nT[0], gtsam.Rot3(nRb_k), K=0.0001)\n",
    "    # print(f\"nT = {nT}, tau_x = {tau_x}, tau_y = {tau_y}, T_U = {T_U}\")\n",
    "\n",
    "    # DYNAMICS:\n",
    "    # Update velocity in navigation frame.\n",
    "    an = nRb_k @ np.array([0, 0, T_U]) / mass\n",
    "    an[2] -= g\n",
    "    an = an_d.flatten() # Currently using the desired acceleration!\n",
    "    vn += an * delta_t\n",
    "\n",
    "    # Update angular velocity in body frame.\n",
    "    wb += np.array([tau_x, tau_y, 0]) * delta_t / I_xy\n",
    "    wb = np.zeros(3)\n",
    "    # print(f\"vn = {np.round(vn,2)}, wb = {np.round(wb,2)}\")\n",
    "\n",
    "    # KINEMATICS:\n",
    "    rn[:, k + 1] = rn[:, k] + vn * delta_t  # integrate position\n",
    "    # print(f\"an = {an}, vn = {vn}, wb = {wb}, rn = {rn[:, k+1]}\")\n",
    "    delta_R = gtsam.Rot3.Expmap(\n",
    "        wb * delta_t\n",
    "    )  # calculate incremental rotation matrix\n",
    "    nRb[:, :, k + 1] = nRb[:, :, k] @ delta_R.matrix()  # integrate attitude\n",
    "\n",
    "#| caption: planned vs executed.\n",
    "#| label: fig:smoothed\n",
    "executed = 10 * rn.T\n",
    "fig = px.imshow(blurred, color_continuous_scale='Reds')\n",
    "fig.add_trace(go.Scatter(x=executed[:,0], y=executed[:,1], mode='lines', line=dict(color='red')))\n",
    "fig.add_trace(go.Scatter(x=10*smoothed[:,0], y=10*smoothed[:,1], mode='lines', line=dict(color='blue')))\n",
    "fig.update_layout(coloraxis_showscale=False, showlegend=False, margin=dict(l=10, r=10, t=10, b=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line_3d(x=rn[0], y=rn[1], z=rn[2])\n",
    "scale, labels = 0.1, [\"F\", \"L\", \"U\"]\n",
    "for k in range(K+1):\n",
    "    nTb = gtsam.Pose3(gtsam.Rot3(nRb[:,:,k]), rn[:,k]);\n",
    "    fig.add_traces(axes(nTb, scale, labels))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "camera = dict(eye=dict(x=-10, y=0.2, z=1.5))\n",
    "fig.update_layout(scene_camera=camera, scene_aspectmode='data', showlegend=False, margin=dict(l=0, r=0, t=0, b=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Planning and Controlling Yaw\n",
    "\n",
    "- We'd like to point in a particular direction\n",
    "- forward looking, or camera drone\n",
    "- Explain why controlling yaw is harder\n",
    "- A simple yaw controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S75_drone_decision_theory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gtbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f7376ced4243bb13dfcffa8a3ba834e0602aa8334cd3a1d8ba8d285f4628083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
