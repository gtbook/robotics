{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S34_vacuum_perception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q gtbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import gtsam\n",
    "import pandas as pd\n",
    "\n",
    "import gtbook\n",
    "import gtbook.display\n",
    "from gtbook import vacuum\n",
    "from gtbook.discrete import Variables\n",
    "VARIABLES = Variables()\n",
    "\n",
    "def pretty(obj):\n",
    "    return gtbook.display.pretty(obj, VARIABLES)\n",
    "\n",
    "def show(obj, **kwargs):\n",
    "    return gtbook.display.show(obj, VARIABLES, **kwargs)\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    import plotly.io as pio\n",
    "    pio.renderers.default = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# From section 3.2:\n",
    "wxyz = gtsam.DiscreteBayesNet()\n",
    "W1 = VARIABLES.binary(\"W\")\n",
    "X1 = VARIABLES.binary(\"X\")\n",
    "Y1 = VARIABLES.binary(\"Y\")\n",
    "Z1 = VARIABLES.binary(\"Z\")\n",
    "wxyz.add(W1, [X1, Z1], \"1/1 1/1 1/1 1/1\")\n",
    "wxyz.add(X1, [Y1, Z1], \"1/1 1/1 1/1 1/1\")\n",
    "wxyz.add(Y1, [Z1], \"1/1 1/1\")\n",
    "wxyz.add(Z1, \"1/1\")\n",
    "\n",
    "# From Section 3.3:\n",
    "N = 3\n",
    "X = VARIABLES.discrete_series(\"X\", range(1, N+1), vacuum.rooms)\n",
    "A = VARIABLES.discrete_series(\"A\", range(1, N), vacuum.action_space)\n",
    "Z = VARIABLES.discrete_series(\"Z\", range(1, N+1), vacuum.light_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Perception with Graphical Models\n",
    "\n",
    "> Perception for dynamic Bayes nets is equivalent to inference in hidden Markov models or HMMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div align='center'>\n",
       "        <img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S34-iRobot%20vacuuming%20robot-07.jpg?raw=1' style='height:256 width:100%'/>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gtbook.display import randomImages\n",
    "from IPython.display import display\n",
    "display(randomImages(3, 4, \"steampunk\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes nets are great for *modeling*, but for infering the state of the robot over time we need better data structures. \n",
    "We first more formally define what we mean by inference, and introduce MAP and MPE inferemce.\n",
    "We then define hidden Markov models, and highlight their connection with robot\n",
    "localization over time. \n",
    "We then show how to efficiently perform inference by converting any Bayes net (with evidence) to a factor graph. \n",
    "We show both full posterior inference, MPE, and MAP estimation for HMMs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in Bayes Nets\n",
    "\n",
    "> Inference can mean full posterior inference, maximum probable explanation, or maximum a posteriori inference.\n",
    "\n",
    "**Inference** is the process of obtaining knowledge about a subset of\n",
    "variables given the known values for another subset of variables. In\n",
    "this section we will talk about how to do inference when the joint\n",
    "distribution is specified using a Bayes net, but we will not take\n",
    "advantage of the sparse structure of the network. Hence, the algorithms\n",
    "below are completely general, for any (discrete) joint probability\n",
    "distribution, as long as you can evaluate the joint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Posterior Inference\n",
    "\n",
    "The simplest case occurs when we can *partition* the variables into two\n",
    "sets: the hidden variables $\\mathcal{X}$ and the observed values\n",
    "$\\mathcal{Z}$. Then we can simply apply Bayes’ rule, but now applied to\n",
    "*sets* of variables, to obtain an expression for the posterior over the\n",
    "hidden variables $\\mathcal{X}$. Using the \"easy\" version of Bayes’ law\n",
    "we obtain\n",
    "\n",
    "$$P(\\mathcal{X}|\\mathcal{Z}=\\mathfrak{z})\\propto P(\\mathcal{X}, \\mathcal{Z}=\\mathfrak{z}), $$\n",
    "\n",
    "where $\\mathfrak{z}$ is the set of observed values for all variables in\n",
    "$\\mathcal{Z}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"128pt\" height=\"260pt\"\n viewBox=\"0.00 0.00 128.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 124,-256 124,4 -4,4\"/>\n<!-- var0 -->\n<g id=\"node1\" class=\"node\">\n<title>var0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">W</text>\n</g>\n<!-- var1 -->\n<g id=\"node2\" class=\"node\">\n<title>var1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"55\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"55\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n</g>\n<!-- var1&#45;&gt;var0 -->\n<g id=\"edge4\" class=\"edge\">\n<title>var1&#45;&gt;var0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M48.36,-72.41C45.09,-64.22 41.06,-54.14 37.38,-44.95\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"40.55,-43.45 33.59,-35.47 34.05,-46.05 40.55,-43.45\"/>\n</g>\n<!-- var2 -->\n<g id=\"node3\" class=\"node\">\n<title>var2</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"93\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"93\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n</g>\n<!-- var2&#45;&gt;var1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>var2&#45;&gt;var1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M84.19,-144.76C79.58,-136.28 73.84,-125.71 68.68,-116.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"71.61,-114.27 63.77,-107.15 65.46,-117.61 71.61,-114.27\"/>\n</g>\n<!-- var3 -->\n<g id=\"node4\" class=\"node\">\n<title>var3</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"38\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"38\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z</text>\n</g>\n<!-- var3&#45;&gt;var0 -->\n<g id=\"edge5\" class=\"edge\">\n<title>var3&#45;&gt;var0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M33.85,-216.12C27.14,-186.86 15.04,-124.76 19,-72 19.64,-63.52 20.84,-54.34 22.12,-46.04\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"25.58,-46.55 23.75,-36.11 18.68,-45.41 25.58,-46.55\"/>\n</g>\n<!-- var3&#45;&gt;var1 -->\n<g id=\"edge3\" class=\"edge\">\n<title>var3&#45;&gt;var1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M40.05,-215.87C42.95,-191.67 48.27,-147.21 51.72,-118.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"55.23,-118.53 52.94,-108.19 48.28,-117.7 55.23,-118.53\"/>\n</g>\n<!-- var3&#45;&gt;var2 -->\n<g id=\"edge1\" class=\"edge\">\n<title>var3&#45;&gt;var2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M49.93,-217.81C57.21,-208.55 66.66,-196.52 74.85,-186.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"77.66,-188.18 81.09,-178.16 72.16,-183.86 77.66,-188.18\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<gtbook.display.show at 0x109c0d7c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(wxyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an easy algorithm to calculate the posterior distribution\n",
    "above: simply enumerate all tuples $\\mathcal{X}$ in a table, evaluate\n",
    "$P(\\mathcal{X}, \\mathcal{Z}=\\mathfrak{z})$ for each one, and then\n",
    "normalize. As an example, let us consider the Bayes net on W, X, Y, Z above, \n",
    "and take $\\mathcal{X}=(X, Y)$ and $\\mathcal{Z}=(W, Z)$. \n",
    "As before, let us assume that each variable can take on 10 different outcomes, and that\n",
    "$\\mathfrak{z}=(2, 7)$. The resulting table for\n",
    "$P(X, Y|W=2, Z=7)\\propto P(W=2, X, Y, Z=7)$ is shown in the table below:\n",
    "\n",
    "|    *x*   |    *y*   |                 *P(W=2, X=x, Y=y, Z=7)*                |\n",
    "|:--------:|:--------:|:---------------------------------------------------:|\n",
    "|     1    |     1    |*P(W=2\\|X=1, Z=7)P(X=1\\Y=1, Z=7)P(Y=1\\Z=7)P(Z=7)*   |\n",
    "|     1    |     2    |    *P(W=2\\|X=1, Z=7)P(X=1\\|Y=2, Z=7)P(Y=2\\|Z=7)P(Z=7)*   |\n",
    "| ... | ... |                       ...                      |\n",
    "|    10    |     9    |   *P(W=2\\|X=10, Z=7)P(X=10\\|Y=9, Z=7)P(Y=9\\|Z=7)P(Z=7)*  |\n",
    "|    10    |    10    | *P(W=2\\|X=10, Z=7)P(X=10\\|Y=10, Z=7)P(Y=10\\|Z=7)P(Z=7)* |\n",
    "\n",
    "We normalize by calculating $\\sum_{x, y} P(W=2, X=x, Y=y, Z=7)$ by summing over all these entries, and subsequently dividing all entries by the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Probably Explanation\n",
    "\n",
    "A common inference problem associated with Bayes nets is the **most\n",
    "probable explanation** or MPE for $\\mathcal{X}$: given the values\n",
    "$\\mathfrak{z}$ for $\\mathcal{Z}$, what is the most probable joint\n",
    "assignment to the other variables $\\mathcal{X}$? While the posterior\n",
    "gives us the complete picture, the MPE is different in nature: it is a\n",
    "single assignment of values to $\\mathcal{X}$:\n",
    "\n",
    "$$x^*_{MPE} = \\arg \\max_x P(\\mathcal{X}|\\mathcal{Z}=\\mathfrak{z}).$$\n",
    "\n",
    "For example, given\n",
    "$\\mathfrak{z}=(2, 7)$, the MPE for $\\mathcal{X}$ could be $X=3$ and\n",
    "$Y=6$. Note that to compute the MPE, we need not bother with\n",
    "normalizing: we can simply find the maximum entry in the unnormalized\n",
    "posterior values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency (Not!)\n",
    "\n",
    "In both these inference problems, the simple algorithm outlined above is\n",
    "*not* efficient. In the example the table is 100 entries long, and in\n",
    "general the number of entries is exponential in the size of\n",
    "$\\mathcal{X}$. However, when inspecting the entries in the table\n",
    "there are already some obvious ways to save: for example, $P(Z=7)$ is a\n",
    "common factor in all entries, so clearly we should not even bother\n",
    "multiplying it in. Below we will discuss methods to fully\n",
    "exploit the structure of the Bayes net to perform efficient inference.\n",
    "\n",
    "If we had an efficient way to do inference, an MPE estimate would be a\n",
    "great way to estimate the trajectory of a robot over time. For example, \n",
    "using the \"robot\" dynamic Bayes net example from the last section, let us\n",
    "assume that we are given the value of all observations $O$ and actions\n",
    "$A$. Then the MPE would simply be a trajectory of robot states through\n",
    "the grid. This is an example of robot localization over time, and is a\n",
    "key capability of a mobile robot. However, it will have to wait until we\n",
    "can do efficient inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum a Posteriori estimation\n",
    "\n",
    "Finally, another well known inference problem is the **maximum a\n",
    "posteriori** or MAP estimate: given the values of some variables\n",
    "$\\mathcal{Z}$, what is the most probable joint assignment to a *subset*\n",
    "$\\mathcal{X}$ of the other variables? In this case the variables are\n",
    "partitioned into three sets: the variables of interest $\\mathcal{X}$, \n",
    "the nuisance variables $\\mathcal{Y}$, and the observed variables\n",
    "$\\mathcal{Z}$:\n",
    "\n",
    "$$\n",
    "P(\\mathcal{X}|\\mathcal{Z}=\\mathfrak{z})=\\sum_{\\mathfrak{y}}P(\\mathcal{X}, \\mathcal{Y}=\\mathfrak{y}|\\mathcal{Z}=\\mathfrak{z})\\propto\\sum_{\\mathfrak{y}}P(\\mathcal{X}, \\mathcal{Y}=\\mathfrak{y}, \\mathcal{Z}=\\mathfrak{z}).\n",
    "$$\n",
    "\n",
    "Finding a MAP estimate is more expensive than finding the MPE, as in\n",
    "addition to enumerating all possible combinations of $\\mathcal{X}$ and\n",
    "$\\mathcal{Y}$ values, we now need to calculate\n",
    "a possibly large number of sums, each exponential in the size of\n",
    "$\\mathcal{Y}$. In addition, the *number* of sums is\n",
    "exponential in the size of $\\mathcal{X}$. Below we will see that\n",
    "while we can still exploit the Bayes net structure, MAP estimates are\n",
    "fundamentally more expensive even in that case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Show that in the example above, if we condition on known values for $\\mathcal{Z}=(X,Z)$, the\n",
    "    posterior $P(W,Z|X,Y)$ factors, and as a consequence we only have to\n",
    "    enumerate two tables of length 10, instead of a large table of\n",
    "    size 100.\n",
    "\n",
    "2.  Calculate the size of the table needed to enumerate the posterior\n",
    "    over the states $S$ the robot dynamic Bayes net from the previous section,\n",
    "    given the value of all observations $Z$ and actions $A$.\n",
    "\n",
    "3.  Show that if we are given the states, inferring the actions is\n",
    "    actually quite efficient, even with the brute force enumeration.\n",
    "    Hint: this is similar to the first exercise above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "> HMMs are a general framework for perception over time.\n",
    "\n",
    "Here we will generalize from robots to arbitrary state and observation spaces. In the previous section we discussed dynamic Bayes networks to model how a robot state evolves over time by taking actions, and how measurements result in a particular state. In this section we will ask *how we can recover the state of the robot given only the observations*, i.e. without knowing the states: the state is \"hidden\". Here we will consider a general framework to answer this question.\n",
    "\n",
    "A **hidden Markov model** or HMM is a dynamic Bayes net that has two\n",
    "types of variables: states $\\mathcal{X}$ and measurements $\\mathcal{Z}$.\n",
    "The states $\\mathcal{X}$ are connected sequentially and satisfy the what\n",
    "is called the **Markov property**: the probability of a state $X_{t}$ is\n",
    "only dependent on the value of the previous state $X_{t-1}$. As we saw before, we call a sequence of random variables with this property a **Markov chain.** \n",
    "In addition, in an HMM we refer to the states $\\mathcal{X}$ as *hidden*\n",
    "states, as typically we cannot directly observe their values. Instead, \n",
    "they are indirectly observed through the measurements $\\mathcal{Z}$, \n",
    "where we have one measurement per hidden state. When these two\n",
    "properties are satisfied, we call this probabilistic model a hidden\n",
    "Markov model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures3/hmm-v2.png?raw=1\" id=\"fig:unrolledHMM\" style=\"width:14cm\" alt=\"\">\n",
    "<figcaption>An HMM, unrolled over three time-steps, represented by a Bayes net.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Figure\n",
    "<a href=\"#fig:unrolledHMM\" data-reference-type=\"ref\" data-reference=\"fig:unrolledHMM\">1</a>\n",
    "shows an example of an HMM for three time steps, i.e.., \n",
    "$\\mathcal{X}=\\{X_1, X_2, X_3\\}$ and\n",
    "$\\mathcal{Z}=\\{Z_1, Z_2, Z_3\\}$. As we discussed, in a Bayes net\n",
    "each node is associated with a conditional distribution: the Markov\n",
    "chain has the prior $P(X_1)$ and transition probabilities\n",
    "$P(X_2|X_1)$ and $P(X_3|X_2)$, whereas the measurements $Z_{t}$\n",
    "depend only on the state $X_{t}$, modeled by measurement models\n",
    "$P(Z_{t}|X_{t})$. In other words, the Bayes net encodes the following\n",
    "joint distribution $P(\\mathcal{X}, \\mathcal{Z})$:\n",
    "\n",
    "$$P(\\mathcal{X}, \\mathcal{Z})=P(X_1)P(Z_1|X_1)P(X_2|X_1)P(Z_2|X_2)P(X_3|X_2)P(Z_3|X_3)$$\n",
    "\n",
    "Note that we can also write this more succinctly as\n",
    "\n",
    "$$P(\\mathcal{X}, \\mathcal{Z})=P(\\mathcal{Z}|\\mathcal{X})P(\\mathcal{X})$$\n",
    "\n",
    "where\n",
    "\n",
    "$$P(\\mathcal{X})=P(X_1, X_2, X_3)=P(X_1)P(X_2|X_1)P(X_3|X_2)$$\n",
    "\n",
    "is the prior over state *trajectories*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Robot HMM\n",
    "\n",
    "Let us re-create the dynamic Bayes net from the previous section here, with 3 time steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"206pt\" height=\"188pt\"\n viewBox=\"0.00 0.00 206.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 202,-184 202,4 -4,4\"/>\n<!-- var4683743612465315841 -->\n<g id=\"node1\" class=\"node\">\n<title>var4683743612465315841</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"54,-180 0,-180 0,-144 54,-144 54,-180\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">A1</text>\n</g>\n<!-- var6341068275337658370 -->\n<g id=\"node4\" class=\"node\">\n<title>var6341068275337658370</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">X2</text>\n</g>\n<!-- var4683743612465315841&#45;&gt;var6341068275337658370 -->\n<g id=\"edge2\" class=\"edge\">\n<title>var4683743612465315841&#45;&gt;var6341068275337658370</title>\n<path fill=\"none\" stroke=\"black\" d=\"M45.17,-143.83C54.73,-134.27 66.53,-122.47 76.65,-112.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"79.4,-114.55 83.99,-105.01 74.45,-109.6 79.4,-114.55\"/>\n</g>\n<!-- var4683743612465315842 -->\n<g id=\"node2\" class=\"node\">\n<title>var4683743612465315842</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"126,-180 72,-180 72,-144 126,-144 126,-180\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">A2</text>\n</g>\n<!-- var6341068275337658371 -->\n<g id=\"node5\" class=\"node\">\n<title>var6341068275337658371</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"171\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">X3</text>\n</g>\n<!-- var4683743612465315842&#45;&gt;var6341068275337658371 -->\n<g id=\"edge4\" class=\"edge\">\n<title>var4683743612465315842&#45;&gt;var6341068275337658371</title>\n<path fill=\"none\" stroke=\"black\" d=\"M117.17,-143.83C126.73,-134.27 138.53,-122.47 148.65,-112.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.4,-114.55 155.99,-105.01 146.45,-109.6 151.4,-114.55\"/>\n</g>\n<!-- var6341068275337658369 -->\n<g id=\"node3\" class=\"node\">\n<title>var6341068275337658369</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n</g>\n<!-- var6341068275337658369&#45;&gt;var6341068275337658370 -->\n<g id=\"edge1\" class=\"edge\">\n<title>var6341068275337658369&#45;&gt;var6341068275337658370</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.22,-90C56.64,-90 59.11,-90 61.6,-90\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"61.74,-93.5 71.74,-90 61.74,-86.5 61.74,-93.5\"/>\n</g>\n<!-- var6485183463413514241 -->\n<g id=\"node6\" class=\"node\">\n<title>var6485183463413514241</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"54,-36 0,-36 0,0 54,0 54,-36\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z1</text>\n</g>\n<!-- var6341068275337658369&#45;&gt;var6485183463413514241 -->\n<g id=\"edge7\" class=\"edge\">\n<title>var6341068275337658369&#45;&gt;var6485183463413514241</title>\n<path fill=\"none\" stroke=\"black\" d=\"M27,-71.83C27,-64.13 27,-54.97 27,-46.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-46.41 27,-36.41 23.5,-46.41 30.5,-46.41\"/>\n</g>\n<!-- var6341068275337658370&#45;&gt;var6341068275337658371 -->\n<g id=\"edge3\" class=\"edge\">\n<title>var6341068275337658370&#45;&gt;var6341068275337658371</title>\n<path fill=\"none\" stroke=\"black\" d=\"M126.22,-90C128.64,-90 131.11,-90 133.6,-90\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"133.74,-93.5 143.74,-90 133.74,-86.5 133.74,-93.5\"/>\n</g>\n<!-- var6485183463413514242 -->\n<g id=\"node7\" class=\"node\">\n<title>var6485183463413514242</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"126,-36 72,-36 72,0 126,0 126,-36\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z2</text>\n</g>\n<!-- var6341068275337658370&#45;&gt;var6485183463413514242 -->\n<g id=\"edge6\" class=\"edge\">\n<title>var6341068275337658370&#45;&gt;var6485183463413514242</title>\n<path fill=\"none\" stroke=\"black\" d=\"M99,-71.83C99,-64.13 99,-54.97 99,-46.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"102.5,-46.41 99,-36.41 95.5,-46.41 102.5,-46.41\"/>\n</g>\n<!-- var6485183463413514243 -->\n<g id=\"node8\" class=\"node\">\n<title>var6485183463413514243</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"198,-36 144,-36 144,0 198,0 198,-36\"/>\n<text text-anchor=\"middle\" x=\"171\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z3</text>\n</g>\n<!-- var6341068275337658371&#45;&gt;var6485183463413514243 -->\n<g id=\"edge5\" class=\"edge\">\n<title>var6341068275337658371&#45;&gt;var6485183463413514243</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171,-71.83C171,-64.13 171,-54.97 171,-46.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"174.5,-46.41 171,-36.41 167.5,-46.41 174.5,-46.41\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<gtbook.display.show at 0x14c622c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbn = gtsam.DiscreteBayesNet()\n",
    "for k in range(1, N+1):\n",
    "    dbn.add(Z[k], [X[k]], vacuum.sensor_spec)\n",
    "for k in reversed(range(1, N)):\n",
    "    dbn.add(X[k+1], [X[k], A[k]], vacuum.action_spec)\n",
    "dbn.add(X[1], \"1/1/1/1/1\")\n",
    "show(dbn, hints={\"A\": 2, \"X\": 1, \"Z\": 0}, boxes={A[k][0] for k in range(1, N)}.union({Z[k][0] for k in range(1, N+1)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Inference in HMMs\n",
    "\n",
    "> Inference is easy to implement naively, but hopelessly inefficient.\n",
    "\n",
    "In inference, we might want to infer the maximum probable explanation\n",
    "(MPE) for the states $\\mathcal{X}$ given values\n",
    "$\\mathfrak{z}=\\{z_1, z_2, z_3\\}$ for $\\mathcal{Z}$. As we saw\n",
    "before, one way to perform inference is to apply Bayes’ rule to obtain an expression for the *posterior* probability distribution over\n",
    "the state trajectory $\\mathcal{X}$, given the measurements\n",
    "$\\mathcal{Z}=\\mathfrak{z}$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(\\mathcal{X}|\\mathcal{Z}) & \\propto P(\\mathcal{Z}=\\mathfrak{z}|\\mathcal{X})P(\\mathcal{X}) \\\\\n",
    "& =L(\\mathcal{X}; \\mathcal{Z}=\\mathfrak{z})P(\\mathcal{X})\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $P(\\mathcal{X})$ is the trajectory prior\n",
    "and the **likelihood** $L(\\mathcal{X}; \\mathcal{Z}=\\mathfrak{z})$ of\n",
    "$\\mathcal{X}$ given $\\mathcal{Z}=\\mathfrak{z}$ is defined as before as a function of $\\mathcal{X}$: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mathcal{X}; \\mathcal{Z}=\\mathfrak{z}) & \\doteq P(\\mathcal{Z}=\\mathfrak{z}|\\mathcal{X})\\\\\n",
    "& =P(z_1|X_1)P(z_2|X_2)P(z_3|X_3)\\\\\n",
    "& =L(X_1; Z_1)L(X_2; Z_2)L(X_3; Z_3)\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, a naive implementation for finding the **most probable\n",
    "explanation** (MPE) for $\\mathcal{X}$ would tabulate all possible\n",
    "trajectories $\\mathcal{X}$ and calculate the posterior $P(\\mathcal{X}|\\mathcal{Z})$ for each one. \n",
    "Unfortunately the number of entries in this giant table is\n",
    "*exponential* in the number of states. Not only is this computationally\n",
    "prohibitive for long trajectories, but intuitively it is clear that for\n",
    "many of these trajectories we are computing the same values over and\n",
    "over again. In fact, there are three different approaches to improve on\n",
    "this:\n",
    "\n",
    "1.  Branch & bound\n",
    "\n",
    "2.  Dynamic programming\n",
    "\n",
    "3.  Inference using factor graphs\n",
    "\n",
    "Branch and bound is a powerful technique but will not generalize to\n",
    "continuous variables, like the other two approaches will. And, we will\n",
    "see that dynamic programming, which underlies the classical inference\n",
    "algorithms in the HMM literature, is just a special case of the last\n",
    "approach. Hence, here we will dive in and immediately go for the most\n",
    "general approach: inference in factor graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Graphs\n",
    "\n",
    "> Factor graphs are *the* correct representation in which do inference.\n",
    "\n",
    "We first introduce the notion of factors. \n",
    "Again referring to the example from Figure\n",
    "<a href=\"#fig:unrolledHMM\" data-reference-type=\"ref\" data-reference=\"fig:unrolledHMM\">1</a>, \n",
    "let us consider the posterior.\n",
    "Since the measurements $\\mathcal{Z}$ are *known*, the posterior is\n",
    "proportional to the product of six **factors**, three of which derive\n",
    "from the the Markov chain, and three are likelihood factors as defined\n",
    "before:\n",
    "\n",
    "$$\n",
    "P(\\mathcal{X}|\\mathcal{Z})\\propto P(X_1)L(X_1; z_1)P(X_2|X_1)L(X_2; z_2)P(X_3|X_2)L(X_3; z_3)\n",
    "$$\n",
    "\n",
    "Some of these factors are unary factors, and some are binary factors. \n",
    "In particular, above some of the factors depend on just one hidden variable, \n",
    "for example $L(X_2; z_2)$, whereas others depend on two variables, e.g., the\n",
    "transition model $P(X_3|X_2)$. \n",
    "Measurements are not counted here, \n",
    "because once we are *given* the measurements $\\mathcal{Z}$, they merely\n",
    "function as known parameters in the likelihoods $L(X_{t}; z_{t})$, which\n",
    "are seen as functions of *just* the state $X_{t}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures3/fg-v2.png?raw=1\" id=\"fig: HMM-FG\" style=\"width:60.0%\" alt=\"\">\n",
    "<figcaption>An HMM with observed measurements, unrolled over time, represented as a factor graph.</figcaption>\n",
    "</figure>\n",
    "\n",
    "This motivates a different graphical model, a **factor graph**, in which\n",
    "we only represent the *hidden* variables $X_1$, $X_2$, and $X_3$, \n",
    "connected to factors that encode probabilistic information on them. For\n",
    "our example with three hidden states, the corresponding factor graph is\n",
    "shown in Figure\n",
    "<a href=\"#fig: HMM-FG\" data-reference-type=\"ref\" data-reference=\"fig: HMM-FG\">2</a> above.\n",
    "It should be clear from the figure that the connectivity of a factor\n",
    "graph encodes, for each factor $\\phi_{i}$, which subset of variables\n",
    "$\\mathcal{X}_{i}$ it depends on. We write:\n",
    "\n",
    "$$\n",
    "\\phi(\\mathcal{X})=\\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2)\\phi_4(X_2)\\phi_5(X_2, X_{3})\\phi_6(X_3)\n",
    "$$\n",
    "\n",
    "where the factors above are defined to correspond one-to-one to the six factors in the posterior, \n",
    "e.g., \n",
    "\n",
    "$$\\phi_6(X_3)\\doteq L(X_3; z_3).$$\n",
    "\n",
    "All measurements are associated with unary factors, whereas the Markov chain is\n",
    "associated mostly with binary factors, with the exception of the unary\n",
    "factor $\\phi_1(X_1)$. Note that in defining the factors we can omit\n",
    "any normalization factors, which in many cases results in computational\n",
    "savings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally a factor graph is a bipartite graph\n",
    "$F=(\\mathcal{U}, \\mathcal{V}, \\mathcal{E})$ with two types of nodes:\n",
    "**factors** $\\phi_{i}\\in\\mathcal{U}$ and **variables**\n",
    "*$X_{j}\\in\\mathcal{V}$.* Edges $e_{ij}\\in\\mathcal{E}$ are always between\n",
    "factor nodes and variables nodes. The set of random variable nodes\n",
    "adjacent to a factor $\\phi_{i}$ is written as $\\mathcal{X}_{i}$. With\n",
    "these definitions, a factor graph $F$ defines the factorization of a\n",
    "global function $\\phi(\\mathcal{X})$ as\n",
    "\n",
    "$$\\phi(\\mathcal{X})=\\prod_{i}\\phi_{i}(\\mathcal{X}_{i}).$$\n",
    "\n",
    "In other words, the independence relationships are encoded by the edges\n",
    "$e_{ij}$ of the factor graph, with each factor $\\phi_{i}$ a function of\n",
    "*only* the variables $\\mathcal{X}_{i}$ in its adjacency set. As example, \n",
    "for the factor graph in Figure\n",
    "<a href=\"#fig: HMM-FG\" data-reference-type=\"ref\" data-reference=\"fig: HMM-FG\">2</a>\n",
    "we have: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{X}_1 & =\\{X_1\\}\\\\\n",
    "\\mathcal{X}_2 & =\\{X_1\\}\\\\\n",
    "\\mathcal{X}_3 & =\\{X_1, X_2\\}\\\\\n",
    "\\mathcal{X}_4 & =\\{X_2\\}\\\\\n",
    "\\mathcal{X}_5 & =\\{X_2, X_3\\}\\\\\n",
    "\\mathcal{X}_6 & =\\{X_3\\}\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Bayes Nets into Factor Graphs.\n",
    "\n",
    "> It is trivial to convert Bayes nets with given variables into factor graphs.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures3/hmm-v2.png?raw=1\" id=\"fig:conversion\" style=\"width:12cm\" alt=\"\">\n",
    "<figcaption>Bayes net representation of an HMM.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures3/fg-v2.png?raw=1\" id=\"fig:conversion\" style=\"width:14cm\" alt=\"\">\n",
    "<figcaption>Conversion of HMM above to a factor graph, where measurements are known.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Every Bayes net can be trivially converted to a factor graph, as shown above.\n",
    "Recall that every node in a Bayes net denotes a conditional density on the\n",
    "corresponding variable and its parent nodes. Hence, the conversion is\n",
    "quite simple: every Bayes net node splits in *both* a variable node and\n",
    "a factor node in the corresponding factor graph. The factor is connected\n",
    "to the variable node, as well as the variable nodes corresponding to the\n",
    "parent nodes in the Bayes net. If some nodes in the Bayes net are\n",
    "evidence nodes, i.e., they are given as known variables, we omit the\n",
    "corresponding variable nodes: the known variable simply becomes a fixed\n",
    "parameter in the corresponding factor.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "1.  Convert the dynamic Bayes net from the previous section into a factor graph, assuming *no* known variables.\n",
    "\n",
    "1.  Finally, do the same again, but now assume the states are given. Reflect on the remarkable phenomenon that happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Graphs in GTSAM\n",
    "\n",
    "Let us create the factor graph directly using GTSAM. Before we do, however, we need to instantiate the given actions and measurements, both of which are assumed known:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = VARIABLES.assignment({A[1]: 'R', A[2]: 'U'})\n",
    "measurements = ['dark', 'medium', 'light']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the factorgraph, first adding the prior $\\phi(X_1)=P(X_1)$ on $X_1$, then the binary factors $\\phi(X_k, X_{k+1}) = P(X_{k+1}|X_k, A_k=a_k)$, and then the measurements likelihood factors $\\phi(X_k; Z_k=z_k) \\propto P(Z_k=z_k|X_k)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"206pt\" height=\"84pt\"\n viewBox=\"0.00 0.00 206.00 83.60\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 79.6)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-79.6 202,-79.6 202,4 -4,4\"/>\n<!-- var6341068275337658369 -->\n<g id=\"node1\" class=\"node\">\n<title>var6341068275337658369</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-57.6\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-53.9\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n</g>\n<!-- factor0 -->\n<g id=\"node4\" class=\"node\">\n<title>factor0</title>\n<ellipse fill=\"black\" stroke=\"black\" cx=\"16\" cy=\"-1.8\" rx=\"1.8\" ry=\"1.8\"/>\n</g>\n<!-- var6341068275337658369&#45;&#45;factor0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>var6341068275337658369&#45;&#45;factor0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M23.52,-39.58C20.7,-25.79 17.06,-7.97 16.19,-3.73\"/>\n</g>\n<!-- factor1 -->\n<g id=\"node5\" class=\"node\">\n<title>factor1</title>\n<ellipse fill=\"black\" stroke=\"black\" cx=\"69\" cy=\"-1.8\" rx=\"1.8\" ry=\"1.8\"/>\n</g>\n<!-- var6341068275337658369&#45;&#45;factor1 -->\n<g id=\"edge3\" class=\"edge\">\n<title>var6341068275337658369&#45;&#45;factor1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M38.92,-41.33C49.79,-27.41 64.67,-8.35 68.22,-3.8\"/>\n</g>\n<!-- factor3 -->\n<g id=\"node7\" class=\"node\">\n<title>factor3</title>\n<ellipse fill=\"black\" stroke=\"black\" cx=\"38\" cy=\"-1.8\" rx=\"1.8\" ry=\"1.8\"/>\n</g>\n<!-- var6341068275337658369&#45;&#45;factor3 -->\n<g id=\"edge6\" class=\"edge\">\n<title>var6341068275337658369&#45;&#45;factor3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M30.48,-39.58C33.3,-25.79 36.94,-7.97 37.81,-3.73\"/>\n</g>\n<!-- var6341068275337658370 -->\n<g id=\"node2\" class=\"node\">\n<title>var6341068275337658370</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-57.6\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-53.9\" font-family=\"Times,serif\" font-size=\"14.00\">X2</text>\n</g>\n<!-- var6341068275337658370&#45;&#45;factor1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>var6341068275337658370&#45;&#45;factor1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M90,-40.46C82.27,-26.59 71.99,-8.16 69.54,-3.77\"/>\n</g>\n<!-- factor2 -->\n<g id=\"node6\" class=\"node\">\n<title>factor2</title>\n<ellipse fill=\"black\" stroke=\"black\" cx=\"135\" cy=\"-1.8\" rx=\"1.8\" ry=\"1.8\"/>\n</g>\n<!-- var6341068275337658370&#45;&#45;factor2 -->\n<g id=\"edge5\" class=\"edge\">\n<title>var6341068275337658370&#45;&#45;factor2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M109.61,-40.75C118.9,-26.86 131.37,-8.22 134.34,-3.78\"/>\n</g>\n<!-- factor4 -->\n<g id=\"node8\" class=\"node\">\n<title>factor4</title>\n<ellipse fill=\"black\" stroke=\"black\" cx=\"99\" cy=\"-1.8\" rx=\"1.8\" ry=\"1.8\"/>\n</g>\n<!-- var6341068275337658370&#45;&#45;factor4 -->\n<g id=\"edge7\" class=\"edge\">\n<title>var6341068275337658370&#45;&#45;factor4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M99,-39.58C99,-25.79 99,-7.97 99,-3.73\"/>\n</g>\n<!-- var6341068275337658371 -->\n<g id=\"node3\" class=\"node\">\n<title>var6341068275337658371</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-57.6\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"171\" y=\"-53.9\" font-family=\"Times,serif\" font-size=\"14.00\">X3</text>\n</g>\n<!-- var6341068275337658371&#45;&#45;factor2 -->\n<g id=\"edge4\" class=\"edge\">\n<title>var6341068275337658371&#45;&#45;factor2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M160.39,-40.75C151.1,-26.86 138.63,-8.22 135.66,-3.78\"/>\n</g>\n<!-- factor5 -->\n<g id=\"node9\" class=\"node\">\n<title>factor5</title>\n<ellipse fill=\"black\" stroke=\"black\" cx=\"171\" cy=\"-1.8\" rx=\"1.8\" ry=\"1.8\"/>\n</g>\n<!-- var6341068275337658371&#45;&#45;factor5 -->\n<g id=\"edge8\" class=\"edge\">\n<title>var6341068275337658371&#45;&#45;factor5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171,-39.58C171,-25.79 171,-7.97 171,-3.73\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<gtbook.display.show at 0x12263c640>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = gtsam.DiscreteFactorGraph()\n",
    "graph.add(X[1], \"1 1 1 1 1\")  # \\phi(X_1) = P(X_1)\n",
    "for k in range(1, N):\n",
    "    conditional = gtsam.DiscreteConditional(X[k + 1], [X[k], A[k]], vacuum.action_spec)\n",
    "    conditional_a_k = conditional.choose(actions)  # \\phi(X,X+) = P(X+|X,A=a)\n",
    "    graph.push_back(conditional_a_k)\n",
    "for i, measurement in enumerate(measurements):\n",
    "    k = i + 1\n",
    "    conditional = gtsam.DiscreteConditional(Z[k], [X[k]], vacuum.sensor_spec)\n",
    "    z_k = vacuum.light_levels.index(measurement)\n",
    "    factor = conditional.likelihood(z_k)  # \\phi(X) = P(Z=z|X)\n",
    "    graph.push_back(factor)\n",
    "show(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that discrete distributions and conditionals, like $P(X_1)$ above, are perfectly fine factors, and in fact *derive* from the factor type in GTSAM. This is what allows us to add them directly the graph as is. Note that in a real implementation we might not take the detour to first construct the conditionals as above: we did so because they were conveniently available here, but typically we would construct factors directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing with Factor Graphs\n",
    "\n",
    "> We can evaluate, optimize, and sample from factor graphs.\n",
    "\n",
    "Once we convert a Bayes net with evidence into a factor graph where the\n",
    "evidence is all implicit in the factors, we can support a number of\n",
    "different computations. First, given any factor graph defining an\n",
    "unnormalized density $\\phi(X)$, we can easily **evaluate** it for any\n",
    "given value, by simply evaluating every factor and multiplying the\n",
    "results. The factor graph represents the unnormalized posterior, i.e., \n",
    "$\\phi(\\mathcal{X})\\propto P(\\mathcal{X}|\\mathcal{Z})$. \n",
    "\n",
    "Evaluation opens up the way to **optimization**, e.g., finding the most probable\n",
    "explanation or MPE, as we will do below. In the case of discrete\n",
    "variables, graph search methods can be applied, but we will use a\n",
    "different approach.\n",
    "\n",
    "While local or global maxima of the posterior are often of most\n",
    "interest, **sampling** from a probability density can be used to\n",
    "visualize, explore, and compute statistics and expected values\n",
    "associated with the posterior. However, the ancestral sampling method we\n",
    "discussed earlier only applies to directed acyclic graphs. There are\n",
    "however more general sampling algorithms that can be used for factor\n",
    "graphs, more specifically Markov chain Monte Carlo (MCMC) methods. One\n",
    "such method is Gibbs sampling, which proceeds by sampling one variable\n",
    "at a time from its conditional density given all other variables it is\n",
    "connected to via factors. This assumes that this conditional density can\n",
    "be easily obtained, which is in fact true for discrete variables.\n",
    "\n",
    "Below we use factor graphs as the organizing principle for probabilistic\n",
    "inference. In later chapters we will expand their use to continuous\n",
    "variables, and will see that factor graphs aptly describe the\n",
    "independence assumptions and sparse nature of the large nonlinear\n",
    "least-squares problems arising in robotics. But their usefulness extends\n",
    "far beyond that: they are at the core of the sparse linear solvers we\n",
    "use as building blocks, they clearly show the nature of filtering and\n",
    "incremental inference, and lead naturally to distributed and/or parallel\n",
    "versions of robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive MPE with GTSAM\n",
    "\n",
    "Because our factor graph is so small, it does not hurt to show off how easy it is to implement the naive algorithm. We just loop over all possible state trajectories, and keep track of the one with the highest value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found MPE solution with value 0.3277:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class='DiscreteValues'>\n",
       "  <thead>\n",
       "    <tr><th>Variable</th><th>value</th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th>X1</th><td>Hallway</td></tr>\n",
       "    <tr><th>X2</th><td>Dining Room</td></tr>\n",
       "    <tr><th>X3</th><td>Kitchen</td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<gtbook.display.pretty at 0x106ee3940>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpe_value = 0\n",
    "mpe_trajectory = None\n",
    "for x1 in vacuum.rooms:\n",
    "    for x2 in vacuum.rooms:\n",
    "        for x3 in vacuum.rooms:\n",
    "            trajectory = VARIABLES.assignment({X[1]: x1, X[2]: x2, X[3]: x3})\n",
    "            value = graph(trajectory)\n",
    "            if value > mpe_value:\n",
    "                mpe_value = value\n",
    "                mpe_trajectory = trajectory\n",
    "print(f\"found MPE solution with value {mpe_value:.4f}:\")\n",
    "pretty(mpe_trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this MPE is *for a given action and measurement sequence*. All those fixed values are implicit in the factors that we have added to the factor graph in `graph` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Max-Product Algorithm for HMMs\n",
    "\n",
    "> Max-product on HMMs, also known as the Viterbi algorithm, is a dynamic-programming algorithm for finding the MPE.\n",
    "\n",
    "In this section we discuss an algorithm that is much faster than the naive algorithm to find the MPE.\n",
    "Given a factor graph, the **max-product algorithm** is an $O(n)$ algorithm\n",
    "to find the maximum probable explanation or MPE.\n",
    "We will use the example from Figure\n",
    "<a href=\"#fig: HMM-FG\" data-reference-type=\"ref\" data-reference=\"fig: HMM-FG\">2</a>\n",
    "to give the intuition. To find the MPE for $\\mathcal{X}$ we need to\n",
    "*maximize* the product\n",
    "\n",
    "$$\\phi(X_1, X_2, X_3)=\\prod\\phi_{i}(\\mathcal{X}_{i})$$\n",
    "\n",
    "i.e., the **value** of the factor graph. \n",
    "\n",
    "Because the value of the factor graph is a product of factor values, we can compute its maximum recursively, dynamic programming style. We start by first looking at ll factors connected to $X_1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\max_\\mathcal{X} \\prod\\phi_{i}(\\mathcal{X}_{i})\n",
    "&= \\max_{X_1, X_2, X_3} ~~~\\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2) &\\phi_4(X_2)\\phi_5(X_2, X_{3})\\phi_6(X_3)\n",
    "\\\\ &= \\max_{X_2, X_3} ~~~\\{ \\max_{X_1} \\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2) \\} &\\phi_4(X_2)\\phi_5(X_2, X_{3})\\phi_6(X_3)\n",
    "\\\\ &= \\max_{X_2, X_3} ~~~\\tau(X_2) &\\phi_4(X_2)\\phi_5(X_2, X_{3})\\phi_6(X_3)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Above $\\tau(X_2)\\doteq \\max_{X_1} \\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2)$ records the maximum value resulting from *only* maximizing $X_1$, but this depends on the value of $X_2$. After we recursively find the optimal values for $X_2$ and $X_3$, we can recover $X_1$ by\n",
    "\n",
    "$$\n",
    "g_1(X_2) = \\arg \\max_{X_1} \\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2)\n",
    "$$\n",
    "\n",
    "where the *lookup table* can be created at the same time that we compute $\\tau(X_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the general principle for max-product by concentrating on $X_1$, but we must do the same computation for all other variables as well. \n",
    "Below we use the factor graph to illustrate how the max-product algorithm\n",
    "proceeds one variable at a time, using \"Bayes-net-style\" directed edges to represent the lookup tables $g_k(X_{k+1})$, and factors to represent the $\\tau(.)$ functions, as indeed they *are* factors. Because at every step, one variable is *eliminated* from the maximization, the max-product algorithm is in fact an instance of the **elimination algorithm**, which we will see pop up in many different guises.\n",
    "\n",
    "We proceed from left to right, i.e. we start with state $X_1$ and proceed until we\n",
    "processed all states. We will tackle the three steps one by one in ths subsections below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating $X_1$\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures3/max-product-1.png?raw=1\" style=\"width:60.0%\" alt=\"Eliminating X1\">\n",
    "</figure>\n",
    "\n",
    "We start by considering the first state $X_1$, and we form a **product\n",
    "factor** $\\phi(X_1, X_2)$ that collects *only* the factors connected\n",
    "to $X_1$:\n",
    "\n",
    "$$\n",
    "\\phi(X_1, X_2)=\\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2).\n",
    "$$\n",
    "\n",
    "When we use a factor in a product, we *remove* it from the original\n",
    "factor graph. Note that because one of those factors, the state\n",
    "transition model $\\phi_3(X_1, X_2)\\doteq P(X_2|X_1)$, is also\n",
    "connected to the second state $X_2$, the product factor is a function\n",
    "of *both* $X_1$ and $X_2$, i.e., it is a binary factor.\n",
    "\n",
    "The key observation in the max-product algorithm is that we can now\n",
    "*eliminate* $X_1$ from the problem, by looking at all possible values\n",
    "$x_2$ of $X_2$, and creating a lookup table $g_1$ for the best\n",
    "possible value of $X_1$:\n",
    "\n",
    "$$\n",
    "g_1(X_2)=\\arg \\max_{x_1}\\phi(x_1, X_2).\n",
    "$$\n",
    " \n",
    "The size of this lookup table is equal to the number of possible outcomes for $X_2$: \n",
    "in our vacuum-world example this is 5, as there are 5 rooms.\n",
    "\n",
    "We also record the value of the product factor for that maximum, so we\n",
    "can use it down the line for taking into account the consequence of each\n",
    "choice:\n",
    "\n",
    "$$\n",
    "\\tau(X_2)=\\max_{x_1}\\phi(x_1, X_2).\n",
    "$$\n",
    "\n",
    "In practice both steps can be implemented in a single \"eliminate\" function.\n",
    "We then put this new factor $\\tau(X_2)$ back into the graph, essentially\n",
    "summarizing the result of eliminating $X_1$ from the problem entirely, \n",
    "obtaining the **reduced graph**\n",
    "\n",
    "$$\n",
    "\\Phi_{2:3}=\\tau(X_2)\\phi_4(X_2)\\phi_5(X_2, X_3)\\phi_6(X_3).\n",
    "$$\n",
    "\n",
    "Let us reflect on what happened above, because it is significant: we\n",
    "eliminated $X_1$ from consideration, and obtained a reduced problem\n",
    "that only depends on the remaining states $X_2$ and $X_3$. You can\n",
    "intuitively see that this algorithm will terminate after $n$ steps, and\n",
    "in fact you could prove it by induction. In addition, the lookup table\n",
    "$g_1$ gives us a way that, once we know what the optimal value for\n",
    "$X_2$ is, we can just read off the optimal value for $X_1$. This is\n",
    "what we will do, in *reverse* elimination order, after the algorithm\n",
    "terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating $X_2$\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures3/max-product-2.png?raw=1\" style=\"width:50.0%\" alt=\"Eliminating X2\">\n",
    "</figure>\n",
    "\n",
    "We now perform exactly the same steps for the state $X_2$. In this\n",
    "case, the product factor $\\phi(X_2, X_3)$ has only factors connected\n",
    "to $X_2$, \n",
    "\n",
    "$$\n",
    "\\phi(X_2, X_3)=\\tau(X_2)\\phi_4(X_2)\\phi_5(X_2, X_3), \n",
    "$$\n",
    "\n",
    "which now includes the factor $\\tau(X_2)$ from the previous step. \n",
    "Note that since we started from the reduced graph, \n",
    "the product factor is guaranteed to not depend on the first state\n",
    "$X_1$: that was eliminated! In fact, we can now in turn eliminate\n",
    "$X_2$ from the problem, by looking at all possible values $x_3$ of\n",
    "$X_3$, and creating a lookup table $g_2$ for the best possible value\n",
    "of $X_2$, given $X_3$, \n",
    "\n",
    "$$\n",
    "g_2(X_3)=\\arg \\max_{x_2}\\phi(x_2, X_3), \n",
    "$$\n",
    "\n",
    "and as above we also\n",
    "record the value of the product factor for that maximum in a new factor\n",
    "$\\tau(X_3)$:\n",
    "\n",
    "$$\n",
    "\\tau(X_3)=\\max_{x_2}\\phi(x_2, X_3).\n",
    "$$\n",
    "\n",
    "We then put this new factor $\\tau(X_3)$ back into the graph, which is now reduced even more: \n",
    "\n",
    "$$\n",
    "\\Phi_{3:3}=\\tau(X_3)\\phi_6(X_3).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating $X_3$\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/gtbook/robotics/main/Figures3/max-product-3.png?raw=1\" style=\"width:50.0%\" alt=\"Eliminating X3\">\n",
    "</figure>\n",
    "\n",
    "Finally, we eliminate $X_3$, where the product factor is now the\n",
    "entire remaining graph and only depends on $X_3$, as all other states\n",
    "have already been eliminated:\n",
    "\n",
    "$$\n",
    "\\phi(X_3)=\\tau(X_3)\\phi_6(X_3).\n",
    "$$\n",
    "\n",
    "We again obtain a lookup table,\n",
    "\n",
    "$$\n",
    "g_3(\\emptyset)=\\arg \\max_{x_3}\\phi(x_3), \n",
    "$$\n",
    "\n",
    "and a new factor:\n",
    "\n",
    "$$\n",
    "\\tau(\\emptyset)=\\max_{x_3}\\phi(x_3).\n",
    "$$\n",
    "\n",
    "Note however that now the value does not depend on any arguments! \n",
    "This is indicated by making the argument list equal to the empty set $\\emptyset$.\n",
    "Indeed, $g_3$ just tells us what the best value for $X_3$ is, and $\\tau$\n",
    "tells us the corresponding value. Because it incorporates the factors\n",
    "from the previous elimination steps, this will in fact be exactly the\n",
    "MPE solution, and the recursion ends!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-substitution\n",
    "\n",
    "Once we know the value for $X_3$, we can simply plug it into the\n",
    "lookup table $g_2(X_3)$ to get the value for $X_2$, which we can\n",
    "then plug into the lookup table $g_1$ to get the value for $X_1$, \n",
    "and we recover the MPE in one single backward pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Max-Product\n",
    "\n",
    "The complete HMM max-product algorithm for any value of $n$ is given below, \n",
    "where we used the shorthand notation $\\Phi_{j:n}\\doteq\\phi(X_{j}, \\ldots, X_{n})$ \n",
    "to denote a reduced factor graph.\n",
    "The algorithm proceeds by eliminating one hidden state $X_{j}$ at\n",
    "a time, starting with the complete HMM factor graph $\\Phi_{1:n}$. As we\n",
    "eliminate each variable $X_{j}$, the function produces a single lookup\n",
    "table $g_{j}(X_{j+1})$, as well as a reduced factor graph $\\Phi_{j+1:n}$\n",
    "on the remaining variables. After all variables have been eliminated, \n",
    "the algorithm returns a chain of lookup tables that can be used to\n",
    "recover the MPE in reverse elimination order.\n",
    "\n",
    "---\n",
    "`MaxProductHMM` ($\\Phi_{1:n}$):\n",
    "* for $j=1...n$:\n",
    "  + $g_{j}(X_{j+1}), \\Phi_{j+1:n}\\gets \\text{CreateLookupTable}(\\Phi_{j:n}, X_{j})$\n",
    "  + return $g_1(X_2)g_2(X_3)\\ldots g_{n}(\\emptyset)$\n",
    "\n",
    "`CreateLookupTable` ($\\Phi_{j:n}, X_{j}$):\n",
    "* Remove all factors $\\phi_{i}(\\mathcal{X}_{i})$ that contain $X_{j}$ \n",
    "* Form the product factor $\\phi(X_{j}, X_{j+1})\\gets\\prod_{i}\\phi_{i}(\\mathcal{X}_{i})$\n",
    "* Eliminate $X_j$: $g_{j}(X_{j+1}), \\tau(X_{j+1})\\gets\\phi(X_{j}, X_{j+1})$\n",
    "* Add new factor $\\tau(X_{j+1})$ back into the graph $\\Phi_{j+1:n}$\n",
    "* return the lookup table $g_{j}(X_{j+1})$ and reduced graph $\\Phi_{j+1:n}$\n",
    "---\n",
    "\n",
    "In the HMM literature, the max-product algorithm is known as the *Viterbi* algorithm. However, we will see that max-product (and sum-product below) can be applied in more general settings than the linear chains one finds in HMMs.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "In the above, the lookup table $g_{j}(X_{j+1})$ resulting from eliminating $X_j$ is a function of only $X_{j+1}$, because an HMM is a *chain* of variables. Think about the more general case: what would $g$ be a function of, in that case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity\n",
    "\n",
    "The complexity of max-product is *linear* in the number of nodes, which is a nice improvement over exponential. The complexity of every elimination step is quadratic in the number of states, because we have to form the product factors and then maximize over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-product in GTSAM\n",
    "\n",
    "GTSAM's bread and butter is factor graphs, and finding the MPE is easy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class='DiscreteValues'>\n",
       "  <thead>\n",
       "    <tr><th>Variable</th><th>value</th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th>X1</th><td>Hallway</td></tr>\n",
       "    <tr><th>X2</th><td>Dining Room</td></tr>\n",
       "    <tr><th>X3</th><td>Kitchen</td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<gtbook.display.pretty at 0x12262f760>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpe = graph.optimize()\n",
    "pretty(mpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sum-Product Algorithm for HMMs\n",
    "\n",
    "> Sum-product on HMMs, also known as the Forward-Backward algorithm algorithm, is a dynamic-programming algorithm for doing full posterior inference.\n",
    "\n",
    "The sum-product algorithm for HMMs is a slight tweak on the max-product\n",
    "algorithm that instead produces a Bayes net that calculates the\n",
    "posterior probability $P(\\mathcal{X}|\\mathcal{Z})$. Whereas the\n",
    "max-product produces a DAG of lookup tables, the sum-product produces a\n",
    "DAG of conditionals, i.e., a Bayes net. This is particularly interesting\n",
    "if one is not content with a maximum probable explanation or MPE, but\n",
    "instead wants the **full Bayesian probability distribution** of which\n",
    "assignments to the states are more probable than others. The fact that\n",
    "we recover this distribution in the form of a Bayes net again is\n",
    "satisfying, because as we saw that is an economical representation of a\n",
    "probability distribution.\n",
    "\n",
    "One might wonder about the wisdom of all this: we started with a Bayes\n",
    "net, converted to a factor graph, and now end up with a Bayes net again?\n",
    "There are two important differences: the first Bayes net\n",
    "represents the joint distribution $P(\\mathcal{X}, \\mathcal{Z})$ and is\n",
    "very useful for modeling. However, the second Bayes represents the\n",
    "posterior $P(\\mathcal{X}|\\mathcal{Z})$, and only has nodes for the\n",
    "random variables in $\\mathcal{X}$, hence it is much smaller. Finally, in\n",
    "many practical cases we do not even bother with the modeling step, but\n",
    "construct the factor graph directly from the measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the key is that we can compute the posterior recursively from the product of factors, in dynamic programming style:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\mathcal{X}|\\mathcal{Z}) &\\propto \\prod\\phi_{i}(\\mathcal{X}_{i})\n",
    "\\\\&\\propto \\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2) \\phi_4(X_2)\\phi_5(X_2, X_{3})\\phi_6(X_3)\n",
    "\\\\&\\propto \\{\\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2)\\} ~~ \\phi_4(X_2)\\phi_5(X_2, X_{3})\\phi_6(X_3)\n",
    "\\\\ &\\propto \\{P(X_1|X_2, \\mathcal{Z}) \\tau(X_2)\\} ~~ \\phi_4(X_2)\\phi_5(X_2, X_{3})\\phi_6(X_3)\n",
    "\\\\ &= P(X_1|X_2, \\mathcal{Z}) ~~ P(X_2, X_3|\\mathcal{Z})\n",
    "\\end{align*}\n",
    "$$\n",
    "where the last equality invoked recursion to calculate the posterior $P(X_2, X_3|\\mathcal{Z})$ on the remaining variables from the remain factors. Below we assume the dependence on the given measurements $\\mathcal{Z}$ (and actions, if appropriate) as implied, and drop $\\mathcal{Z}$ from the equations.\n",
    "\n",
    "In contrast to max-product, we now define the factor $\\tau$ obtained by *summing* over the variable that is being eliminated\n",
    "\n",
    "$$\n",
    "\\tau(X_2)\\doteq \\sum_{X_1} \\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2)\n",
    "$$\n",
    "\n",
    "and, from the definition of conditional probability:\n",
    "\n",
    "$$\n",
    "P(X_1|X_2) = \\frac{\\phi_1(X_1)\\phi_2(X_1)\\phi_3(X_1, X_2)}{\\tau(X_2)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the only tweak necessary to the max-product is to replace the maximization and $\\arg \\max$ in the elimination step with the chain rule. \n",
    "Indeed, we factor each product\n",
    "factor $\\phi(X_{j}, X_{j+1})$ into a conditional $P(X_{j}|X_{j+1})$ and\n",
    "an (unnormalized) marginal $\\tau(X_{j+1})$:\n",
    "\n",
    "$$\n",
    "P(X_{j}|X_{j+1})\\tau(X_{j+1})\\gets\\phi(X_{j}, X_{j+1})\n",
    "$$\n",
    "\n",
    "The algorithm is called the **sum-product algorithm** because the\n",
    "marginal is obtained by summing over all values of the state $X_{j}$:\n",
    "\n",
    "$$\n",
    "\\tau(X_{j+1})=\\sum_{x_{j}}\\phi(x_{j}, X_{j+1})\n",
    "$$\n",
    "\n",
    "We do not bother normalizing this into a proper distribution, as these\n",
    "marginals are just intermediate steps in the algorithm. However, when\n",
    "computing the conditional, we do normalize, and is it so happens the\n",
    "normalization constant is simply equal to $1/\\tau(X_{j+1})$:\n",
    "\n",
    "$$\n",
    "P(X_{j}|X_{j+1})=\\frac{\\phi(X_{j}, X_{j+1})}{\\tau(X_{j+1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire algorithm is is listed below:\n",
    "\n",
    "---\n",
    "`SumProductHMM` ($\\Phi_{1:n}$):\n",
    "* for $j=1...n$:\n",
    "    - $P(X_{j}|X_{j+1}),\\Phi_{j+1:n}\\gets \\text{ApplyChainRule}(\\Phi_{j:n},X_{j})$\n",
    "    - return Bayes net $P(X_1|X_2)P(X_2|X_3)\\ldots P(X_{n})$\n",
    "\n",
    "`ApplyChainRule` ($\\Phi_{j:n}, X_{j}$):\n",
    "* Remove all factors $\\phi_{i}(\\mathcal{X}_{i})$ that contain $X_{j}$ \n",
    "* Create product factor $\\phi(X_{j}, X_{j+1})\\gets\\prod_{i}\\phi_{i}(\\mathcal{X}_{i})$\n",
    "* Factorize the product $P(X_{j}|X_{j+1})\\tau(X_{j+1})\\gets\\phi(X_{j}, X_{j+1})$\n",
    "* Add the new factor $\\tau(X_{j+1})$ back into the graph $\\Phi_{j+1:n}$\n",
    "* return the conditional $P(X_{j}|X_{j+1})$ and reduced graph $\\Phi_{j+1:n}$\n",
    "---\n",
    "\n",
    "Note that after we recover the Bayes net the algorithm terminates: there\n",
    "is no back-substitution step. However, one might consider ancestral\n",
    "sampling as a type of back-substitution: the reverse elimination order\n",
    "is always a topological sort of the resulting Bayes net! Hence, after\n",
    "the sum-product algorithm, we can sample as many realizations from the\n",
    "posterior as we want: rather than just one MPE, we now have thousands of\n",
    "plausible explanations, and ancestral sampling will yield them in\n",
    "exactly the correct frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar\n",
    "\n",
    "When we can produce samples $\\mathcal{X}^{(s)}$ from a posterior\n",
    "$P(\\mathcal{X}|\\mathcal{Z})$, we can calculate empirical means of any\n",
    "real-valued function $f(\\mathcal{X})$ as follows:\n",
    "\n",
    "$$\n",
    "E_{P(\\mathcal{X}|\\mathcal{Z})}[f(x)]\\approx\\sum f(\\mathcal{X}^{(s)})\n",
    "$$\n",
    "\n",
    "For example, we can calculate the posterior mean of how far the robot\n",
    "traveled, either in Euclidean or Manhattan distance. These estimators\n",
    "will have less variability than just calculating the distance for the\n",
    "MPE, as they average over the entire probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-Product in GTSAM\n",
    "\n",
    "In GTSAM, calling `sumProduct` yields a Bayes net, which encodes the full posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"206pt\" height=\"44pt\"\n viewBox=\"0.00 0.00 206.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 202,-40 202,4 -4,4\"/>\n<!-- var6341068275337658369 -->\n<g id=\"node1\" class=\"node\">\n<title>var6341068275337658369</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n</g>\n<!-- var6341068275337658370 -->\n<g id=\"node2\" class=\"node\">\n<title>var6341068275337658370</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X2</text>\n</g>\n<!-- var6341068275337658370&#45;&gt;var6341068275337658369 -->\n<g id=\"edge2\" class=\"edge\">\n<title>var6341068275337658370&#45;&gt;var6341068275337658369</title>\n<path fill=\"none\" stroke=\"black\" d=\"M71.78,-18C69.36,-18 66.89,-18 64.4,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"64.26,-14.5 54.26,-18 64.26,-21.5 64.26,-14.5\"/>\n</g>\n<!-- var6341068275337658371 -->\n<g id=\"node3\" class=\"node\">\n<title>var6341068275337658371</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"171\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X3</text>\n</g>\n<!-- var6341068275337658371&#45;&gt;var6341068275337658370 -->\n<g id=\"edge1\" class=\"edge\">\n<title>var6341068275337658371&#45;&gt;var6341068275337658370</title>\n<path fill=\"none\" stroke=\"black\" d=\"M143.78,-18C141.36,-18 138.89,-18 136.4,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"136.26,-14.5 126.26,-18 136.26,-21.5 136.26,-14.5\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<gtbook.display.show at 0x12262fca0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = graph.sumProduct()\n",
    "show(posterior, hints={\"X\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we can do with this *exact* posterior is sample from it, which is one possible state history conditioned on the available sensor measurements *and* the known action sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class='DiscreteValues'>\n",
       "  <thead>\n",
       "    <tr><th>Variable</th><th>value</th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th>X1</th><td>Hallway</td></tr>\n",
       "    <tr><th>X2</th><td>Dining Room</td></tr>\n",
       "    <tr><th>X3</th><td>Kitchen</td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<gtbook.display.pretty at 0x12254e850>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = posterior.sample()\n",
    "pretty(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go even further: the code below samples 1000 alternate state histories, parallel universes of what *could* have happened: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((3, 5))\n",
    "num_samples = 1000\n",
    "for i in range(num_samples):\n",
    "    sample = posterior.sample()\n",
    "    for k in range(1,3+1):\n",
    "        key = X[k][0]\n",
    "        room_index = sample[key]\n",
    "        counts[k-1][room_index] += 1 # base 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we summarize these 1000 alternate histories some way other than printing all of them out? One idea is to summarize, for every time step, what the probability is to be in a particular room. It turns out we can do this with a one-liner, because we kept track of counts in the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Living Room</th>\n",
       "      <th>Kitchen</th>\n",
       "      <th>Office</th>\n",
       "      <th>Hallway</th>\n",
       "      <th>Dining Room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>83.9</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>90.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>91.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Living Room  Kitchen  Office  Hallway  Dining Room\n",
       "1          1.7      1.4     2.8     83.9         10.2\n",
       "2          0.7      2.4     0.6      5.4         90.9\n",
       "3          5.8     91.1     0.6      0.3          2.2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=100*counts/num_samples, \n",
    "             index=range(1, N+1), columns=vacuum.rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These approximate marginals say how probable it is that the robot was in a particular room at a particular time step. This is much richer information that what is available in the MPE, which is just a point estimate for the trajectory. In the next section we will see how to go even further, and compute approximate costs associated with taking actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Execute the cell above multiple times and observe you *do* get different realizations (almost) every time, but that the approximate marginals stay roughly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTSAM 101\n",
    "\n",
    "> The GTSAM concepts used in this section, explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created, for the first time, an instance of the `gtsam.DiscreteFactorGraph` class. The constructor is trivial - takes no arguments.\n",
    "To add factors, we can use the following methods:\n",
    "\n",
    " 1. `add(self, j: Tuple[int, int], spec: str) -> None`\n",
    "\n",
    " 2. `add(self, j: Tuple[int, int], spec: List[float]) -> None`\n",
    "\n",
    " 3. `add(self, keys: List[Tuple[int, int]], spec: str) -> None`\n",
    "\n",
    "These are very similar to the `gtsam.DiscreteBayesNet` methods, but in factor graphs distinction between frontal and parent values, so we just have a key, or a list of keys as in the last method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key factor graph methods we used above are `optimize`, `maxProduct` and `sumProduct`:\n",
    "\n",
    "```python\n",
    "- optimize(self) -> gtsam::DiscreteValues\n",
    "- sumProduct(self) -> gtsam.DiscreteBayesNet\n",
    "```\n",
    "\n",
    "The first one returns the MPE as an assignment to discrete variables, whereas the second returns an entire Bayes net, encoding the posterior.\n",
    "\n",
    "There is actually a method `maxProduct as well, which we have not discussed:\n",
    "\n",
    "```python\n",
    "- maxProduct(self) -> gtsam.DiscreteLookupDAG\n",
    " ```\n",
    "\n",
    " It returns a `DiscreteLookupDAG` instance, which, similarly to a Besy net is a DAG, but instead contains lookup tables, not conditionals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gtsam.gtsam.DiscreteLookupDAG"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag = graph.maxProduct()\n",
    "type(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's mostly an internal data structure, and is not yet very \"inspectable\" in python. However, you can ask it to `argmax`, which is exactly what happens *inside* `optimize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class='DiscreteValues'>\n",
       "  <thead>\n",
       "    <tr><th>Variable</th><th>value</th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th>X1</th><td>Hallway</td></tr>\n",
       "    <tr><th>X2</th><td>Dining Room</td></tr>\n",
       "    <tr><th>X3</th><td>Kitchen</td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<gtbook.display.pretty at 0x12262fa60>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty(dag.argmax())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S34_vacuum_perception.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('nbdev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  },
  "vscode": {
   "interpreter": {
    "hash": "341996cd3f3db7b5e0d1eaea072c5502d80452314e72e6b77c40445f6e9ba101"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
