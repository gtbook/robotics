{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "tags": [
     "no-tex"
    ]
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S36_vacuum_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U gtbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gtsam\n",
    "\n",
    "import gtbook\n",
    "import gtbook.display\n",
    "from gtbook import vacuum\n",
    "from gtbook.discrete import Variables\n",
    "VARIABLES = Variables()\n",
    "def pretty(obj):\n",
    "    return gtbook.display.pretty(obj, VARIABLES)\n",
    "def show(obj, **kwargs):\n",
    "    return gtbook.display.show(obj, VARIABLES, **kwargs)\n",
    "\n",
    "# From section 3.2:\n",
    "N = 5\n",
    "X = VARIABLES.discrete_series(\"X\", range(1, N+1), vacuum.rooms)\n",
    "A = VARIABLES.discrete_series(\"A\", range(1, N), vacuum.action_space)\n",
    "\n",
    "# From section 3.5:\n",
    "conditional = gtsam.DiscreteConditional((2,5), [(0,5), (1,4)], vacuum.action_spec)\n",
    "R = np.empty((5, 4, 5), float)\n",
    "T = np.empty((5, 4, 5), float)\n",
    "for assignment, value in conditional.enumerate():\n",
    "    x, a, y = assignment[0], assignment[1], assignment[2]\n",
    "    R[x, a, y] = 10.0 if y == vacuum.rooms.index(\"Living Room\") else 0.0\n",
    "    T[x, a, y] = value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "> We will talk about model-based and model-free learning.\n",
    "\n",
    "<img src=\"Figures3/S36-iRobot_vacuuming_robot-04.jpg\" alt=\"Splash image with intelligent looking robot\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring to get Data\n",
    "\n",
    "> Where we gather experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adapt the `policy_rollout` code from the previous section to generate a whole lot of experiences of the form $(x,a,x',r)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_randomly(x1, horizon=N):\n",
    "    \"\"\"Roll out states given a random policy, for given horizon.\"\"\"\n",
    "    data = []\n",
    "    x = x1\n",
    "    for _ in range(1, horizon):\n",
    "        a = np.random.choice(4)\n",
    "        next_state_distribution = gtsam.DiscreteDistribution(X[1], T[x, a])\n",
    "        x_prime = next_state_distribution.sample()\n",
    "        data.append((x, a, x_prime, R[x, a, x_prime]))\n",
    "        x = x_prime\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use it to create 499 experiences and show the first 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 0, 10.0), (0, 1, 1, 0.0), (1, 1, 1, 0.0), (1, 0, 1, 0.0), (1, 3, 4, 0.0), (4, 1, 4, 0.0), (4, 2, 1, 0.0), (1, 0, 0, 10.0), (0, 1, 1, 0.0), (1, 3, 1, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "data = explore_randomly(vacuum.rooms.index(\"Living Room\"), horizon=500)\n",
    "print(data[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Reinforcement Learning\n",
    "\n",
    "> Just count, then solve the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can *estimate* the transition probabilities $T$ and reward table $R$ from the data, and then we can use the algorithms from before to calculate the value function and/or optimal policy.\n",
    "\n",
    "The math is just a variant of what we saw in the learning section of the last chapter. The rewards is easiest:\n",
    "\n",
    "$$\n",
    "R(x,a,x') \\approx \\frac{1}{N(x,a,x')} \\sum_{x,a,x'} r\n",
    "$$\n",
    "\n",
    "where $N(x,a,x')$ counts how many times an experience $(x,a,x')$ was recorded. The transition probabilities are a bit trickier:\n",
    "\n",
    "$$\n",
    "P(x'|x,a) \\approx \\frac{N(x,a,x)}{N(x,a)}\n",
    "$$\n",
    "\n",
    "where $N(x,a)=\\sum_{x'} N(x,a,x')$ is the number of times we took action $a$ in a state $x$. \n",
    "\n",
    "The code associated with that is fairly simple, modulo some numpy trickery to deal with division by zero and *broadcasting* the division:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_sum = np.zeros((5, 4, 5), float)\n",
    "T_count = np.zeros((5, 4, 5), float)\n",
    "count = np.zeros((5, 4), int)\n",
    "for x, a, x_prime, r in data:\n",
    "    R_sum[x, a, x_prime] += r\n",
    "    T_count[x, a, x_prime] += 1\n",
    "R_estimate = np.divide(R_sum, T_count, where=T_count!=0)\n",
    "xa_count = np.sum(T_count, axis=2)\n",
    "T_estimate = T_count/np.expand_dims(xa_count, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above `T_count` corresponds to $N(x,a,x')$, and the variable `xa_count` is $N(x,a)$. It is good to check the latter to see whether our experiences were more or less representative, i.e., visited all state-action pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17., 22., 21., 21.],\n",
       "       [25., 26., 26., 30.],\n",
       "       [32., 25., 24., 22.],\n",
       "       [26., 22., 14., 18.],\n",
       "       [28., 42., 29., 29.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xa_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems pretty good. If not, we can always gather more data, which we encourage you to experiment with.\n",
    "\n",
    "We can compare the ground truth transition probabilities $T$ with the estimated transition probabilities $\\hat{T}$, e.g., for the living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.2 0.8 0.  0.  0. ]\n",
      " [1.  0.  0.  0.  0. ]\n",
      " [0.2 0.  0.  0.8 0. ]]\n",
      "estimate:\n",
      "[[1.   0.   0.   0.   0.  ]\n",
      " [0.23 0.77 0.   0.   0.  ]\n",
      " [1.   0.   0.   0.   0.  ]\n",
      " [0.33 0.   0.   0.67 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ground truth:\\n{T[0]}\")\n",
    "print(f\"estimate:\\n{np.round(T_estimate[0],2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. And for the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:\n",
      "[[10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]]\n",
      "estimate:\n",
      "[[10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ground truth:\\n{R[0]}\")\n",
    "print(f\"estimate:\\n{np.round(R_estimate[0],2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, learning in this context can simply be done by gathering lots of experiences, and estimating models for how the world behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free Reinforcement Learning\n",
    "\n",
    "> All you need is Q, la la la la."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different, model-free approach is **Q_learning**. In the above we tried to *model* the world by trying estimate the (large) transition and reward tables. However, remember from the previous section that there is a much smaller table of Q-values $Q(x,a)$ that also allow us to act optimally, because we have\n",
    "\n",
    "$$\n",
    "\\pi^*(x) = \\arg \\max_a Q^*(x,a)\n",
    "$$\n",
    "\n",
    "where the Q-values are defined as\n",
    "\n",
    "$$\n",
    "Q^*(x,a) \\doteq \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^*(x')\n",
    "$$\n",
    "\n",
    "This begs the question whether we can simply learn the Q-values instead, which might be more *sample-efficient*, i.e., we would get more accurate values with less training data, as we have less quantities to estimate.\n",
    "\n",
    "To do this, remember that the Bellman equation can be written as \n",
    "\n",
    "$$\n",
    "V^*(x) = \\max_a Q^*(x,a)\n",
    "$$\n",
    "\n",
    "allowing us to rewrite the Q-values from above as \n",
    "\n",
    "$$\n",
    "Q^*(x,a) = \\sum_{x'} P(x'|x, a) \\{ R(x,a,x') + \\gamma \\max_{a'} Q^*(x',a') \\}\n",
    "$$\n",
    "\n",
    "This gives us a way to estimate the Q-values, as we can approximate the above using a Monte Carlo estimate, summing over our experiences:\n",
    "\n",
    "$$\n",
    "Q^*(x,a) \\approx \\frac{1}{N(x,a)} \\sum_{x,a,x'} R(x,a,x') + \\gamma \\max_{a'} Q^*(x',a')\n",
    "$$\n",
    "\n",
    "Unfortunately the estimate above *depends* on the optimal Q-values. Hence, the final Q-learning algorithm applies this estimate gradually, by \"alpha-blending\" between old and new estimates, which also averages over the reward:\n",
    "\n",
    "$$\n",
    "\\hat{Q}(x,a) \\leftarrow (1-\\alpha) \\hat{Q}(x,a) + \\alpha \\{R(x,a,x') +  \\gamma \\max_{a'} \\hat{Q}(x',a') \\}\n",
    "$$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70.16615474 66.74151053 74.12119126 67.74557498]\n",
      " [65.90656811 53.42049487 52.92272442 53.19868122]\n",
      " [48.86928102 54.59959115 49.762931   51.03132376]\n",
      " [56.57556793 52.43518011 68.36941774 61.11737138]\n",
      " [59.89698619 53.19146259 53.26810274 53.01350551]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "Q = np.zeros((5, 4), float)\n",
    "for x, a, x_prime, r in data:\n",
    "    old_Q_estimate = Q[x,a]\n",
    "    new_Q_estimate = r + gamma * np.max(Q[x_prime])\n",
    "    Q[x, a] = (1.0-alpha) * old_Q_estimate + alpha * new_Q_estimate\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are not yet quite accurate, as you can ascertain yourself by changing the number of experiences above, but note that an optimal policy can be achieved before we even converge."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S36_vacuum_RL.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
