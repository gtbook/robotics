{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S36_vacuum_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U gtbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gtsam\n",
    "import pandas as pd\n",
    "\n",
    "import gtbook\n",
    "import gtbook.display\n",
    "from gtbook import vacuum\n",
    "from gtbook.discrete import Variables\n",
    "VARIABLES = Variables()\n",
    "def pretty(obj): \n",
    "    return gtbook.display.pretty(obj, VARIABLES)\n",
    "def show(obj, **kwargs): \n",
    "    return gtbook.display.show(obj, VARIABLES, **kwargs)\n",
    "\n",
    "# From section 3.2:\n",
    "N = 5\n",
    "X = VARIABLES.discrete_series(\"X\", range(1, N+1), vacuum.rooms)\n",
    "A = VARIABLES.discrete_series(\"A\", range(1, N), vacuum.action_space)\n",
    "\n",
    "# From section 3.5:\n",
    "conditional = gtsam.DiscreteConditional((2,5), [(0,5), (1,4)], vacuum.action_spec)\n",
    "R = np.empty((5, 4, 5), float)\n",
    "T = np.empty((5, 4, 5), float)\n",
    "for assignment, value in conditional.enumerate():\n",
    "    x, a, y = assignment[0], assignment[1], assignment[2]\n",
    "    R[x, a, y] = 10.0 if y == vacuum.rooms.index(\"Living Room\") else 0.0\n",
    "    T[x, a, y] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "> We will talk about model-based and model-free learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Section is still in draft mode and was released for adventurous spirits (and TAs) only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div align='center'>\n",
       "        <img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S36-iRobot%20vacuuming%20robot-07.jpg?raw=1' style='height:256 width:100%'/>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gtbook.display import randomImages\n",
    "from IPython.display import display\n",
    "display(randomImages(3, 6, \"steampunk\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring to get Data\n",
    "\n",
    "> Where we gather experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adapt the `policy_rollout` code from the previous section to generate a whole lot of experiences of the form $(x,a,x',r)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_randomly(x1, horizon=N):\n",
    "    \"\"\"Roll out states given a random policy, for given horizon.\"\"\"\n",
    "    data = []\n",
    "    x = x1\n",
    "    for _ in range(1, horizon):\n",
    "        a = np.random.choice(4)\n",
    "        next_state_distribution = gtsam.DiscreteDistribution(X[1], T[x, a])\n",
    "        x_prime = next_state_distribution.sample()\n",
    "        data.append((x, a, x_prime, R[x, a, x_prime]))\n",
    "        x = x_prime\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use it to create 499 experiences and show the first 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2, 0, 10.0), (0, 0, 0, 10.0), (0, 3, 3, 0.0), (3, 0, 2, 0.0), (2, 1, 3, 0.0), (3, 0, 2, 0.0), (2, 2, 2, 0.0), (2, 2, 2, 0.0), (2, 1, 3, 0.0), (3, 0, 2, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "data = explore_randomly(vacuum.rooms.index(\"Living Room\"), horizon=500)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Reinforcement Learning\n",
    "\n",
    "> Just count, then solve the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can *estimate* the transition probabilities $T$ and reward table $R$ from the data, and then we can use the algorithms from before to calculate the value function and/or optimal policy.\n",
    "\n",
    "The math is just a variant of what we saw in the learning section of the last chapter. The rewards is easiest:\n",
    "\n",
    "$$\n",
    "R(x,a,x') \\approx \\frac{1}{N(x,a,x')} \\sum_{x,a,x'} r\n",
    "$$\n",
    "\n",
    "where $N(x,a,x')$ counts how many times an experience $(x,a,x')$ was recorded. The transition probabilities are a bit trickier:\n",
    "\n",
    "$$\n",
    "P(x'|x,a) \\approx \\frac{N(x,a,x)}{N(x,a)}\n",
    "$$\n",
    "\n",
    "where $N(x,a)=\\sum_{x'} N(x,a,x')$ is the number of times we took action $a$ in a state $x$. \n",
    "\n",
    "The code associated with that is fairly simple, modulo some numpy trickery to deal with division by zero and *broadcasting* the division:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_sum = np.zeros((5, 4, 5), float)\n",
    "T_count = np.zeros((5, 4, 5), float)\n",
    "count = np.zeros((5, 4), int)\n",
    "for x, a, x_prime, r in data:\n",
    "    R_sum[x, a, x_prime] += r\n",
    "    T_count[x, a, x_prime] += 1\n",
    "R_estimate = np.divide(R_sum, T_count, where=T_count!=0)\n",
    "xa_count = np.sum(T_count, axis=2)\n",
    "T_estimate = T_count/np.expand_dims(xa_count, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above `T_count` corresponds to $N(x,a,x')$, and the variable `xa_count` is $N(x,a)$. It is good to check the latter to see whether our experiences were more or less representative, i.e., visited all state-action pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25., 31., 31., 28.],\n",
       "       [34., 17., 26., 27.],\n",
       "       [20., 25., 22., 17.],\n",
       "       [22., 31., 31., 15.],\n",
       "       [24., 18., 29., 26.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xa_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems pretty good. If not, we can always gather more data, which we encourage you to experiment with.\n",
    "\n",
    "We can compare the ground truth transition probabilities $T$ with the estimated transition probabilities $\\hat{T}$, e.g., for the living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.2 0.8 0.  0.  0. ]\n",
      " [1.  0.  0.  0.  0. ]\n",
      " [0.2 0.  0.  0.8 0. ]]\n",
      "estimate:\n",
      "[[1.   0.   0.   0.   0.  ]\n",
      " [0.19 0.81 0.   0.   0.  ]\n",
      " [1.   0.   0.   0.   0.  ]\n",
      " [0.14 0.   0.   0.86 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ground truth:\\n{T[0]}\")\n",
    "print(f\"estimate:\\n{np.round(T_estimate[0],2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. And for the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:\n",
      "[[10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]]\n",
      "estimate:\n",
      "[[10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ground truth:\\n{R[0]}\")\n",
    "print(f\"estimate:\\n{np.round(R_estimate[0],2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, learning in this context can simply be done by gathering lots of experiences, and estimating models for how the world behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free Reinforcement Learning\n",
    "\n",
    "> All you need is Q, la la la la."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different, model-free approach is **Q_learning**. In the above we tried to *model* the world by trying estimate the (large) transition and reward tables. However, remember from the previous section that there is a much smaller table of Q-values $Q(x,a)$ that also allow us to act optimally, because we have\n",
    "\n",
    "$$\n",
    "\\pi^*(x) = \\arg \\max_a Q^*(x,a)\n",
    "$$\n",
    "\n",
    "where the Q-values are defined as\n",
    "\n",
    "$$\n",
    "Q^*(x,a) \\doteq \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^*(x')\n",
    "$$\n",
    "\n",
    "This begs the question whether we can simply learn the Q-values instead, which might be more *sample-efficient*, i.e., we would get more accurate values with less training data, as we have less quantities to estimate.\n",
    "\n",
    "To do this, remember that the Bellman equation can be written as \n",
    "\n",
    "$$\n",
    "V^*(x) = \\max_a Q^*(x,a)\n",
    "$$\n",
    "\n",
    "allowing us to rewrite the Q-values from above as \n",
    "\n",
    "$$\n",
    "Q^*(x,a) = \\sum_{x'} P(x'|x, a) \\{ R(x,a,x') + \\gamma \\max_{a'} Q^*(x',a') \\}\n",
    "$$\n",
    "\n",
    "This gives us a way to estimate the Q-values, as we can approximate the above using a Monte Carlo estimate, summing over our experiences:\n",
    "\n",
    "$$\n",
    "Q^*(x,a) \\approx \\frac{1}{N(x,a)} \\sum_{x,a,x'} R(x,a,x') + \\gamma \\max_{a'} Q^*(x',a')\n",
    "$$\n",
    "\n",
    "Unfortunately the estimate above *depends* on the optimal Q-values. Hence, the final Q-learning algorithm applies this estimate gradually, by \"alpha-blending\" between old and new estimates, which also averages over the reward:\n",
    "\n",
    "$$\n",
    "\\hat{Q}(x,a) \\leftarrow (1-\\alpha) \\hat{Q}(x,a) + \\alpha \\{R(x,a,x') +  \\gamma \\max_{a'} \\hat{Q}(x',a') \\}\n",
    "$$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84.81793141 74.78943818 87.23744972 71.16209362]\n",
      " [85.78854973 73.52011122 73.02514028 60.90818195]\n",
      " [52.42934192 67.64563199 52.22683159 43.43022893]\n",
      " [54.28411796 59.33905004 84.23076831 69.8358031 ]\n",
      " [68.60452978 53.89759237 65.17961712 60.7351691 ]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "Q = np.zeros((5, 4), float)\n",
    "for x, a, x_prime, r in data:\n",
    "    old_Q_estimate = Q[x,a]\n",
    "    new_Q_estimate = r + gamma * np.max(Q[x_prime])\n",
    "    Q[x, a] = (1.0-alpha) * old_Q_estimate + alpha * new_Q_estimate\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are not yet quite accurate, as you can ascertain yourself by changing the number of experiences above, but note that an optimal policy can be achieved before we even converge."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S36_vacuum_RL.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
