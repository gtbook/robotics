{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "tags": [
     "no-tex"
    ]
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S51_diffdrive_state.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U gtbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import gtsam\n",
    "from gtbook.display import show, pretty\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Space for a Differential Drive Robot\n",
    "\n",
    "> Unlike robots with omni-directional wheels, the orientation of a differential drive robot matters.\n",
    "\n",
    "<img src=\"Figures5/S51-Two-wheeled_Toy_Robot-03.jpg\" alt=\"Splash image with differential drive robots in different states\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen two kinds of state space: discrete state spaces (the categories of trash, the rooms in a house)\n",
    "and continuous state spaces that were equivalent to $\\mathbb{R}^2$ (the position of a logistics robot in a warehouse).\n",
    "In this chapter, we consider a robot whose state space includes the orientation of the robot as well as its position.\n",
    "In particular, we consider differential drive robots (DDRs), such as the Duckiebot shown below.\n",
    "DDRs have two actuated wheels that share a common axis of rotation, and typically have a castor wheel in the back\n",
    "to stabilize the robot (without this castor wheel, the DDR would essentially be equivalent to a Segway two-wheeled scooter).\n",
    "Unlike robots with omni-directional wheels, DDRs cannot move in the direction parallel to the wheel axis -- they can only move\n",
    "in the steering direction. Because of this, it is necessary to include the orientation of the robot in its state description.\n",
    "\n",
    "<figure id=\"fig:duckiebot\">\n",
    "<img src=\"https://github.com/gtbook/robotics/blob/main/Figures5/duckiebot.png?raw=1\" style=\"width:14cm\" alt=\"\">\n",
    "<figcaption>Two views of the Duckiebot platform. Note the two actuated wheels in the front of the robot, and the castor wheel in the back.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing the state of the logistics robot was straightforward, we merely used the x- and y-coordinates of the robot's center of mass\n",
    "(in the case of a robot with circular shape, the center of this circle).\n",
    "Representing orientation is slightly more complex, and cannot be accomplished by merely encoding properties of a single point on the robot.\n",
    "Instead, we rigidly attach a coordinate frame to the robot, and define the robot state by the position of the origin of this\n",
    "frame and the orientation of the frame with repsect to the world frame.\n",
    "We refer to the robot's frame as the body-attached frame, or merely the robot frame.\n",
    "For DDR robots, it is typical to place the origin of the body-attached frame at the midpoint between the two wheels, and to align its\n",
    "x-axis with the forward steering direction. The y-axis is coincident with the axis of wheel rotation.\n",
    "This frame is illustrated in the figure below.\n",
    "\n",
    "<figure id=\"fig:DDR-coordinate-frame\">\n",
    "<img src=\"https://github.com/gtbook/robotics/blob/main/Figures5/DDR-coordinate-frame.png?raw=1\" style=\"width:9cm\" alt=\"\">\n",
    "<figcaption>A Coordinate frame that is rigidly attached to a DDR.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many robotics applications, if we are interested only in geometric aspects of the problem (e.g., if we are not concerned with dynamics, or with forces that are required to effect motion), we use the term *configuration space* instead of the term *state space*. \n",
    "A **configuration**, denoted by $q$, is a complete specificiation of the location of every point on a robotic system (assuming that a model of the robot\n",
    "is available).  The **configuration space**, denoted by ${\\cal Q}$, is the set of all configurations.\n",
    "For a DDR, the position and orientation of the robot provide such a specification; if we know the position and orientation of the robot,\n",
    "we can infer the location of any point on the robot.\n",
    "In this chapter, we will therefore use ${\\cal Q} = \\mathbb{R}^2 \\times [0, 2\\pi),$\n",
    "and $q = (x,y,\\theta)$ to parameterize the configuration space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the problem of determining the x-y position of the wheel centers for our DDR.\n",
    "If the wheelbase (i.e., the distance between the two wheel centers) is denoted by $L$,\n",
    "and the robot is in configuration $q=(x,y.\\theta)$,\n",
    "then\n",
    "the x-y coordinates of the left and right wheel centers are given by\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{c} x_{\\mathrm{left}} \\\\ y_{\\mathrm{left}} \\end{array}\\right]\n",
    "=\n",
    "\\left[ \\begin{array}{c} x - \\frac{L}{2} \\sin \\theta \\\\ \\ \\\\ y + \\frac{L}{2} \\cos \\theta \\end{array}\\right]\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\left[ \\begin{array}{c} x_{\\mathrm{right}}  \\\\ y_{\\mathrm{right}} \\end{array}\\right]\n",
    "=\n",
    "\\left[ \\begin{array}{c} x + \\frac{L}{2} \\sin \\theta \\\\ \\ \\\\ y - \\frac{L}{2} \\cos \\theta \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "<figure id=\"fig:DDR-wheel-centers\">\n",
    "<img src=\"https://github.com/gtbook/robotics/blob/main/Figures5/DDR-wheel-centers.png?raw=1\" style=\"width:9cm\" alt=\"\">\n",
    "<figcaption>Determining the position of the wheel centers.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same geometric analysis to any point on the robot. Consider a point $p$ that is\n",
    "rigidly attached to the robot.\n",
    "We can define the coordinates of $p$ with respect to the body-attached frame as\n",
    "$p^{\\mathrm{body}} = [p_x, p_y]^T$.\n",
    "If the robot is in configuration $q=(x,y.\\theta)$,\n",
    "then\n",
    "the x-y coordinates of $p$ with respect to the world coordinate frame are given by\n",
    "\n",
    "$$\n",
    "p^{\\mathrm{world}} =\n",
    "\\left[ \\begin{array}{c} \n",
    "x +p_x \\cos \\theta - p_y \\sin \\theta \\\\ \n",
    "y +p_x \\sin \\theta + p_y \\cos \\theta\n",
    " \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "<figure id=\"fig:DDR-arbitrary-point\">\n",
    "<img src=\"https://github.com/gtbook/robotics/blob/main/Figures5/DDR-arbitrary-point.png?raw=1\" style=\"width:9cm\" alt=\"\">\n",
    "<figcaption>A Coordinate frame that is rigidly attached to a DDR.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in these examples, given a model of the robot,\n",
    "knowing the configuration $q = (x,y,\\theta)$ is sufficient to allow us to determine the position of any point on the robot.\n",
    "Nevertheless, the procedure we used above -- using simple planar geometry to infer positions in an ad hoc way -- is not\n",
    "so satisfying.\n",
    "Furthermore, our choice to represent orientation using $\\theta \\in [0,2\\pi)$ also has some problems.\n",
    "Mainly these problems are due to wrap-around at $\\theta = 2\\pi$.\n",
    "Suppose, for example, that the orientation of the robot is $2\\pi - \\epsilon$ for any $\\epsilon > 0$.\n",
    "If the robot rotates in the positive direction, we will see large change in $\\theta$.\n",
    "In fact, no matter how small we make $\\epsilon$, the change in $\\theta$ will be approximately $2\\pi$ when the\n",
    "wrap-around happens.\n",
    "Therefore, our mapping from the actual robot orientation to the representation by $\\theta$ is not continuous.\n",
    "This causes lots of problems, both mathematically and for implementations in code.\n",
    "For now, we will use GTSAM to deal with these difficulties,\n",
    "but in the next chapter we will introduce homogeneous transformations, which provide a solution to all of these difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations in GTSAM\n",
    "\n",
    "In GTSAM, we use the class `gtsam.Pose2` to represent a configuration $q = (x,y,\\theta)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose: (12.4, 42.5, 0.785398)\n",
      "with x=12.4, y=42.5, theta=0.7853981633974482\n"
     ]
    }
   ],
   "source": [
    "pose = gtsam.Pose2(12.4, 42.5, math.radians(45))\n",
    "print(f\"pose: {pose}with x={pose.x()}, y={pose.y()}, theta={pose.theta()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that internally we represent poses using radians, hence the ugly looking number above. Often, it makes sense to specify *and* display angles in degrees, which makes specifying poses and debugging code easier. Hence, we also provide a \"pretty\" version that does the conversion for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "(x=12.4, y=42.5, theta=45.0)"
      ],
      "text/plain": [
       "<gtbook.display.pretty at 0x10e2a6940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty(pose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a real number $\\theta$ to represent orientation, while convenient and familiar, is *not* ideal as numbers that are offset by 360 degrees represent the same orientation. In other words, there is *not* a one-to-one relationship between orientation and its representation as a float value. Hence, internally GTSAM stores the orientation as *two* numbers, the unit vector $(\\cos\\theta,\\sin\\theta)$, which *is* a unique representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densities over Pose\n",
    "\n",
    "> We also need to think about probability densities over poses. \n",
    "\n",
    "It is conceptually easy to extend the *finite element* approximation to include orientation: just discretize $\\theta$ using some chosen resolution, e.g., one bin for every 5 degrees. However, one thing to keep in mind is that angles *wrap*. Hence, the topology of the \"map\" in the orientation dimension is like a torus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sampling-based representation is *much* easier: we just add a value $\\theta$ to each sample, or, even better, uses `gtsam.Pose2` samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Gaussian densities over pose/orientation are not actually trivial to reason over, and we will postpone discussion of this to the next chapter."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S51_diffdrive_state.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
