{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S35_vacuum_decision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JoW4C_OkOMhe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U gtbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gtsam\n",
    "import pandas as pd\n",
    "\n",
    "import gtbook\n",
    "import gtbook.display\n",
    "from gtbook import vacuum\n",
    "from gtbook.discrete import Variables\n",
    "VARIABLES = Variables()\n",
    "def pretty(obj): \n",
    "    return gtbook.display.pretty(obj, VARIABLES)\n",
    "def show(obj, **kwargs): \n",
    "    return gtbook.display.show(obj, VARIABLES, **kwargs)\n",
    "\n",
    "# From section 3.2:\n",
    "N = 5\n",
    "X = VARIABLES.discrete_series(\"X\", range(1, N+1), vacuum.rooms)\n",
    "A = VARIABLES.discrete_series(\"A\", range(1, N), vacuum.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "> For controlled Markov chains, planning is the process of choosing the control inputs. This leads to\n",
    "the concept of Markov decision processes (MDPs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div align='center'>\n",
       "        <img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S35-iRobot%20vacuuming%20robot-04.jpg?raw=1' style='height:256 width:100%'/>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gtbook.display import randomImages\n",
    "from IPython.display import display\n",
    "display(randomImages(3, 5, \"steampunk\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in this chapter, we described how conditional probability distributions can be used to model uncertainty\n",
    "in the effects of actions. We defined the belief state $b_{t+1}$ to be the posterior probability distribution\n",
    "for the state at time $t+1$ given the sequence of actions $a_1 \\dots a_t$.\n",
    "In every example, the sequence of actions was predetermined, and we merely calculated probabilities\n",
    "associated with performing these actions from some specified initial state, described\n",
    "by a probability distribution $P(X_1)$.\n",
    "\n",
    "In this chapter, we consider the problem of choosing which actions to execute.\n",
    "Making these decisions requires that we have quantitative criteria for evaluating actions and their effects.\n",
    "We encode these criteria using *reward functions*.\n",
    "Because the effects of actions are uncertain, it is not possible to know the reward\n",
    "that will be obtained by executing a specific action (or a sequence of actions). Thus, we will again\n",
    "invoke the concept of expectation to compute expected future benefits of applying actions.\n",
    "\n",
    "As we work through these concepts, it will rapidly become apparent that executing a predefined sequence of actions\n",
    "is not the best way to face the future.\n",
    "Suppose, for example, that our vacuuming robot wants to move from the office to the living room.\n",
    "Because it is not possible to know how many times we should execute the move right action\n",
    "to arrive to the hallway, we might construct a sequence of actions\n",
    "to move right many times (to increase the probability of arriving to the dining room),\n",
    "then move up many times (to increase the probability of arriving to the kitchen),\n",
    "then move left many times (to increase the probability of arriving to the living room).\n",
    "\n",
    "This kind of plan might make sense if the robot is unable to know its current location.\n",
    "For example, moving for a long time to the right should eventually bring the robot to the dining room with\n",
    "high probability,\n",
    "but if the robot moves to the right for a short time, it could remain in the office,\n",
    "arrive to the hallway, or arrive to the dining room.\n",
    "\n",
    "In contrast, if the robot is able to know its location (using perception), it can act\n",
    "opportunistically when it reaches the hallway, and immediately move up.\n",
    "The key idea here is that the optimal action at any moment in time depends\n",
    "on the state in which the action is executed.\n",
    "The recipe of which action to execute in each state is called a *policy*,\n",
    "and determining optimal policies is the main goal for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Functions\n",
    "\n",
    "> Stochastic actions plus a reward make an MDP.\n",
    "\n",
    "How should our robot evaluate the effects of actions?\n",
    "One simple approach might be to specify the robot's goal, and then determine\n",
    "how effective an action might be with respect to that goal,\n",
    "e.g., by using the conditional probability tables for the various actions.\n",
    "If, for example, the goal is to arrive to the living room, we could evaluate\n",
    "the probability that a particular action would achieve the goal.\n",
    "\n",
    "The approach above has several limitations.  First, if the robot is not currently in a room\n",
    "that is adjacent to the living room, it is not clear how to measure the possible progress\n",
    "toward the living room for a specific action.\n",
    "If, for example, the robot is in the dining room, would it be better to move\n",
    "to the kitchen or to the hallway as an intermediate step toward the living room?\n",
    "Second, this approach does not allow the robot to consider\n",
    "benefits that could arise from visiting other states.\n",
    "For example, even though the living room is the goal, arriving to the kitchen might not be\n",
    "such a bad outcome, if for example, the kitchen floor is also in need of cleaning.\n",
    "Finally, it may be the case that the benefit of executing an action depends\n",
    "not only on the destination, but also on the state in which the action\n",
    "is executed.\n",
    "For example, entering the living room from the hallway might be less desirable\n",
    "than entering from the kitchen (if, for example, guests are likely to arrive\n",
    "in the entrance hallway)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward functions provide a very general solution that addresses all of these limitation.\n",
    "Let $\\cal X$ denote the set of states and $\\cal A$ the set of actions.\n",
    "The reward function, $R : {\\cal X} \\times {\\cal A} \\times {\\cal X} \\rightarrow \\mathbb{R}$,\n",
    "assigns a numeric reward to specific state transitions under specific actions.\n",
    "In particular, $R(x_t, a_t, x_{t+1})$ is the reward obtained by arriving to state\n",
    "$x_{t+1}$ from state $x_t$ as a result of executing action $a_t$.\n",
    "\n",
    "In what follows we assume that the reward function is time-invariant, i.e., the benefit of moving up from\n",
    "the hallway to the living room does not change as time passes.\n",
    "This allows us to use the more compact notation\n",
    "$R(x, a, x')$ to represent the award obtained by arriving to (the next) state $x'$\n",
    "by executing action $a$ in state $x$. We will frequently use this notation in the remainder of the section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This form for the reward function is very general, allowing us to encode context dependent rewards\n",
    "that depend on the state in which an action is executed.\n",
    "It is also common to specify rewards merely as a function of state.\n",
    "In this case, we denote by $R(x)$ the reward obtained for arriving to state $x$, regardless\n",
    "of the action that was applied, and regardless of the previous state.\n",
    "An example of such a reward function is given below, implemented as a python function.\n",
    "It simply returns a reward of 10 upon entering the living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def reward_function(state:int, action:int, next_state:int):\n",
    "    \"\"\"Reward that returns 10 upon entering the living room.\"\"\"\n",
    "    return 10.0 if next_state == \"Living Room\" else 0.0\n",
    "\n",
    "print(reward_function(\"Kitchen\", \"L\", \"Living Room\"))\n",
    "print(reward_function(\"Kitchen\", \"L\", \"Kitchen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Reward\n",
    "\n",
    "> Maximizing immediate expected reward leads to greedy planning.\n",
    "\n",
    "Because the effects of actions are uncertain,\n",
    "there is no way for the robot to know definitively the reward that it will obtain\n",
    "by executing a specific action $a$ in state $x$.\n",
    "In this case, we can use the **expected reward** as a surrogate.\n",
    "As we have seen in the previous chapter, this expectation will provide a good \n",
    "estimate of the average reward that will be received if action $a$ is executed from state $x$ may times,\n",
    "even if it provides no guarantees about the reward that will be obtained by any specific moment in time.\n",
    "\n",
    "We denote the expected reward for executing action $a$ in state $x$\n",
    "by $\\bar{R}(x,a)$,\n",
    "and its value is obtained by evaluating the expectation\n",
    "\n",
    "$$\n",
    "\\bar{R}(x,a) \\doteq E[R(x,a,X')] = \\sum_{x'} P(x'|x,a) R(x, a, x')\n",
    "$$\n",
    "\n",
    "Note that we use the upper case $X'$ to indicate that the expectation is taken with\n",
    "respect to the random next state.\n",
    "Accordingly, the sum is over all possible next states, and the reward for each next\n",
    "state is weighted by the probability of arriving to that state from state $x$ by executing action $a$.\n",
    "\n",
    "<!-- In some cases (e.g., when the transition probabilities are *known*),\n",
    "it can be convenient to work only with the expected reward, instead of dealing wit $R(x,a,x')$\n",
    "which some texts do. However, below we will continue to work with the most general formulation. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To *calculate* the expected reward in code, it would be nice to rely on functions that do string compares, which is not very efficient. Since states and actions are finite (and of low cardinality), we can create a multidimensional array $R(x,a,x')$ that the rewards for every possible transition $x,a \\rightarrow x'$. We use the handy GTSAM `enumerate` method below to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional = gtsam.DiscreteConditional((2,5), [(0,5), (1,4)], vacuum.action_spec)\n",
    "R = np.empty((5, 4, 5), float)\n",
    "for assignment, _ in conditional.enumerate():\n",
    "    x, a, y = assignment[0], assignment[1], assignment[2]\n",
    "    R[x, a, y] = 10.0 if y == vacuum.rooms.index(\"Living Room\") else 0.0\n",
    "\n",
    "# For example, taking action \"L\" in \"Kitchen\":\n",
    "R[vacuum.rooms.index(\"Kitchen\"), vacuum.action_space.index(\"L\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is simple to interpret: if the robot\n",
    "arrives to the living room (index 0) the robot obtains a reward of 10, otherwise 0. \n",
    "\n",
    "Note that $R$ is a *multidimensional* array, with shape $(5, 4, 5)$, corresponding to the dimensionality of the arguments of the reward function $R(x,a,x')$.\n",
    "We can do the same with the transition probabilities, creating a multidimensional array $T$ with an entry for every possible transition $x \\rightarrow a \\rightarrow x'$ with probability $P(x'|a,x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 0.2, 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.empty((5, 4, 5), float)\n",
    "for assignment, P_y_given_ax in conditional.enumerate():\n",
    "    x, a, y = assignment[0], assignment[1], assignment[2]\n",
    "    T[x, a, y] = P_y_given_ax\n",
    "\n",
    "# For example, taking action \"L\" in \"Kitchen\":\n",
    "T[vacuum.rooms.index(\"Kitchen\"), vacuum.action_space.index(\"L\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we end up in the living room with 80% chance, while staying in place (the Kitchen) 20% of the time.\n",
    "\n",
    "Then, finally, calculating the expected reward $\\bar{R}(x,a)$ for a particular state-action pair $(x,a)$ becomes a simple dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (Kitchen, L) = 8.0\n"
     ]
    }
   ],
   "source": [
    "x, a = vacuum.rooms.index(\"Kitchen\"), vacuum.action_space.index(\"L\")\n",
    "print(f\"Expected reward (Kitchen, L) = {np.dot(T[x,a], R[x,a])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes total sense: if we stay in the kitchen, we get zero reward, but *if* we succeed to move to the living room, we receive a reward of 10. This happens only in 80% of the cases, though, so the *expected reward* is only 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Note that this computation can also be realized using belief states.  In\n",
    "particular, if encode the reward function as a vector (as above),\n",
    "say $R = [10, 0, 0, 0, 0]^T$,\n",
    "the the expected reward at time $t+1$ is given by\n",
    "\n",
    "$$E[ R(X_t, a, X_{t+t})] = b_{t+1} R = \\left( b_t M_{a}\\right) R\n",
    "$$\n",
    "\n",
    "in which $M_{a}$ denotes the transition probability matrix associated\n",
    "to action $ a$. Note that in this case, we the expected reward is computed for a given\n",
    "prior distribution on $X_t$. If we know with certainty that $X_t =x$, the belief\n",
    "vector would have a $1$ in the entry corresponding to the current room, and zero's for\n",
    "all other entries.\n",
    "Therefore, this form is somewhat more general than that given above. -->\n",
    "\n",
    "Equipped with the definition of expected reward, we can introduce a first, **greedy planning** algorithm:\n",
    "Given the current belief state $b_t$, execute the action that maximizes the expected reward:\n",
    "\n",
    "$$\n",
    "a^* = \\arg  \\max_{a \\in {\\cal A}} E[ R(X_t, a, X_{t+t})]\n",
    "$$\n",
    "\n",
    " <!-- = \\arg \\max_{a \\in {\\cal A}}\\left( b_t M_{A}\\right) R -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the \"Kitchen\", we can calculate the expected award for all actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (Kitchen, L) = 8.0\n",
      "Expected reward (Kitchen, R) = 0.0\n",
      "Expected reward (Kitchen, U) = 0.0\n",
      "Expected reward (Kitchen, D) = 0.0\n"
     ]
    }
   ],
   "source": [
    "x = vacuum.rooms.index(\"Kitchen\")\n",
    "for a in range(4):\n",
    "    print(f\"Expected reward ({vacuum.rooms[x]}, {vacuum.action_space[a]}) = {np.dot(T[x,a], R[x,a])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that $8.0$ is the best we can do by greedily choosing a single action, as the expected *immediate* reward for any other action is zero. Hence, whenever we find ourselves in the Kitchen, a good course of action is to always go left!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility for a Defined Sequence of Actions\n",
    "\n",
    "> Utility is the total discounted award over finite or infinite horizons.\n",
    "\n",
    "The greedy strategy above focuses on the immediate benefit of applying an action.\n",
    "For our robot, and for most robots operating in the real world, it is important to perform\n",
    "effectively over a prolonged period of time, not merely for the next instant.\n",
    "In order to maximize long-term benefit, instead of considering only the next-stage reward (as the greedy strategy does)\n",
    "we could consider the sum of all rewards achieved in the future.\n",
    "There are two immediate disadvantages to a direct implementation of this approach:\n",
    "\n",
    "- First, because the effects of actions are uncertain, it is likely that the further we look\n",
    "into the future, the more uncertain we will be about the robot's anticipated state.\n",
    "Therefore, it makes sense to discount our consideration of rewards that might occur far into the future,\n",
    "say at times $X_{t+T}$, for increasing values of $T$.\n",
    "\n",
    "- Second, it is often convenient to reason with an infinite time horizon, e.g., consider the case when the robot will operate forever.\n",
    "While this is certainly not a realistic expectation,\n",
    "reasoning over an infinite time horizon often simplifies the mathematical complexities of planning into the future.\n",
    "If we merely compute the sum of all future rewards, this sum will diverge to infinity as $T$ approaches infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deal with both of these disadvantages by multiplying the reward at time $t + T$ by a discounting\n",
    "factor $\\gamma^T$, with $0 < \\gamma \\leq 1$.\n",
    "We refer to $\\gamma$ as the **discount factor**\n",
    "and to the term $\\gamma^T R(x_t,a_t,x_{t+T})$ as a **discounted reward.**\n",
    "Note that for $\\gamma = 1$, there is no discount, and all future rewards are treated with equal weight.\n",
    "\n",
    "Suppose the robot executes a sequence of actions, $a_1, \\dots, a_n$,\n",
    "starting in state  $X_1=x_1$, and passes through\n",
    "state sequence $x_1,x_2,x_3\\dots x_{n+1}$.\n",
    "We define the utility function $U: {\\cal A}^n \\times {\\cal X}^{n+1} \\rightarrow \\mathbb{R}$ as\n",
    "\n",
    "$$\n",
    "U(a_1, \\dots, a_n, x_1, \\dots x_{n+1}) =\n",
    "R(x_1,a_1, x_2) + \\gamma R(x_2, a_2, x_3) + \\gamma^2 R(x_3, \\pi(x_3), x_4) + \\dots \\gamma^{n} R(x_{n}, a_{n}, x_{n+1})$$\n",
    "\n",
    "We can write this more compactly as the summation\n",
    "\n",
    "$$\n",
    "U(a_1, \\dots, a_n, x_1, \\dots x_{n+1}) =\n",
    "\\sum_{t=1}^{n} = \\gamma^{t-1} R(x_t, a_t, x_{t+1}) \n",
    "$$\n",
    "\n",
    "For $n < \\infty$, we refer to this as a finite-horizon utility function.\n",
    "Note that influence of future rewards decreases exponentially with the time horizon,\n",
    "and that the use of $\\gamma^{t-1}$ ensures that the sum will converge for the infinite horizon case\n",
    "(under mild assumptions about $R$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression above is defined for a specific sequence of actions and a specific sequence of states.\n",
    "When planning, as we have noted above, we are unable to know with certainty the future states.\n",
    "We can, again, deal with this difficulty by computing the expected utility for a\n",
    "given sequence of actions, \n",
    "$E[U(a_1, \\dots, a_n, X_1, \\dots X_n)]$.\n",
    "We can now formulate a slightly more sophisticated version of our planning problem:\n",
    "\n",
    "\n",
    "$$ a_1^*, \\dots a_n^* = \\arg  \\max_{a_1 \\dots a_n \\in {\\cal A}^n} E[U(a_1, \\dots, a_n, X_1, \\dots X_{n+1})]\n",
    "$$\n",
    "\n",
    "As formulated above, this problem could be solved by merely enumerating every possible action sequence,\n",
    "and choosing the sequence that maximizes the expectation.\n",
    "Obviously this is not a computationally tractable approach.\n",
    "Not only does the number of possible action sequences grow exponentially with the time horizon $n$,\n",
    "but the computation of the expectation for a specific action sequence is also computationally heavy.\n",
    "We can, however, approximate this optimization process using the concept of rollouts, as we will now see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating Expected Utility Using Control Tape Rollouts\n",
    "\n",
    "> Rollin', rollin', rollin'...\n",
    "\n",
    "Consider the computation required to determine the expected utility for a sequence of only two actions:\n",
    "\n",
    "$$\n",
    "E[U(a_1,a_2, X_1, X_2, X_3)] = E[R(X_1, a_1, X_2) + \\gamma R(X_2, a_2, X_3)]\n",
    "$$\n",
    "\n",
    "Computing the expectation requires summing over all combinations of values for states\n",
    "$X_1, X_2, X_3$.  Clearly, as $n$ becomes large, this is not a tractable computation.\n",
    "We can approximate this computation by recalling the relationship between\n",
    "the expected value for a probability distribution and the average value over\n",
    "many realizations of the underlying random process.\n",
    "Namely, as we have seen before, the expected of a random variable (in this case the value of the utility function)\n",
    "corresponds to what we expect to observe as the average of that random variable over many trials.\n",
    "This immediately suggests an approximation algorithm: \n",
    "\n",
    "- Generate many sample trajectories\n",
    "for the action sequence $a_1, \\dots, a_n$;\n",
    "- compute the average of the discounted rewards over\n",
    "these sample trajectories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A specific sequence of actions, $a_1, \\dots, a_n$ is sometimes called a **control tape**.\n",
    "The process of evaluating the discounted reward for such a sequence is called a **rollout** for the control tape.\n",
    "Each rollout produces one sample trajectory, and one corresponding discounted reward.\n",
    "As an example, suppose the robot starts in the office executes the sequence\n",
    "$a_1, a_2, a_3, a_4$ = R,U,L,L (i.e., the robot executes actions *move right, move up, move left, move left*)\n",
    "in an attempt to reach the living room.\n",
    "\n",
    "Because we took care to specify the Markov chain above in reverse topological order, we can then use the GTSAM method `sample` to do the rollouts for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"350pt\" height=\"116pt\"\n viewBox=\"0.00 0.00 350.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 346,-112 346,4 -4,4\"/>\n<!-- var4683743612465315841 -->\n<g id=\"node1\" class=\"node\">\n<title>var4683743612465315841</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"54,-108 0,-108 0,-72 54,-72 54,-108\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">A1</text>\n</g>\n<!-- var6341068275337658370 -->\n<g id=\"node6\" class=\"node\">\n<title>var6341068275337658370</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X2</text>\n</g>\n<!-- var4683743612465315841&#45;&gt;var6341068275337658370 -->\n<g id=\"edge2\" class=\"edge\">\n<title>var4683743612465315841&#45;&gt;var6341068275337658370</title>\n<path fill=\"none\" stroke=\"black\" d=\"M45.17,-71.83C54.73,-62.27 66.53,-50.47 76.65,-40.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"79.4,-42.55 83.99,-33.01 74.45,-37.6 79.4,-42.55\"/>\n</g>\n<!-- var4683743612465315842 -->\n<g id=\"node2\" class=\"node\">\n<title>var4683743612465315842</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"126,-108 72,-108 72,-72 126,-72 126,-108\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">A2</text>\n</g>\n<!-- var6341068275337658371 -->\n<g id=\"node7\" class=\"node\">\n<title>var6341068275337658371</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"171\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X3</text>\n</g>\n<!-- var4683743612465315842&#45;&gt;var6341068275337658371 -->\n<g id=\"edge4\" class=\"edge\">\n<title>var4683743612465315842&#45;&gt;var6341068275337658371</title>\n<path fill=\"none\" stroke=\"black\" d=\"M117.17,-71.83C126.73,-62.27 138.53,-50.47 148.65,-40.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.4,-42.55 155.99,-33.01 146.45,-37.6 151.4,-42.55\"/>\n</g>\n<!-- var4683743612465315843 -->\n<g id=\"node3\" class=\"node\">\n<title>var4683743612465315843</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"198,-108 144,-108 144,-72 198,-72 198,-108\"/>\n<text text-anchor=\"middle\" x=\"171\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">A3</text>\n</g>\n<!-- var6341068275337658372 -->\n<g id=\"node8\" class=\"node\">\n<title>var6341068275337658372</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"243\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"243\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X4</text>\n</g>\n<!-- var4683743612465315843&#45;&gt;var6341068275337658372 -->\n<g id=\"edge6\" class=\"edge\">\n<title>var4683743612465315843&#45;&gt;var6341068275337658372</title>\n<path fill=\"none\" stroke=\"black\" d=\"M189.17,-71.83C198.73,-62.27 210.53,-50.47 220.65,-40.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"223.4,-42.55 227.99,-33.01 218.45,-37.6 223.4,-42.55\"/>\n</g>\n<!-- var4683743612465315844 -->\n<g id=\"node4\" class=\"node\">\n<title>var4683743612465315844</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"270,-108 216,-108 216,-72 270,-72 270,-108\"/>\n<text text-anchor=\"middle\" x=\"243\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">A4</text>\n</g>\n<!-- var6341068275337658373 -->\n<g id=\"node9\" class=\"node\">\n<title>var6341068275337658373</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"315\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X5</text>\n</g>\n<!-- var4683743612465315844&#45;&gt;var6341068275337658373 -->\n<g id=\"edge8\" class=\"edge\">\n<title>var4683743612465315844&#45;&gt;var6341068275337658373</title>\n<path fill=\"none\" stroke=\"black\" d=\"M261.17,-71.83C270.73,-62.27 282.53,-50.47 292.65,-40.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"295.4,-42.55 299.99,-33.01 290.45,-37.6 295.4,-42.55\"/>\n</g>\n<!-- var6341068275337658369 -->\n<g id=\"node5\" class=\"node\">\n<title>var6341068275337658369</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n</g>\n<!-- var6341068275337658369&#45;&gt;var6341068275337658370 -->\n<g id=\"edge1\" class=\"edge\">\n<title>var6341068275337658369&#45;&gt;var6341068275337658370</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.22,-18C56.64,-18 59.11,-18 61.6,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"61.74,-21.5 71.74,-18 61.74,-14.5 61.74,-21.5\"/>\n</g>\n<!-- var6341068275337658370&#45;&gt;var6341068275337658371 -->\n<g id=\"edge3\" class=\"edge\">\n<title>var6341068275337658370&#45;&gt;var6341068275337658371</title>\n<path fill=\"none\" stroke=\"black\" d=\"M126.22,-18C128.64,-18 131.11,-18 133.6,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"133.74,-21.5 143.74,-18 133.74,-14.5 133.74,-21.5\"/>\n</g>\n<!-- var6341068275337658371&#45;&gt;var6341068275337658372 -->\n<g id=\"edge5\" class=\"edge\">\n<title>var6341068275337658371&#45;&gt;var6341068275337658372</title>\n<path fill=\"none\" stroke=\"black\" d=\"M198.22,-18C200.64,-18 203.11,-18 205.6,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"205.74,-21.5 215.74,-18 205.74,-14.5 205.74,-21.5\"/>\n</g>\n<!-- var6341068275337658372&#45;&gt;var6341068275337658373 -->\n<g id=\"edge7\" class=\"edge\">\n<title>var6341068275337658372&#45;&gt;var6341068275337658373</title>\n<path fill=\"none\" stroke=\"black\" d=\"M270.22,-18C272.64,-18 275.11,-18 277.6,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"277.74,-21.5 287.74,-18 277.74,-14.5 277.74,-21.5\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<gtbook.display.show at 0x12528bb80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markovChain = gtsam.DiscreteBayesNet()\n",
    "for k in reversed(range(1,N)):\n",
    "    markovChain.add(X[k+1], [X[k], A[k]], vacuum.action_spec)\n",
    "show(markovChain, hints={\"A\":2, \"X\":1, \"Z\":0}, boxes={A[k][0] for k in range(1,N)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rollout(x1, actions):\n",
    "    \"\"\"Roll out states given actions as a dictionary\"\"\"\n",
    "    dict = actions.copy()\n",
    "    dict[X[1]] = x1\n",
    "    given = VARIABLES.assignment(dict)\n",
    "    return markovChain.sample(given)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute a specific rollout for the control tape R,U,L,L, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class='DiscreteValues'>\n",
       "  <thead>\n",
       "    <tr><th>Variable</th><th>value</th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th>A1</th><td>R</td></tr>\n",
       "    <tr><th>A2</th><td>U</td></tr>\n",
       "    <tr><th>A3</th><td>L</td></tr>\n",
       "    <tr><th>A4</th><td>L</td></tr>\n",
       "    <tr><th>X1</th><td>Office</td></tr>\n",
       "    <tr><th>X2</th><td>Office</td></tr>\n",
       "    <tr><th>X3</th><td>Office</td></tr>\n",
       "    <tr><th>X4</th><td>Office</td></tr>\n",
       "    <tr><th>X5</th><td>Office</td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<gtbook.display.pretty at 0x11394ac70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = {A[1]:\"R\", A[2]:\"U\", A[3]:\"L\", A[4]:\"L\"}\n",
    "rollout = perform_rollout(\"Office\", actions)\n",
    "pretty(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remember that any individual rollout corresponds to a sample trajectory from a stochastic process.\n",
    "If you execute the above code several times, you should observe that the robot does not always arrive to the living room.\n",
    "\n",
    "The code below executes the rollout and computes the corresponding utility for the sample trajectory.\n",
    "Sample trajectories that do not arrive to the living room will have zero utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def reward(rollout, k):\n",
    "    \"\"\"Return state, action, next_state triple for given rollout at time k.\"\"\"\n",
    "    state = rollout[X[k][0]]\n",
    "    action = rollout[A[k][0]]\n",
    "    next_state = rollout[X[k+1][0]]\n",
    "    return R[state, action, next_state]\n",
    "\n",
    "def rollout_reward(rollout, horizon=N, gamma=1.0):\n",
    "    \"\"\"Calculate reward for a given rollout\"\"\"\n",
    "    discounted_rewards = [gamma**(k-1) * reward(rollout,k) for k in range(1,horizon)]\n",
    "    return sum(discounted_rewards)\n",
    "    \n",
    "print(rollout_reward(rollout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes 20 rollouts for the action sequence R,U,L,L, and prints the utility for each sample trajectory.\n",
    "You can see that in many cases the robot fails to arrive to the living room (thus earning zero utility)!\n",
    "This is because each of the first two actions have a 0.2 probability of failure, and if either of these fail, the robot is unable to reach the living room using this control tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.0, 0.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 0.0, 30.0, 0.0, 0.0, 30.0, 30.0, 30.0, 0.0, 30.0, 30.0]\n"
     ]
    }
   ],
   "source": [
    "def control_tape_reward(x1, actions):\n",
    "    \"\"\"Calculate reward given a dictionary of actions\"\"\"\n",
    "    rollout = perform_rollout(x1, actions)\n",
    "    return rollout_reward(rollout)\n",
    "\n",
    "print([control_tape_reward(\"Office\", actions) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the expected utility of the action sequence R,U,L,L simply averages over all 20 rollouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([control_tape_reward(\"Office\", actions) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "- Execute both cells above multiple times and observe the effect.\n",
    "- The rewards above seems to be always either zero or 30. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "> A policy is a function that specifies which action to take in each state.\n",
    "\n",
    "In the example above, the control tape rollout for the action sequence R,U,L,L failed many times.\n",
    "The reason for this failure may seem obvious: the robot executed the same sequence of actions,\n",
    "regardless of the state trajectory.\n",
    "If the robot had been able to choose its actions based on the current state,\n",
    "it would have chosen to move right until it reached the hallway,\n",
    "at which time it would have chosen to move up until reaching the living room.\n",
    "Clearly, the robot would make better choices if it were allowed to dynamically\n",
    "choose which action to execute based on its current state.\n",
    "\n",
    "A **policy**, $\\pi: {\\cal X} \\rightarrow {\\cal A}$ is a mapping from states to actions.\n",
    "Specifying a policy instead of a control tape has the potential to significantly improve the robot's performance, by adapting the action sequence based on the actual state trajectory that occurs during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a fairly intuitive policy. If in the office, move right. If in the dining room or kitchen, move left.\n",
    "If in the hallway or living room, move up.\n",
    "Note we implement policies as a simple list in python, so `pi[0]` is the the action take in state with index 0 (the Living Room):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIGHT = vacuum.action_space.index(\"R\")\n",
    "LEFT  = vacuum.action_space.index(\"L\")\n",
    "UP    = vacuum.action_space.index(\"U\")\n",
    "DOWN  = vacuum.action_space.index(\"D\")\n",
    "\n",
    "reasonable_policy = [UP, LEFT, RIGHT, UP, LEFT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we return an index into the `action_space` to make the following code efficient. \n",
    "Once we have a given policy, $\\pi$, we can compute a policy rollout in a manner analogous to computing\n",
    "control tape rollouts described above.\n",
    "In particular, at each state, instead of sampling from the distribution\n",
    "$P(X_{t+1} | a_t, X_t)$ we sample from the distribution\n",
    "$P(X_{t+1} | \\pi(X_t), X_t)$. In other words, instead of simulating a pre-specified action $a_t$, we choose $a_t = \\pi(X_t)$.\n",
    "\n",
    "Here is a function that computes a rollout given a policy, rather than a control tape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class='DiscreteValues'>\n",
       "  <thead>\n",
       "    <tr><th>Variable</th><th>value</th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th>A1</th><td>R</td></tr>\n",
       "    <tr><th>A2</th><td>U</td></tr>\n",
       "    <tr><th>A3</th><td>U</td></tr>\n",
       "    <tr><th>A4</th><td>U</td></tr>\n",
       "    <tr><th>X1</th><td>Office</td></tr>\n",
       "    <tr><th>X2</th><td>Hallway</td></tr>\n",
       "    <tr><th>X3</th><td>Living Room</td></tr>\n",
       "    <tr><th>X4</th><td>Living Room</td></tr>\n",
       "    <tr><th>X5</th><td>Living Room</td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<gtbook.display.pretty at 0x112f50190>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_rollout(x1, pi, horizon=N):\n",
    "    \"\"\"Roll out states given a policy pi, for given horizon.\"\"\"\n",
    "    rollout = gtsam.DiscreteValues()\n",
    "    x = x1\n",
    "    for k in range(1, horizon):\n",
    "        a = pi[x]\n",
    "        rollout[X[k][0]] = x\n",
    "        rollout[A[k][0]] = a\n",
    "        next_state_distribution = gtsam.DiscreteDistribution(X[k+1], T[x, a])\n",
    "        x = next_state_distribution.sample()\n",
    "    rollout[X[horizon][0]] = x\n",
    "    return rollout\n",
    "\n",
    "pretty(policy_rollout(vacuum.rooms.index(\"Office\"), reasonable_policy, horizon=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we proposed two methods to plan an action sequence,\n",
    "a greedy approach that maximizes the expected reward for executing a single action,\n",
    "and an optimization-based method that chooses a fixed action sequence to maximize the\n",
    "expected utility of the action sequence.\n",
    "We described how to implement the computations required for the latter approach using control tape rollouts.\n",
    "\n",
    "With the introduction of policies, planning is reduced to the search for an appropriate\n",
    "policy.\n",
    "In the best case, this policy would maximize the expected utility with respect to the policy.\n",
    "Because policies are state-dependent, the combinatorics of enumerating all possible\n",
    "policies precludes any approach that attempts to explicitly enumerate candidate policies.\n",
    "Instead, we will focus on methods that explore the utility associated to\n",
    "a policy.\n",
    "As we will now see, this is somewhat more complicated than exploring the utility\n",
    "for a fixed sequence of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Value Function (for a given policy)\n",
    "> The value function $V^\\pi$ measures the expected outcome from each state, under a given policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we defined the utility for a specific sequence of $n$ actions as\n",
    "\n",
    "$$\n",
    "U(a_1, \\dots, a_n, x_1, \\dots x_{n+1}) =\n",
    "R(x_1,a_1, x_2) + \\gamma R(x_2, a_2, x_3) + \\gamma^2 R(x_3, \\pi(x_3), x_4) + \\dots \\gamma^{n} R(x_{n}, a_{n}, x_{n+1})$$\n",
    "\n",
    "and used the expected utility, $E[U(a_1, \\dots, a_n, X_1, \\dots X_n)]$ as a quantitative measure\n",
    "of the efficacy of the specific action sequence.\n",
    "We can apply this same type of reasoning for a policy, $\\pi$.\n",
    "When evaluating policies, it is typical to use a discounted\n",
    "reward over an infinite time horizon.\n",
    "In this case, we define the **value function for policy $\\pi$**,\n",
    "$V^\\pi:{\\cal X} \\rightarrow \\mathbb{R}$ as\n",
    "\n",
    "\n",
    "$$\n",
    "V^\\pi(x_1) \\doteq E [R(x_1, \\pi(x_1), X_2) + \\gamma R(X_2, \\pi(X_2), X_3) + \\gamma^2 R(X_3, \\pi(X_3), X_4) + \\dots]\n",
    "$$\n",
    "\n",
    "where the expectation is taken over the possible values\n",
    "of the states $X_2, X_2, X_3 \\dots$.\n",
    "Note that the policy $\\pi$ is deterministic, but that $\\pi(X_i)$ is the random action that results\n",
    "from applying the deterministic policy to the stochastic state $X_i$.\n",
    "\n",
    "In the definition for $V^\\pi$, the argument is given as $x_1$, the initial state.\n",
    "We can easily generalize this to arbitrary time steps as\n",
    "\n",
    "$$\n",
    "V^\\pi(x_t) \\doteq E [R(x_t, \\pi(x_t), X_{t+1}) + \\gamma R(X_{t+1}, \\pi(X_{t+1}), X_{t+2}) + \\gamma^2 R(X_{t+2}, \\pi(X_{t+2}), X_{t+3}) + \\dots]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function for a policy can be written in a nice recursive form\n",
    "that can be obtained by the following derivation.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^\\pi(x_1) &=\n",
    "E [R(x_1, \\pi(x_1), X_2) + \\gamma R(X_2, \\pi(X_2), X_3) + \\gamma^2 R(X_3, \\pi(X_3), X_4) + \\dots] \\\\\n",
    "&=\n",
    " \\sum_{x_2} P(x_2|x_1, \\pi(x_1)) \\{R(x_1, \\pi(x_1), x_2) + E [\\gamma R(x_2, \\pi(x_2), X_3) + \\gamma^2 R(X_3, \\pi(X_3), X_4) + \\dots]\\}\\\\\n",
    "&= \\sum_{x_2} P(x_2|x_1, \\pi(x_1)) \\{R(x_1, \\pi(x_1), x_2) + \\gamma V^\\pi(x_2)\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the second line is obtained by explicitly writing the summation for the expectation\n",
    "taken with respect to the random state $X_2$,\n",
    "and the third line is obtained by noticing that\n",
    "$V^\\pi(x_2) =E [\\gamma R(x_2, \\pi(x_2), X_3) + \\gamma^2 R(X_3, \\pi(X_3), X_4) + \\dots]$.\n",
    "\n",
    "We can apply the distributivity property to this expression to obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^\\pi(x_1) &= \\sum_{x_2} P(x_2|x_1, \\pi(x_1)) R(x_1, \\pi(x_1), x_2) + \\gamma  \\sum_{x_2} P(x_2|x_1, \\pi(x_1))  V^\\pi(x_2) \\\\\n",
    "&= \\bar{R}(x_1,\\pi(x_1)) + \\gamma \\sum_{x_2} P(x_2|x_1, \\pi(x_1)) V^\\pi(x_2)\n",
    "\\end{align*}\n",
    "$$\n",
    " \n",
    "in which the term $\\bar{R}(x_1,\\pi(x_1))$ is the expected reward for applying action $a = \\pi(x_1)$ in state $x_1$.\n",
    "This can be computed directly using the reward function and transition probabilities.\n",
    "By now substituting $x$ for $x_1$, and $x'$ for $x_{2}$ we can generalize this expression\n",
    "to apply to the state $x$ at any arbitrary time:\n",
    "\n",
    "$$V^\\pi(x) = \\bar{R}(x,\\pi(x)) + \\gamma \\sum_{x'} P(x'|x, \\pi(x)) V^\\pi(x')$$\n",
    "\n",
    "This has a very nice interpretation: the value of a state under a given policy is the expected reward $\\bar{R}(x,\\pi(x))$ under that policy, *plus* the discounted expected value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought exercise\n",
    "\n",
    " Without a discount factor, we would always be hopeful that if we take one more step, we will find a pot of gold. Reflect on what various values for the discount factor mean in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the Value Function\n",
    "\n",
    "> We can use rollouts to approximate the value function as well.\n",
    "\n",
    "Just as we approximated the expected utility of an action sequence\n",
    "using control tape rollouts,\n",
    "we can approximate the value function by sampling over a number of policy rollouts.\n",
    "This process provides a *Monte Carlo approximation* of the value function for a given policy. \n",
    "Of course we cannot apply a policy rollout over an infinite time horizon, so we apply\n",
    "the rollout only to some finite number of time steps, say $N_{\\rm{ro}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is an approximation in *two* ways: \n",
    "1. We approximate the expectation by averaging over $N_{\\rm{Samples}}$ sample trajectories.\n",
    "2. We only roll out for  $N_{\\rm{ro}}$ steps.\n",
    "We can improve on this by increasing the number of samples or by increasing the horizon, but at a cost linear in their product, i.e., \n",
    "$O(N_{\\rm{ro}} N_{\\rm{Samples}})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might worry that evaluating sample paths of length $N_{\\rm{ro}}$ could lead to significant errors\n",
    "in our approximation.\n",
    "In fact, it is easy to determine an upper bound on this error.\n",
    "Since $R(x, a, x') $ is finite,\n",
    "we know that there is some upper bound $R_{\\rm{max}}$ such that $R(x, a, x') < R_{\\rm{max}}$, for all\n",
    "possible $x,a,x'$.\n",
    "We can use this fact to find a bound on $V^\\pi$:\n",
    "\n",
    "$$V^\\pi(x_t)\n",
    "\\leq \\max_{x_t, x_{t+1}, \\dots} \\sum_{i=0}^\\infty \\gamma ^i R(x_{t+i},\\pi(x_{t+i}), x_{t+i+1})\n",
    "\\leq \\sum_{i=0}^\\infty \\gamma ^i R_{\\rm{max}} = \\frac{R_{\\rm{max}}}{1 - \\gamma}$$\n",
    "\n",
    "in which the final term applies for $0 < \\gamma < 1$.\n",
    "This expression can then be used to bound the error \n",
    "\n",
    "$$\n",
    "V^\\pi(x) - \\sum_{i=0}^{N_{\\rm{ro}}}\\gamma ^i R(x_i,\\pi(xi), x_{i+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have functions to sample a policy rollout *and* to calculate the value of a rollout, the code is simple enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(Living Room) ~ 34.39\n",
      "V(Kitchen) ~ 29.68\n",
      "V(Office) ~ 20.88\n",
      "V(Hallway) ~ 27.59\n",
      "V(Dining Room) ~ 15.83\n"
     ]
    }
   ],
   "source": [
    "def approximate_value_function(x1, pi, nr_samples=10, horizon=N, gamma=0.9):\n",
    "    \"\"\" Approximate the value function by performing `nr_samples` rollouts \n",
    "        starting from x1, and averaging the result.\n",
    "    \"\"\"\n",
    "    rollouts = [policy_rollout(x1, pi, horizon) for _ in range(nr_samples)]\n",
    "    rewards = [rollout_reward(rollout, horizon, gamma) for rollout in rollouts]\n",
    "    return sum(rewards)/nr_samples\n",
    "\n",
    "nr_samples=10\n",
    "for x1, room in enumerate(vacuum.rooms):\n",
    "    V_x1 = approximate_value_function(x1, reasonable_policy, nr_samples)\n",
    "    print(f\"V({room}) ~ {V_x1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation was done with a horizon of $N=5$ and 10 samples. Of course, we can use more samples and a longer horizon to obtain a much more accurate estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(Living Room) ~ 99.43\n",
      "V(Kitchen) ~ 97.25\n",
      "V(Office) ~ 84.82\n",
      "V(Hallway) ~ 97.37\n",
      "V(Dining Room) ~ 86.13\n"
     ]
    }
   ],
   "source": [
    "nr_samples = 100\n",
    "horizon = 50\n",
    "X = VARIABLES.discrete_series('X', range(1, horizon+1), vacuum.rooms)\n",
    "A = VARIABLES.discrete_series('A', range(1, horizon), vacuum.action_space)\n",
    "for x1, room in enumerate(vacuum.rooms):\n",
    "    V_x1 = approximate_value_function(x1, reasonable_policy, nr_samples, horizon)\n",
    "    print(f\"V({room}) ~ {V_x1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing The Value Function*\n",
    "\n",
    "> For any fixed policy we can *exactly* compute the value function.\n",
    "\n",
    "We can compute the exact value of $V^\\pi$ by solving a system of linear equations.\n",
    "Recall our recursive definition of the value function:\n",
    "\n",
    "$$V^\\pi(x) = \\bar{R}(x,\\pi(x)) + \\gamma \\sum_{x'} P(x'|x, \\pi(x)) V^\\pi(x')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation holds for every possible value $x$ for the state.\n",
    "Hence, if there are $n$ possible states, we obtain $n$ linear equations in $n$ unknowns.\n",
    "Each of these $n$ equations is obtained by evaluating $V^\\pi(x)$ for a specific value of $x$.\n",
    "Collecting the unknown $V^\\pi$ terms on the left hand side and the known $\\bar{R}(x,\\pi(x))$ \n",
    "terms on the right hand side, we obtain\n",
    "\n",
    "$$V^\\pi(x) - \\gamma \\sum_{x'} P(x'|x, \\pi(x)) V^\\pi(x') = \\bar{R}(x,\\pi(x))$$\n",
    "\n",
    " \n",
    "To make this explicit yet concise for our vacuum cleaning robot example,\n",
    "let us define the *scalar* $T^\\pi_{xy}\\doteq P(y|x,\\pi(x))$ as the transition probability from state $x$ to state $y$ under policy $\\pi$. \n",
    "In addition, we use the abbreviations L,K,O,H, and D for the rooms, and use the shorthand $V^\\pi_x\\doteq V^\\pi(x)$ for the value of state $x$ under policy $\\pi$. \n",
    "Using this notation, we can evaluate the above expression for \n",
    "r $x = L$, we obtain\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "V^\\pi(L) - \\gamma \\sum_{x'\\in {L,K,O,H,D}} T^\\pi_{Lx'} V^\\pi_{x'} & = \\bar{R}(L,\\pi(L)) \\\\\n",
    "V^\\pi_{L}\n",
    "- \\gamma T^\\pi_{LL} V^\\pi_{L}\n",
    "- \\gamma T^\\pi_{LK} V^\\pi_{K} \n",
    "- \\gamma T^\\pi_{LO} V^\\pi_{O} \n",
    "- \\gamma T^\\pi_{LH} V^\\pi_{H} \n",
    "- \\gamma T^\\pi_{LD} V^\\pi_{D} \n",
    "&= \\bar{R}(L,\\pi(L)) \\\\\n",
    "(1 - \\gamma T^\\pi_{LL}) V^\\pi_{L}\n",
    "- \\gamma T^\\pi_{LK} V^\\pi_{K} \n",
    "- \\gamma T^\\pi_{LO} V^\\pi_{O} \n",
    "- \\gamma T^\\pi_{LH} V^\\pi_{H} \n",
    "- \\gamma T^\\pi_{LD} V^\\pi_{D} \n",
    "&= \\bar{R}(L,\\pi(L))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this same process for each of the five rooms,\n",
    "we obtain the following five equations:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(1 - \\gamma T^\\pi_{LL}) V^\\pi_{L}\n",
    "- \\gamma T^\\pi_{LK} V^\\pi_{K} \n",
    "- \\gamma T^\\pi_{LO} V^\\pi_{O} \n",
    "- \\gamma T^\\pi_{LH} V^\\pi_{H} \n",
    "- \\gamma T^\\pi_{LD} V^\\pi_{D} \n",
    "&= \\bar{R}(L,\\pi(L))\n",
    "\\\\\n",
    "- \\gamma T^\\pi_{KL} V^\\pi_{L}\n",
    "+ (1 - \\gamma T^\\pi_{KK}) V^\\pi_{K} \n",
    "- \\gamma T^\\pi_{KO} V^\\pi_{O} \n",
    "- \\gamma T^\\pi_{KH} V^\\pi_{H} \n",
    "- \\gamma T^\\pi_{KD} V^\\pi_{D} \n",
    "&= \\bar{R}(K,\\pi(K))\n",
    "\\\\\n",
    "- \\gamma T^\\pi_{OL} V^\\pi_{L}\n",
    "- \\gamma T^\\pi_{OK} V^\\pi_{K} \n",
    "+ (1 - \\gamma T^\\pi_{OO}) V^\\pi_{O} \n",
    "- \\gamma T^\\pi_{OH} V^\\pi_{H} \n",
    "- \\gamma T^\\pi_{OD} V^\\pi_{D} \n",
    "&= \\bar{R}(O,\\pi(O))\n",
    "\\\\\n",
    "- \\gamma T^\\pi_{HL} V^\\pi_{L}\n",
    "- \\gamma T^\\pi_{HK} V^\\pi_{K} \n",
    "- \\gamma T^\\pi_{HO} V^\\pi_{O} \n",
    "+ (1 - \\gamma T^\\pi_{HH}) V^\\pi_{H} \n",
    "- \\gamma T^\\pi_{HD} V^\\pi_{D} \n",
    "&= \\bar{R}(H,\\pi(H))\n",
    "\\\\\n",
    "- \\gamma T^\\pi_{DL} V^\\pi_{L}\n",
    "- \\gamma T^\\pi_{DK} V^\\pi_{K} \n",
    "- \\gamma T^\\pi_{DO} V^\\pi_{O} \n",
    "- \\gamma T^\\pi_{DH} V^\\pi_{H} \n",
    "+ (1 - \\gamma T^\\pi_{DD}) V^\\pi_{D} \n",
    "&= \\bar{R}(D,\\pi(D))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The unknowns in these equations are \n",
    "$V^\\pi_{L}, V^\\pi_{K}, V^\\pi_{O}, V^\\pi_{H}, V^\\pi_{D}$. All of the other terms are\n",
    "either transition probabilities or expected rewards, whose values are either given,\n",
    "or can easily be computed.\n",
    "\n",
    "In code, this becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_system(pi, gamma=0.9):\n",
    "    \"\"\"Calculate A, b matrix of linear system for value computation.\"\"\"\n",
    "    b = np.empty((5,), float)\n",
    "    AA = np.empty((5,5), float)\n",
    "    for x, room in enumerate(vacuum.rooms):\n",
    "        a = pi[x] # action under policy\n",
    "        b[x] = T[x,a] @ R[x,a] # expected reward under policy pi\n",
    "        AA[x] = -gamma * T[x,a]\n",
    "        AA[x,x] += 1\n",
    "    return AA,b\n",
    "    \n",
    "def calculate_value_function(pi, gamma=0.9):\n",
    "    \"\"\"Calculate value function for given policy\"\"\"\n",
    "    AA, b = calculate_value_system(pi, gamma)\n",
    "    return np.linalg.solve(AA,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the linear system for the policy `reasonable_policy` (which is really almost always wrong):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =\n",
      "[[ 0.1  -0.   -0.   -0.   -0.  ]\n",
      " [-0.72  0.82 -0.   -0.   -0.  ]\n",
      " [-0.   -0.    0.82 -0.72 -0.  ]\n",
      " [-0.72 -0.   -0.    0.82 -0.  ]\n",
      " [-0.   -0.   -0.   -0.72  0.82]]\n",
      "b = [10.  8.  0.  8.  0.]\n"
     ]
    }
   ],
   "source": [
    "AA, b = calculate_value_system(reasonable_policy)\n",
    "print(f\"A =\\n{AA}\")\n",
    "print(f\"b = {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we calculate the value function $V^\\pi$ under this policy `reasonable_policy` we see that our *exact* value function was well approximated by Monte Carlo estimate above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(reasonable_policy):\n",
      "  Living Room : 100.00\n",
      "  Kitchen     : 97.56\n",
      "  Office      : 85.66\n",
      "  Hallway     : 97.56\n",
      "  Dining Room : 85.66\n"
     ]
    }
   ],
   "source": [
    "value_for_pi = calculate_value_function(reasonable_policy)\n",
    "print(\"V(reasonable_policy):\")\n",
    "for i,room in enumerate(vacuum.rooms):\n",
    "    print(f\"  {room:12}: {value_for_pi[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Why is the value function in the living room $100$ and not the immediate reward $10$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy and Value Function\n",
    "\n",
    "> The optimal policy and optimal value function are besties.\n",
    "\n",
    "Now that we know how to compute the value function for an arbitrary policy $\\pi$,\n",
    "we turn our attention to computing the **optimal value function**,\n",
    "which can be used to construct the **optimal policy** $\\pi^*$\n",
    "\n",
    "The optimal value function  $V^*: {\\cal X} \\rightarrow {\\cal A}$\n",
    "is merely the value function for the optimal policy.\n",
    "This can be written mathmatically as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^*(x) &= \\max_\\pi V^{\\pi}(x) \\\\\n",
    "&=\n",
    "\\max_\\pi \\left\\{ \\bar{R}(x,\\pi(x)) + \\gamma \\sum_{x'} P(x'|x, \\pi(x)) V^\\pi(x')   \\right\\}\\\\\n",
    "&=\n",
    "\\max_a  \\left\\{ \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^*(x')   \\right\\} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the above, the second line follows immediately by substituting\n",
    "our earlier expression for $V^\\pi$ into the maximization.\n",
    "The third line is more interesting.\n",
    "Because the value function has been written in recursive form,\n",
    "$\\pi$ is only applied to the current state (i.e., when $\\pi$ is evaluated in the optimization,\n",
    "it always appears as $\\pi(x)$).\n",
    "Therefore, we can write the optimization\n",
    "as a maximization with respect to the *action* applied in the *current state*, rather than as a\n",
    "maximization with respect to the entire policy $\\pi$!\n",
    "This equation is known as the **Bellman equation**.\n",
    "It is named after Richard Bellman, the mathematician\n",
    "who discovered it, and it is one of the most important equations in all of computer science.\n",
    "\n",
    "The Bellman equation has a very nice interpretation: \n",
    "the optimal value function of a state is the maximum expected reward \n",
    "*plus* the discounted expected value function when acting optimally in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bellman's equation, it is straightforward to compute the optimal policy from a given state.\n",
    "\n",
    "$$\n",
    "\\pi^*(x) = \\arg\n",
    "\\max_a  \\left\\{ \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^*(x')   \\right\\} \n",
    "$$\n",
    "\n",
    "This computation is performed so often that it is convenient to introduce the so-called $Q$-function\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q^*(x,a) \\doteq \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^*(x') \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which allows us to write the optimal policy as\n",
    "\n",
    "\n",
    "$$\n",
    "\\pi^*(x) = \\arg\n",
    "\\max_a  Q^*(x,a)\n",
    "$$\n",
    "\n",
    "We will see the $Q$-function again, when we discuss reinforcement learning.\n",
    "The code for computing a Q-value from a value function is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_value(value_function, x, a, gamma=0.9):\n",
    "    \"\"\"Calculate Q(x,a) from given value function\"\"\"\n",
    "    return T[x,a] @ (R[x,a] + gamma * value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will describe two methods for determining the optimal policy.\n",
    "The first method, policy iteration, iteratively improves candidate policies,\n",
    "ultimately converging to the optimal policy $\\pi^*$.\n",
    "The second method, value iteration, iteratively improves an estimate of $V^*$,\n",
    "ultimately converging to the optimal value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By iteratively improving an estimate of the optimal policy, we eventually find $\\pi^*$.\n",
    "\n",
    "One way to compute an optimal policy is to start with an initial guess\n",
    "at the optimal policy, and then iteratively improve our guess until until no futher improvements are possible.\n",
    "This is exactly the approach taken by **policy iteration**.\n",
    "In particular, policy iteration generates a sequence of policies\n",
    "$\\pi^0, \\pi^1, \\dots \\pi^n$, such that $\\pi^{k+1}$ is better than policy $\\pi^k$.\n",
    "This process ends when no further improvement is possible, which\n",
    "occurs when $\\pi^{k+1} = \\pi^k.$\n",
    "\n",
    "To improve the policy $\\pi^k$, we update the action chosen *for each state* by applying\n",
    "Bellman's equation using $\\pi^k$ in place of $\\pi^*$.\n",
    "The can be achieved with the following algorithm:\n",
    "\n",
    "Start with a random policy $\\pi^0$, and repeat until convergence:\n",
    "1. Compute the value function $V^{\\pi^k}$\n",
    "2. Improve the policy for each state $x \\in {\\cal X}$ using the update rule: \n",
    "$\\pi^{k+1}(x) \\leftarrow\\arg \\max_a \\sum_{x'} \\{P(x'|x, a) \\{R(x, a, x') + \\gamma V^k(x')\\}$\n",
    "\n",
    "Notice that this algorithm has the side benefit of computing \n",
    "successively better approximations to the value function at each iteration.\n",
    "Because there are a finite number of actions that can be applied ini each state, there are only finitely many ways to update\n",
    "a policy. Therefore, we expect this policy iteration algorithm to converge in finite time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to do step one, via `calculate_value_function`. The second step of the algorithm is easily\n",
    "implemented with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(value_function):\n",
    "    \"\"\"Update policy given a value function\"\"\"\n",
    "    new_policy = [None for _ in range(5)]\n",
    "    for x, room in enumerate(vacuum.rooms):\n",
    "        Q_values = [Q_value(value_function, x, a) for a in range(4)]\n",
    "        new_policy[x] = np.argmax(Q_values)\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole policy iteration algorithm then simply iterates these until the policy no longer changes. If no initial policy is given, we can\n",
    "start with a zero value function\n",
    "$V^{\\pi^0}(x) = 0$ for all $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(pi=None, max_iterations=100):\n",
    "    \"\"\"Do policy iteration, starting from policy `pi`.\"\"\"\n",
    "    for _ in range(max_iterations):\n",
    "        value_for_pi = calculate_value_function(pi) if pi is not None else np.zeros((5,))\n",
    "        new_policy = update_policy(value_for_pi)\n",
    "        if new_policy == pi:\n",
    "            return pi, value_for_pi\n",
    "        pi = new_policy\n",
    "    raise RuntimeError(\"No stable policy found after {max_iterations} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we have a guess for the initial policy, we can intialize\n",
    "$\\pi^0$ accordingly.\n",
    "For example, we can start with a not-so-smart `always_right` policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIGHT = vacuum.action_space.index(\"R\")\n",
    "\n",
    "always_right = [RIGHT, RIGHT, RIGHT, RIGHT, RIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'L', 'R', 'U', 'U']\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value_function = policy_iteration(always_right)\n",
    "print([vacuum.action_space[a] for a in optimal_policy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the `always_right` policy, our policy iteration algorithm converges to an\n",
    "intuitively pleasing policy.\n",
    "In the dining room and kitchen we go `left`, in the office we go `right`, and in the hallway and dining room we go `up`.\n",
    "This is significantly different from the `always_right` policy (which might be better named `almost_always_wrong`).\n",
    "In fact, it is exactly the `reasonable_policy` that we created above.\n",
    "We already knew that it should be pretty good at getting to the living room as fast as possible. In fact, it is optimal!\n",
    "\n",
    "We also print out the optimal value function below, which shows that if we are close to the living room the value function is very high, but it is a bit lower in the office in the dining room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Living Room : 100.00\n",
      "  Kitchen     : 97.56\n",
      "  Office      : 85.66\n",
      "  Hallway     : 97.56\n",
      "  Dining Room : 85.66\n"
     ]
    }
   ],
   "source": [
    "for i,room in enumerate(vacuum.rooms):\n",
    "    print(f\"  {room:12}: {optimal_value_function[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy is also obtained when we start without a policy, starting with a zero value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'L', 'R', 'U', 'U']\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, _ = policy_iteration()\n",
    "print([vacuum.action_space[a] for a in optimal_policy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just be sure, let us sanity check the solution above using the Monte Carlo estimate of the policy, which should give the same answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(Living Room) ~ 100.00\n",
      "V(Kitchen) ~ 98.03\n",
      "V(Office) ~ 84.55\n",
      "V(Hallway) ~ 97.48\n",
      "V(Dining Room) ~ 85.91\n"
     ]
    }
   ],
   "source": [
    "nr_samples = 100\n",
    "horizon = 100\n",
    "X = VARIABLES.discrete_series('X', range(1, horizon+1), vacuum.rooms)\n",
    "A = VARIABLES.discrete_series('A', range(1, horizon), vacuum.action_space)\n",
    "for x1, room in enumerate(vacuum.rooms):\n",
    "    V_x1 = approximate_value_function(x1, optimal_policy, nr_samples, horizon)\n",
    "    print(f\"V({room}) ~ {V_x1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are remarkably similar to the exact values computed above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "> Dynamic programming can be used to obtain the optimal value function.\n",
    "\n",
    "Recall Bellman's equation, which must hold for each state $x$.\n",
    "\n",
    "$$\n",
    "V^*(x) = \\max_a  \\left\\{ \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^*(x')   \\right\\} \n",
    "$$\n",
    "\n",
    "Sadly, this is not a linear equation (the maximimation operation is not linear), so we cannot solve this\n",
    "equation for $V^*$ as a system of linear equations.\n",
    "**Value iteration** approximates $V^*$ by constructing a sequence of estimates,\n",
    "$V^0, V^1, \\dots , V^n$ that converges to $V^*$.\n",
    "Starting with an initial guess, $V^0$, at each iteration we update\n",
    "our approximation of the value function by the update rule:\n",
    "\n",
    "$$\n",
    "V^{k+1}(x) \\leftarrow \\max_a \\left\\{ \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^k(x')   \\right\\} \n",
    "$$\n",
    "\n",
    "Notice that the right hand side of includes two terms:\n",
    "the expected reward (which we can compute exactly), and a term in $V^k$ (our current best guess at the value function).\n",
    "Value iteration operates by iteratively using our *current best guess* of $V^*$ along with the *known* expected reward\n",
    "to update the approximation.\n",
    "Unlike policy iteration, we do not expect value iteration to converge to the exact result in finite time.\n",
    "Therefore, we cannot use $V^{k+1} = V^k$ as our termination condition.\n",
    "Instead, we often use a condition such as $|V^{k+1} - V^k| < \\epsilon$, for some small value of $\\epsilon$\n",
    "as the termination condition.\n",
    "\n",
    "Finally, note that we can define Q values for the $k^{th}$ iteration as\n",
    "$$\n",
    "Q(x, a; V^k) \\doteq \\bar{R}(x,a) + \\gamma \\sum_{x'} P(x'|x, a) V^k(x'),\n",
    "$$\n",
    "\n",
    "and hence a value update is simply\n",
    "$$\n",
    "V^{k+1}(x) \\leftarrow \\max_a Q(x, a; V^k).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, this is actually easier than policy iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.  98.  90.  98.  90.]\n",
      "[100.    97.64  86.76  97.64  86.76]\n",
      "[100.    97.58  85.92  97.58  85.92]\n",
      "[100.    97.56  85.72  97.56  85.72]\n",
      "[100.    97.56  85.68  97.56  85.68]\n",
      "[100.    97.56  85.67  97.56  85.67]\n",
      "[100.    97.56  85.66  97.56  85.66]\n",
      "[100.    97.56  85.66  97.56  85.66]\n",
      "[100.    97.56  85.66  97.56  85.66]\n",
      "[100.    97.56  85.66  97.56  85.66]\n"
     ]
    }
   ],
   "source": [
    "V_k = np.full((5,), 100)\n",
    "for k in range(10):\n",
    "    Q_k = np.sum(T * (R + 0.9 * V_k), axis=2) # 5 x 4\n",
    "    V_k = np.max(Q_k, axis=1) # max over actions\n",
    "    print(np.round(V_k,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with optimal value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.    97.56  85.66  97.56  85.66]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(optimal_value_function, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can easily *extract* the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy = [0 0 1 2 0]\n",
      "['L', 'L', 'R', 'U', 'L']\n"
     ]
    }
   ],
   "source": [
    "Q_k = np.sum(T * (R + 0.9 * V_k), axis=2)\n",
    "pi_k = np.argmax(Q_k, axis=1)\n",
    "print(f\"policy = {pi_k}\")\n",
    "print([vacuum.action_space[a] for a in pi_k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Markov Decion Processes or MDPs are used to model decision in a stochastic environment, albeit with complete knowledge of the state. It is a rich subject, and we introduced many new concepts in this section:\n",
    "\n",
    "- The reward function $R : {\\cal X} \\times {\\cal A} \\times {\\cal X} \\rightarrow \\mathbb{R}$.\n",
    "- The expected reward $\\bar{R}(x,a)$ for executing action $a$ in state $x$, and the corresponding greedy planning algorithm.\n",
    "- The utility function $U: {\\cal A}^n \\times {\\cal X}^{n+1} \\rightarrow \\mathbb{R}$ as the sum of discounted rewards.\n",
    "- The notion of rollouts to approximate the expected utility of actions.\n",
    "- The policy $\\pi: {\\cal X} \\rightarrow {\\cal A}$ as a mapping from states to actions.\n",
    "- the value function $V^\\pi:{\\cal X} \\rightarrow \\mathbb{R}$ associated with a given policy $\\pi$.\n",
    "- The use of policy rollouts to approximate the value function $V^\\pi$.\n",
    "- Exact calculation of the value function for a fixed policy.\n",
    "- The optimal policy and value function, governed by the Bellman equation.\n",
    "- Two algorithms to compute those: policy iteration and value iteration.\n",
    "\n",
    "There are two important extensions to MDPs that are not covered in this section:\n",
    "\n",
    "- Partially Observable MDPs (or POMDPS) are appropriate when we cannot directly observe the state. We will not cover these in this book.\n",
    "- Reinforcement learning, a way to learn MDP policies from *data*. This will be covered next. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S35_vacuum_decision.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('nbdev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  },
  "vscode": {
   "interpreter": {
    "hash": "341996cd3f3db7b5e0d1eaea072c5502d80452314e72e6b77c40445f6e9ba101"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
