{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S56_diffdrive_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q -U gtbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# No imports for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "> Stochastic gradient descent and the like.\n",
    "\n",
    "**This Section is still in draft mode and was released for adventurous spirits (and TAs) only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div align='center'>\n",
       "        <img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S56-Two-wheeled%20Toy%20Robot-05.jpg?raw=1' style='height:256 width:100%'/>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gtbook.display import randomImages\n",
    "from IPython.display import display\n",
    "display(randomImages(5, 6, \"steampunk\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Setup\n",
    "\n",
    "> From data, learn concept.\n",
    "\n",
    "In the **supervised learning** setup, we have a large number of examples of inputs $x$ and corresponding labels $y$.\n",
    "We will often refer to the *training dataset* as $D$, consisting of pairs $(x,y)$.\n",
    "If the labels $y$ are *discrete*, we talk about a supervised **classification** problem. If the labels $y$ are *continuous*, this is called a supervised **regression** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "> Calculate gradient, reduce loss.\n",
    "\n",
    "A neural network output, and in particular a CNN, depends on the large set of continuous weights $W$ that make up its convolutional layers, pooling layers, and fully connected layers. When we train a neural networks, we adjust its weights $W$ to perform better on the task at hand, be it classification or regression. To measure whether the model performs \"better\", we find a **loss function**. To adjust the weights, we calculate the gradient of the loss function with respect to each of the weights, and adjust the weights accordingly. That procedure is called **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "> Calculate approximate gradient, probably reduce loss.\n",
    "\n",
    "Stochastic gradient descent is an approximate gradient descent procedure, to cope with the very large data sets typically thrown at supervised problems. It is typically impossible to calculate the *exact* gradient, which requires looping over all the examples, which can run in the millions (or billions). An easy approximation scheme is to *randomly sample* a small subset of the examples, and calculate the gradient of the weights using only those examples. The upside is that this is much faster, but the downside is that this is only approximate. Hence, if we adjust weights with this approximate gradient, we might or might not make progress on the task. This procedure is called **stochastic gradient descent**, and it works amazingly well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "> A loss function for every occasion.\n",
    "\n",
    "Different tasks require different loss functions, and a lot of creativity and research goes into crafting loss functions for complex tasks. For \"vanilla\" regression tasks, we typically use a **sum of squares differences** loss function as we already encountered before:\n",
    "\n",
    "$$L_{\\text{SSD}}(W; D) \\doteq \\sum_{(x,y)\\in D}|f(x;W)-y|^2$$\n",
    "\n",
    "Above $f(x;W)$ is the continuous prediction function implemented by the neural network, where $W$ represents the weights in all layers. Note that the formula above can be easily generalized to vector-valued labels $y$.\n",
    "\n",
    "For classification, the **cross entropy** loss function is very popular: it measures the average disagreement of the predicted labels with the ground truth labels:\n",
    "\n",
    "$$L_{\\text{CE}}(W; D) \\doteq \\sum_c \\sum_{(x,y=c)\\in D}\\log\\frac{1}{p_c(x;W)}$$\n",
    "\n",
    "This formula seems perhaps unintuitive and rather complicated. However, it is actually quite intuitive once you understand a few concepts.\n",
    "In particular, in the multi-class classification problem we assume that the neural network outputs a probability $p_c(x;W)$ for every class $c\\in[N]$, where $N$ is the number of classes. The quantity \n",
    "\n",
    "$$\\log\\frac{1}{p_c(x;W)}$$\n",
    "\n",
    "is called the *surprise* that we should experience when seeing a label $y=c$. \n",
    "Indeed, for example, if we see class $3$ and the probability output by the network is $p_3(x;W)=1$, we are not surprised at all, as $\\log\\frac{1}{1}=\\log 1 = 0$. \n",
    "However, if the probability is only $0.01$, our surprise is $\\log\\frac{1}{0.01}=\\log 100 = 2$.\n",
    "The lower the probability, the higher the surprise. Hence, the cross-entropy above measures the *average surprise* for seeing the labeled examples in the training data. After training, the network is the least surprised possible, hopefully, which is why it is an intuitive lost function to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S56_diffdrive_learning.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
