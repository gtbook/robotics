{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vu4yAEm8M-A-",
   "metadata": {},
   "source": [
    "# Chapter Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a655b",
   "metadata": {
    "tags": [
     "no-pdf"
    ]
   },
   "source": [
    "<img src=\"Figures2/S20-Trash_sorting_robot_with_gripper-03.jpg\" alt=\"Splash image with heroic trash-sorting robot in sunset light\" width=\"60%\" align=center style=\"vertical-align:middle;margin:10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d-p1SegWG7q",
   "metadata": {},
   "source": [
    "In this chapter, we used the example of a trash sorting robot to introduce key concepts from probability theory, decision theory, and even machine learning, and showed how these concepts can be used to begin building robotic systems that operate in real world settings, where significant uncertainty may exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aNuID86bwNq9",
   "metadata": {},
   "source": [
    "```{index} pair: probability density function; PDF\n",
    "```\n",
    "## Models\n",
    "Probability theory provides the mathematical foundation for dealing with uncertainties that confront\n",
    "robotic systems.\n",
    "In this chapter, we used probability to reason about the uncertain category of an object in the robot's work space.\n",
    "We began by introducing *probability distributions*, which assign probability values to *events* (i.e., to subsets of the set of all possible outcomes). In the case of discrete outcomes, such as our five categories\n",
    "of trash, these probability distributions are called *probability mass functions* or *PMFs*, and they assign finite probability to each possible category.\n",
    "When the set of outcomes is continuous, such as the scale we used to measure an object's weight,\n",
    "these probability distributions are called *probability density functions* or *PDFs*, and they assign probability values to subsets of real numbers, for example, to intervals of the form $[a,b]$,\n",
    "and this probability value can be computed by integrating the PDF over the interval.\n",
    "Interestingly, for the continuous case, zero probability is assigned to any individual outcome,\n",
    "since this would correspond to an interval of the form $[a,a]$, and for any continuous\n",
    "function $f$, we have $\\int_a^a f(u)du = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JczKNG58OUS5",
   "metadata": {},
   "source": [
    "While probability distributions give some insight into the behavior of individual random outcomes,\n",
    "in robotics we are often interested in average behavior over long periods of time. For the example of our trash sorting robot, we might like to characterize the average performance of the system over days, or even weeks.\n",
    "*Expectation* is a property of probability distributions that gives insight into average behavior over many trials.\n",
    "The expected value of a discrete random variable is given by\n",
    "\\begin{equation}\n",
    "E[X] = \\sum_{i=1}^n x_i p_X(x_i)\n",
    "\\end{equation}\n",
    "This expression bears strong resemblance to the equation for the weighted average of\n",
    "a set of data values, and this is no coincidence.\n",
    "In fact, this is the essence of the weak law of large numbers, which says that,\n",
    "as the number of data points goes to infinity,\n",
    "the average of a set of data points will approach the expected value of the probability\n",
    "distribution from which the data were drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ijMMxWKrWYN",
   "metadata": {},
   "source": [
    "When there are multiple sources of uncertainty in the world, we can use *joint probability distributions* to characterize the stochastic relationships between different random quantities.\n",
    "For example, what is the probability that a piece of trash\n",
    "will conduct electricity and simultaneously weigh between $1.5$ and $3.0$ kg?\n",
    "A joint PMF for two discrete random variables $X$ and $Z$ is denoted by\n",
    "$p_{XZ}(x,z)$, and is defined by $p_{XZ}(x,z) = P(X=x, Z=z)$.\n",
    "(Recall that we use upper case letters to denote random variables,\n",
    "and lower case letters to denote possible values taken by these random variables.)\n",
    "This can be extended to any number of random variables, so that, in theory, all uncertainties\n",
    "in the world could be modeled by a single joint probability distribution.\n",
    "However, specifying a complete joint probability distribution is exceedingly expensive.\n",
    "Consider that if we have $n$ random variables which take on $N_1, N_2,\\dots N_n$ possible values, respectively,\n",
    "the size of a table to represent the joint probability distribution\n",
    "would be $N_1 \\times N_2 \\times \\dots N_n$, i.e., the size of this data structure\n",
    "grows exponentially with the number of random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y-GOu6o9jZRg",
   "metadata": {},
   "source": [
    "Happily, for most real-world scenarios, we really don't need the complete joint probability distribution.\n",
    "The reason for this is that the interactions between random quantities often have a fairly local kind\n",
    "of behavior.\n",
    "Because of this property, we can often capture the essential interactions among random variables\n",
    "using *conditional probability distributions*, which capture the individual stochastic relationships between\n",
    "smaller sets of random variables.\n",
    "For example, we can describe the conditional probability that an object is scrap metal if it conducts electricity,\n",
    "or, conversely, the probability that an object will conduct electricity if it is scrap metal.\n",
    "Conditional probability distributions are typically used to model sensor behavior;\n",
    "the conditional probability $P(Z| X=x)$ denotes the probability distribution for random variable $Z$\n",
    "given that the random variable $X$ takes the value $x$.\n",
    "This kind of model is sometimes called a forward sensor model, since it models the behavior of the sensor\n",
    "given the state of the world.\n",
    "Note that the conditional distribution $P(Z | X=x)$ is itself a valid probability distribution.\n",
    "For example, if $Z$ is a discrete random variable, we would have $\\sum_i P(Z=z_i | X=x) =1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TbGCJu5fxdNR",
   "metadata": {},
   "source": [
    "We also saw how probabilistic models can be incorporated into computational solutions via sampling.\n",
    "In particular, we developed an algorithm that uses the cumulative distribution function to generate samples for a given particular probability distribution. We used this sampling algorithm to investigate the empirical relationship between outcomes generated from a probability distribution and the expected value of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3SOvftjCHTlp",
   "metadata": {},
   "source": [
    "## Reasoning\n",
    "\n",
    "While the various kinds of probability models nicely describe the stochastic nature of the world,\n",
    "it is not immediately obvious how they could be used to make inferences about the world,\n",
    "or as a basis for decision-making in an uncertain world.\n",
    "The key idea that enables this kind of reasoning is captured in Bayes’ theorem:\n",
    "\\begin{equation}\n",
    "P(X|Z=z)=\\frac{P(Z=z|X)P(X)}{P(Z=z)}\n",
    "\\end{equation} \n",
    "Note that the forward model, $P(Z=z|X)$ appears on the right hand side, and is used to compute the *inverse model*\n",
    "$P(X | Z=z)$. For example, if we are given the value of a sensor reading, $Z$, Bayes’ theorem can\n",
    "be used to compute the probability associated to the state $X$.\n",
    "For this reason, Bayes’ theorem is sometimes referred to as an inversion theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oMX7Kt6IHLbX",
   "metadata": {},
   "source": [
    "We can use Bayes’ theorem to compute the *maximum a posteriori* estimate (or *MAP* estimate) for the value of $X$ given sensor reading $Z =z$ by solving the optimization problem:\n",
    "\\begin{equation}\n",
    "x^*_{MAP} = \\arg \\max_x P(x|z)\n",
    "\\end{equation}\n",
    "Note that this computation requires that we have access to the prior probability distribution $P(X)$.\n",
    "In cases where this is not available, we may instead choose to compute the *maximum likelihood* estimate (or *MLE*),\n",
    "which is given by\n",
    "\\begin{equation}\n",
    "x^*_{MLE} =  \\arg \\max_x L(x;z) =  \\arg \\max_x P(Z=z|X) \n",
    "\\end{equation}\n",
    "in which $L(x;z) = P(Z=z|X)$ is called the likelihood.\n",
    "Note that the likelihood is *not* a valid probability distribution,\n",
    "since typically $\\sum_i P(Z=z | X=x_i) \\neq 1$.\n",
    "\n",
    "<!-- **Some words about learning - estimating a pdf, modeling senors and calibration, parameter estimation.\n",
    "OR DOES THIS GO IN THE MODELING SECTION?** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tABNs8XxP3Vl",
   "metadata": {},
   "source": [
    "\n",
    "## Background and History\n",
    "The origins of probability theory can be traced back to games of chance in ancient societies, but the first real attempts to formalize the study of probability came during the Renaissance, in the works of mathematicians such as Cardano, Pascal, and Fermat. These early mathematical approaches focused mainly on games of chance, with a strong empirical flavor. The line between statistics and probability theory was a blurry one in Renaissance times.\n",
    "\n",
    "It was Bayes, in the eighteenth century, who pioneered the idea of using evidence, together with ideas from probability theory, to draw inferences. While what we now know as Bayes’ theorem is a general result that does not depend on the specific probability distributions under consideration, Bayes studied the specific case of inferring the parameter of a binomial distribution given observed outcomes. The more general development is due largely to Laplace, in the years following the death of Bayes.\n",
    "\n",
    "What we think of today as probability theory was formalized in the early 1930’s by Kolmogorov. It was Kolmogorov who formulated the three axioms that form the basis for modern probability theory:\n",
    "1.\tFor any event $A$, $P(A) \\geq 0$.\n",
    "2.\t$P(\\Omega)=1$.\n",
    "3.\tFor disjoint events $A$ and $B$, $P(A\\cup B) = P(A) + P(B)$.\n",
    "\n",
    "Equipped with these three axioms and a background in *real analysis*, one can derive most all of the important results that comprise modern probability theory.\n",
    "The Renaissance mathematicians were interested in understanding random phenomena. Kolmogorov, a Russian mathematician, was interested in establishing a rigorous theoretical foundation for probability theory. \n",
    "\n",
    "One of the best recent books we have found useful is the book [\"Introduction to Probability for Data Science\"](https://probability4datascience.com/index.html) {cite:p}`Chan23book_prob4ds`.\n",
    "The classic reference for statistical reasoning, including \n",
    "maximum likelihood estimation and Bayesian decision theory \n",
    "is {cite:p}`duda2012pattern`.\n",
    "Anders Hald has written two volumes on the history of probability theory, \n",
    "one that covers the period from Bernoulli, De Moivre and Laplace to the mid-twentieth century,\n",
    "with particular attention to estimation problems {cite:p}`HaldBook98`,\n",
    "and one that covers developments prior to 1750 {cite:p}`HaldBook03`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
